<?xml version="1.0" encoding="UTF-8"?>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link href='http://fonts.googleapis.com/css?family=PT+Mono|Open+Sans:400italic,700italic,400,700,800,300&subset=latin,latin-ext,greek-ext,greek' rel='stylesheet' type='text/css'>
<link rel="stylesheet" type="text/css" href="..\oxdoc.css">
<link rel="stylesheet" type="text/css" media="print" href="..\print.css">
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<title>GetStartedDIY - CFMPI</title>
</head>
<body>
<div class="header">
[ <img class="icon" src="icons/uplevel_s.png">&nbsp;<a href="..\default.html">Up Level</a> |
<img class="icon" src="icons/project_s.png">&nbsp;<a href="default.html">Project home</a>
 | <img class="icon" src="icons/index_s.png">&nbsp;<a href="index.html">Index</a>
 | <img class="icon" src="icons/hierarchy_s.png">&nbsp;<a href="hierarchy.html">Class hierarchy</a> ]</div>
<h1><span class="icon"><img class="icon" src="icons/file.png">&nbsp;</span><span class="text">GetStartedDIY</span></h1>

A simple example of calling MPI yourself to run code in parallel.
<P/>
This document explains the example program <code>MPItest.ox</code> and shows how to run it on a particular cluster.  The details of running programs will different across clusters.  This is the <q>DIY</q> version because it uses the low-level routines that interface between Ox and the MPI library.  The objects in CFMPI make it possible to avoid some of this programming or to avoid it entirely.
<P/>
The program:
<details><summary></summary></details>
<P/>
This program is an example of a very simply client/server setup.
<P/>
<DT>Include CFMPI</DT>
<P/>
<DT>Find out about the MPI environment</DT>
<P/>
<DT>Start the client task if I am the client</DT>
<DD>Send messages to all clients</DD>
<DD>Wait for all return messages to arrive</DD>
<DD>Use the results</DD>
<P/>
<DT>Start the server task if I am a server</DT>
<DD>Wait for a message from the client</DD>
<DD>Process the message</DD>
<DD>Send the results back to the client</DD>
<P/>
<DT>Finalize the MPI environment</DT>
<P/>
CFMPI uses Ox's <code>??</code> routine to add a call to <code>MPI_Finalize()</code> as Ox exits.
<P/>
Run the program in fake MPI mode.
<P/>
Follow <a href="./InstallAndUse.html">Install and Use</a> to compile <code>CFMPI.c</code> to a <em>shared object</em> file and ensure both that and the MPI library are on the <code>LD_LIBRARY_PATH</code>.
<P/>
Run the program using MPI,
oxl -DMPI MPItest
<P/>
For this to work the program (<code>oxl</code>) must be run within the MPI environment.  How that is done depends on the cluster, but usually there is a command or script which will execute your job with MPI.
<P/>
This script that works on <code>SHARCnet</code> is included in <code>niqlow/examples</code> and is very simple:
<P/>
this says "submit the job to the MPI execution queue, ask for ?? processors, put the output in the file <code>???</code>.
<P/>
After all those arguments comes the actual command line that you will run your parallel program.
<P/>
Output
The output is below.

<div class="footer">
Generated by <a href="http://oxdoc.sourceforge.net">oxdoc 1.1-beta</a> &copy Copyright 2005-2014 by Y. Zwols<br>
Math typesetting by <a href="http://www.mathjax.org/">Mathjax</a>
</div>
