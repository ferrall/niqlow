<?xml version="1.0" encoding="UTF-8"?>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link href='http://fonts.googleapis.com/css?family=PT+Mono|Open+Sans:400italic,700italic,400,700,800,300&subset=latin,latin-ext,greek-ext,greek' rel='stylesheet' type='text/css'>
<link rel="stylesheet" type="text/css" href="..\oxdoc.css">
<link rel="stylesheet" type="text/css" media="print" href="..\print.css">
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML"></script>
<title>GetStartedDIY - CFMPI</title>
</head>
<body>
<div class="header">
[ <img class="icon" src='../icons/glass.png' width=16px' height='16px'/> <a href="../search.htm">Search</a> | <img class="icon" src="icons/uplevel_s.png">&nbsp;<a href="..\default.html">Up Level</a> |
<img class="icon" src="icons/project_s.png">&nbsp;<a href="default.html">Project home</a>
 | <img class="icon" src="icons/index_s.png">&nbsp;<a href="index.html">Index</a>
 | <img class="icon" src="icons/hierarchy_s.png">&nbsp;<a href="hierarchy.html">Class hierarchy</a> ]</div>
<h1><span class="icon"><img class="icon" src="icons/file.png">&nbsp;</span><span class="text">GetStartedDIY</span></h1>

A simple example of calling MPI yourself to run code in parallel.
<P/>
This document explains the example program <code>baseMPI.ox</code> and shows how to run it on a particular cluster.  The details of running programs will different across clusters.  This is the <q>DIY</q> version because it uses the low-level routines that interface between Ox and the MPI library.  The objects in CFMPI make it possible to avoid some of this programming or to avoid it entirely.</p>
<P/>
<DT>The program:</DT>
<DD><details><summary>Source: <a href="javascript:popoxcode('../../examples/CFMPI/baseMPI.ox');">examples/CFMPI/baseMPI.ox</a></summary><pre>
<object width="100%" height="300" type="text/plain" data="../../examples/CFMPI/baseMPI.ox" border="1" ></object>
</pre></details></DD>
is an example a very simply client/server setup.</p>
<P/>
<DT>Include CFMPI</DT>
<DD>Note that <code>&#35;include</code> is used not <code>&#35;import</code>.   The reason is that the MPI interface routines have to be compiled on your machine and unlike the rest of <span class="n">niqlow</span> the Ox code cannot be distributed as compiled <code>.oxo</code> files</DD>
<P/>
<DT>Foil automatic (and problematic) processing by OxDoc</DT>
<details><summary>Explanation of the next 3 lines</summary><DD>The next three lines could just be the middle line: <code>&#35;include "UseMPI.ox";</code>.  The conditional compilation around it is really just a trick to keep the OxDoc program from trying to create documentation for that file.  (<code>OX_Parallel</code> is defined for versions of Ox required by <span class="n">niqlow</span>, so this always happens in Ox but OxDoc does not know about this macro so it skips the file.</DD></details>
<P/>
<DT>Declare shared (global) space.</DT>
<DD>In this code, the <code>server()</code> and <code>client()</code> routines are separate, but in a serial environment they may have to act as if they are passing messages back and fort. The global variables (defined outside curly brackets) are used as share space for <code>main()</code> and the other routines. In the object-oriented solutions in CFMPI this would not be required.  The classes for client and server tasks refer to inherited data members that will get set properly on all nodes.</DD>
<P/>
<DT><code>main(){&hellip;}</code></DT>
<DD>First, call <code><a href="MPIinterface.ox.html#MPI_Init">MPI_Init</a>()</code> in order to find out which <code>node</code> I am and how many other nodes there are. Next, if I the client node, start the client task.  Otherwise, become a server.</DD>
<P/>
<DT><code>client(){&hellip;}</code></DT>
<DD>Send messages to all servers.</DD>
<DD>Act like a server to myself (do some work while the other servers are busy)</DD>
<DD>Wait for all return messages to arrive (in a random order)</DD>
<DD>Use the results (here just print out the results)</DD>
<P/>
<DT><code>server(){&hellip;}</code></DT>
<DD>Wait to receive a message from the client</DD>
<DD>Process the message</DD>
<DD>Send the results back to the client</DD>
<P/>
<DT>Finalize the MPI environment</DT>
<DD>CFMPI uses Ox's <code>OxRunMainExitCall()</code> routine (described in the Developer's manual) to add a call to <code>MPI_Finalize()</code> as Ox exits. This means that the user's code can simply exit and no MPI error will be thrown.</DD>
<P/>
<DT>Run the program <em>without</em> using MPI</DT>
<DD><pre>
oxl baseMPI
</pre></DD>
<DD>When the MPI environment is not requested, fake routines are inserted for <code>MPI_Init()</code>, <code>MPI_Recv()</code> and <code>MPI_Send()</code></DD>.
<DD>The first will set <code>I=0</code> and <code>Nodes=1</code> (serial environment).  Then the others will act as if a message was past when really nothing happens.  But by referencing the same location as the <code>buffer</code> the "message" is sitting there.</DD>
<DD>And since the client calls the server after getting (the non-existent) real servers started, the job is done.</DD>
<P/>
<DT>Run the program using MPI</DT>
<DD>Follow <a href="./InstallAndUse.html">Install and Use</a> to compile <code>CFMPI.c</code> to a <em>shared object</em> file and ensure both that and the MPI library are on the <code>LD_LIBRARY_PATH</code>.
<DD><pre>
oxl -DMPI baseMPI
</pre>
For this to work the program (<code>oxl</code>) must be run within the MPI environment.  How that is done depends on the cluster, but usually there is a command or script which will execute your job with MPI.</DD>
<P/>
<details><summary>SHARCnet script.  Source: <a href="javascript:popoxcode('../../examples/sharc_test_MPI');">examples/sharc_test_MPI</a></summary>
<pre><object width="100%" height="100" type="text/plain" data="../../examples/sharc_test_MPI" border="1"></object>
</pre></details></DD>
<DD>This says "run the command at the end of the line in job queue; run it for no more than 18 seconds (0.3 minutes), send output to <code>MPITest.out</code>, send the job to the MPI execution queue (not serial or other options), ask for 5 processors.  THe job is then simply to run the program and define <code>MPI</code> so that  <span class="n">niqlow</span> will
try to link in the MPI interface routines.  </DD>
<DD>Run the script<pre>
[ferrallc @saw-login2 examples]$ ./sharc_test_MPI
WARNING: no memory requirement defined; assuming 1GB per process.
WARNING: your timelimit (0.3m) is only 18 seconds; is this right?
submitted as jobid 4169545
</pre></DD>
<P/>
<details><summary>Output Produced</summary><pre>
Ox Console version 7.00 (Linux_64) (C) J.A. Doornik, 1994-2013
This version may be used for academic research and teaching only
<P/>
Ox Console version 7.00 (Linux_64) (C) J.A. Doornik, 1994-2013
This version may be used for academic research and teaching only
<P/>
Ox Console version 7.00 (Linux_64) (C) J.A. Doornik, 1994-2013
This version may be used for academic research and teaching only
<P/>
Ox Console version 7.00 (Linux_64) (C) J.A. Doornik, 1994-2013
This version may be used for academic research and teaching only
<P/>
Ox Console version 7.00 (Linux_64) (C) J.A. Doornik, 1994-2013
This version may be used for academic research and teaching only
Server 4 received
      0.22489
       1.7400
     -0.20426
     -0.91760
     -0.67417
     -0.34353
      0.22335
     -0.14139
     -0.18338
      0.68035
Node 4 is done
Server 1 received
       1.2282
       1.5784
     -0.39334
      0.45016
       1.2814
     -0.36170
       1.0653
      -1.9544
     -0.10203
     -0.21674
Node 1 is done
Server 2 received
       2.0016
      0.57912
     -0.70797
      0.59336
     -0.58939
       1.4674
    -0.020230
      0.73706
       1.4795
     -0.26881
Node 2 is done
Server 3 received
     0.090558
     -0.83328
      0.81350
       1.1174
      0.31499
     -0.50031
      -1.6268
      0.61943
      -1.4574
      -1.8035
Node 3 is done
Server 0 received
      0.40654
      0.13833
      0.65715
     -0.16683
      0.47835
      0.46105
      0.11538
     0.038075
       1.1944
      -1.4600
Server 0 reports roots equal to
     -0.57083       1.3755
     -0.57083      -1.3755
      -1.0186      0.49171
      -1.0186     -0.49171
      0.19962       1.1266
      0.19962      -1.1266
      0.82948      0.74242
      0.82948     -0.74242
      0.78031       0.0000
Server 4 reports roots equal to
      -7.7927       0.0000
     -0.87074      0.30347
     -0.87074     -0.30347
     -0.35572      0.86423
     -0.35572     -0.86423
      0.91222      0.12612
      0.91222     -0.12612
      0.34186      0.70678
      0.34186     -0.70678
Server 2 reports roots equal to
      0.91394      0.46905
      0.91394     -0.46905
      -1.2016       0.0000
      0.27711      0.84810
      0.27711     -0.84810
     -0.41358      0.90114
     -0.41358     -0.90114
     -0.80983       0.0000
      0.16712       0.0000
Server 3 reports roots equal to
       7.8523       0.0000
       1.9209       0.0000
       1.4604       0.0000
     -0.53961       1.0129
     -0.53961      -1.0129
      0.42852      0.83050
      0.42852     -0.83050
      -1.0863       0.0000
     -0.72346       0.0000
Server 1 reports roots equal to
      -1.4156      0.42365
      -1.4156     -0.42365
      0.65216      0.81208
      0.65216     -0.81208
      0.84066       0.0000
     -0.24734      0.86936
     -0.24734     -0.86936
    -0.052098      0.32521
    -0.052098     -0.32521
Node 0 is done
--- SharcNET Job Epilogue ---
              job id: 4169663
         exit status: 0
        elapsed time: 3s / 18s (16 %)
      virtual memory: 2.7M / 1.0G (0 %)
<P/>
Job completed successfully
WARNING: Job only used 16 % of its requested walltime.
WARNING: Job only used 0% of its requested memory.
</pre></details>

<div class="footer">
Generated by <a href="http://oxdoc.sourceforge.net">oxdoc 1.1-beta</a> &copy Copyright 2005-2014 by Y. Zwols<br>
Math typesetting by <a href="http://www.mathjax.org/">Mathjax</a>
</div>
