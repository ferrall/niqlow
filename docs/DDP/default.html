<?xml version="1.0" encoding="UTF-8"?>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link href='http://fonts.googleapis.com/css?family=PT+Mono|Open+Sans:400italic,700italic,400,700,800,300&subset=latin,latin-ext,greek-ext,greek' rel='stylesheet' type='text/css'><link rel="icon" href="icons/favicon16.png" type="image/png">
<link rel="stylesheet" type="text/css" href="../oxdoc.css">
<link rel="stylesheet" type="text/css" media="print" href="../print.css">
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML"></script>
<title>DDP.ox - Derived Dynamic Programs</title>
</head>
<body>
<div class="header">
[ <img class="icon" src='../icons/glass.png' width=16px' height='16px'/> <a href="../search.htm">Search</a> | <img class="icon" src="icons/uplevel_s.png">&nbsp;<a href="..\default.html">Up Level</a> |
<img class="icon" src="icons/project_s.png">&nbsp;<a href="default.html">Project home</a>
 | <img class="icon" src="icons/index_s.png">&nbsp;<a href="index.html">Index</a>
 | <img class="icon" src="icons/hierarchy_s.png">&nbsp;<a href="hierarchy.html">Class hierarchy</a> ]</div>
<h1><span class="icon"><img class="icon" src="icons/file.png">&nbsp;</span><span class="text">DDP.ox</span></h1>

Dynamic Programming using DDP.
<P/>
<a name="over"><H1>Overview</H1></a>
<P/>
<span class="n">DDP</span> is the part of <span class="n">niqlow</span> that allows you to build a dynamic program, solve it and use the solution to make predictions, simulate data, etc.  If you get to the stage of estimating parameters of your model, then you will also use the <a href="../FiveO/default.html"><span class="n">FiveO</span></a> part that handles optimization.</p>
<P/>
<h2>What Kind of <span class="n">niqlow</span> User Are You?</h2>
<P/>
Before trying to use this documentation or coding with <span class="n">niqlow</span>, consider your background.
<P/>
<DT>If you are new to programming and the Ox language &hellip;</DT>
<DD>This document may not be the right place to start.  You might start with my open online textbook, <a href="https://ferrall.github.io/C4E/">Computation For Economists (C4E)</a>.  That text starts with "hello, world" in Ox and covers basic programming skills and numerical algorithms for mathematics. It includes exercises and economic examples.</dd>
<P/>
<DT>If you are an experienced programmer (whether new to Ox or not), but are new to dynamic programming models &hellip;</DT>
<DD>You might start with the <a href="#guide">Beginner's Guide</a> below.  That begins with the simplest discrete choice model and how to code it in <span class="n">DDP</span>.  It then builds up to a dynamic program.</DD>
<P/>
<DT>If you are confident in your knowledge of computer programming and the mathematical technique of dynamic programming as used in economics &hellip;</DT>
<DD>You might start with <a href="GetStarted.html">GetStarted</a> which implements a simple search model in DDP. That jumps right in with a simple dynamic model and Ox code using <span class="n">DDP</span>.</DD>
<DD>You can also start with   <a href="https://ferrall.github.io/OODP/">my academic paper</a> that discusses how to build a model from scratch and estimate it using <span class="n">niqlow</span>.</DD>
<P/>
<DT>Finally, if you have already written your code to estimate parameters of a dynamic programming model from data &hellip;</DT>
<DD>You could also start with <a href="https://ferrall.github.io/OODP/">the academic paper</a> and follow the replications of existing models.  You could use that to see how you might translate your ideas into <span class="n">niqlow</span> in order to write more flexible, useable and possibly more efficient code.</DD>
<P/>
This document has two parts.  The first begins with the simplest discrete choice problem and how it could be coded in DDP.  It builds up to a true dynamic program one feature at a time.   The <a href="#tech">second part</a> is a technical description of dynamic programming and DDP. </p>
<P/>
<HR/>
<P/>
<a name="guide"><H1>Beginner's Guide</H1></a>
<P/>
<h2>Contents</h2>
<OL class="contents">
<LI><a href="OV">Overview</a></LI>
<LI><a href="#DC">A Single Discrete Choice</a></LI>
<LI><a href="#SP">Smoothed Choice Probabilities</a></LI>
<LI><a href="#MS">More than One Unlinked Choice (Multiple Static States)</a></LI>
<LI><a href="#SC">Choices Linked over Time (Dynamic States)</a></LI>
<LI><a href="#MD">Multiple Deciders</a></LI>
<LI><a href="#Next">Where to go next</a></LI>
</OL>
<P/>
<a name="OV"><h2>Overview</h2></a>
<OL class="faq">
<LI>What is Dynamic Programming?</LI>
<P/>
<DD>DP  is a framework for choices when:</DD>
<UL>
<LI>choices are sequential over time and choices can be made contingent on what has happened in the past that lead up to the current situation.</LI>
<li>the value of choices now depends on what may happen in the future</li>
<LI>choices now affect what happens in the future, and therefore anticipating future choices is crucial for understanding choices today. </LI>
</UL>
<P/>
<LI>Does Dynamic Programming Include Simpler Situations as Special Cases?</LI>
<UL>
<LI>A single choice in a single state is the most special case of DP. So static optimization can be considered a special case of DP.</LI>
<LI>When there are multiple choices but they are not linked together through time  it is also a special case.</LI>
<LI>If there multiple choices linked together through time but the agent does not care about the future then this again is a special case of DP.  It is the case of myopic decision-making.</LI>
</UL>
<LI>What is the <span class="n">DDP</span> component of <span class="n">niqlow</span></LI>
<P/>
    <DD><span class="n">DDP</span> is the component of <span class="n">niqlow</span> that lets a user design and solve a discrete dynamic programming. It is written in Ox and requires Ox to run.  The user <em>imports</em> <span class="n">DDP</span> into their Ox program to use it.</DD>
<LI>Why <em>Discrete</em> Dynamic Programming (DDP)?</LI>
<P/>
     <DD>DDP is a term used in <span class="n">niqlow</span> that applies to models that have discrete (as opposed to continuous) choices and/or states.  A model of decisions with discrete states (including as a very special case a single state/decisin) but continuous choices typically involves solving first order conditions or carrying non-linear optimization. <span class="n">DDP</span> does not incorporate continuous choice directly.</DD>
<P/>
<LI>What about continuous choice?</LI>
     <DD>Optimization over continually varying controls (and solving non-linear systems) is the topic of the <span class="n"><a href="../FiveO/default.html">FiveO</a></span> component <span class="n">niqlow</span>. Methods that embed continuous choice inside a discrete state spaces is therefore a hybrid of the two parts of <span class="n">niqlow</span>. </DD>
<P/>
<LI>What does this first part of the document do?</LI>
    <DD>It presents DDP from scratch starting with simple static choices then building up to dynamics, and shows how the key concepts of DDP are represented in <span class="n">DDP</span>.</DD>
<LI>What does part 1 of the document <em>not</em> do?</LI>
<P/>
    <DD>It does <em>not</em> teach computer programming or the Ox programming language. It also does <em>not</em> present the general DDP framework nor the technical elements of <span class="n">DDP</span> in a systematic way. If you are already comfortable with Ox and with dynamic programming, then you might find this document too slow.  The <a href="#tech">second part</a> below may be a better place to start (and that includes links to examples the presume more background knowledge than what is presented here).</DD>
<P/>
<LI>What about estimation?</LI>
<P/>
    <DD>Within <span class="n">niqlow</span>, <em>estimation</em> refers to finding statistical estimates (guesses) of parameters of the model using data that is assumed to be generated by the model. A parameter is a quantity that enters the model and whose value is determined outside the model (an exogenous or so-called structural parameter). Parameters are typically continuous values and so guessing or estimating them is typically carried out using algorithms in <span class="n"><a href="../FiveO/default.html">FiveO</a></span>. <span class="n">DDP</span> includes methods to handle data generated by the model or created outside but assumed to be generated from the model. This document does not concern itself with DDP data and estimation, which is discussed in <a href="Data.ox.html">Data</a>.</DD>
</OL>
<P/>
<h1>Foundations of DDP</h1>
<P/>
<OL class="body">
<P/>
<a name="DC"><LI>Discrete Choice</LI></a>
<P/>
<p>We start with  <em>choice</em> without states and dynamics.  We start with such a simple  static choice framework that it might seem trivial. The computer code shown to implement the model will appear to be overkill.  Bear with this simple start because you will see that all the elements of dynamic programming in many areas of economics can be handled easily within the framework.</p>
<P/>
<h2>The choice set</h2>
<P/>
<p>Choice is simply picking from a set of options.  We will let <span class="expressions">\(A\)</span> denote the finite set of possible actions to choose among.   And we will let an arbitrary element of <span class="expressions">\(A\)</span> be denoted <span class="expressions">\(\alpha\)</span>.  We will work through a couple basic choices:</p>
<P/>
<DT>Example 1A. Bob wants to go to university. Four have accepted him. These four places make up his discrete choice set:</DT>
<DD><span class="expressions">\(A = \)</span> {<code>Harvard, Yale, Queen's, McGill</code>}.  </DD>
<DD>We sometimes need a symbol for the number of options available and will use <code>J</code> for that purpose.  So for Bob, <code>J=4</code>. Of course there are other schools but Bob either did not apply or did not get in to them.  They  are <em>infeasible</em> choices. <span class="expressions">\(A\)</span> is his feasible choice set.</DD>
<DD>While <span class="expressions">\(\alpha\)</span> stands for any of the feasible actions, at some point Bob makes a choice. The school he chooses becomes special, and we distinguish the choice made from the options considered. If Bob chooses to go to Yale we would write <span class="expressions">\(\alpha^\star = \)</span> <code>Yale</code>.   In these notes things marked with <span class="expressions">\(\star\)</span> are typically related to the optimal choice. </DD>
<P/>
In some cases we might use discrete choice to approximate a continuous choice, so here is another example:
<DT>Example 2A.  Tommy is choosing how much time to study for a test the next day.</DT>
<P/>
    <DD>So the choice might be any number between 0 and, say, 12 hours. It would then be natural (or convenient or intuitive) to think of the choice set as <span class="expressions">\(A=[0,12]\)</span>, to let <span class="expressions">\(\alpha\)</span> denote any real number in that range, and to characterize the optimal choice with a first-order condition o marginal utility, <span class="expressions">\(U^{\,\prime}(\alpha).\)</span>  However, for various reasons, we stick with a discrete choice set to model Tommy's choice.  To be specific, we might think of him as studying for a number of hours: <span class="expressions">\(A=\)</span> {<code>0,1,2,&hellip;,12</code>}.   In this case <span class="expressions">\(J=13\)</span>.  </DD>
<P/>
    <DD><details><summary>What if Tommy were an actual person (say in a data set) and studied 2.2 hours?  </summary> Let's write <span class="expressions">\(\hat\alpha\)</span> = 2.2 to denote the observed choice.  This would suggest that Tommy did not choose among whole hours. This difference between actual and model behaviour might not matter to us (this choice might be part of a much bigger model).  So we might approximate by rounding to the nearest element of <span class="expressions">\(A\)</span> so that <span class="expressions">\(\alpha^\star=\)</span><code>2</code> would explain Tommy's choice. This is getting ahead of ourselves, but you might be wondering about accepting 2 to stand for an observed choice of 2.2.  One way in which empirical DDP work deals with this kind of discrepancy is to assume that there is <em>measurement error</em> in the data.  So the difference <span class="expressions">\(d =\)</span> <code>2.2-2</code> would be attributed to the measurement error (i.e. Tommy says he worked 2.2 hours but he really worked exactly 2 hours).  This can bother empirical people who are used to working with, say, regression models.  But the error in a regression model plays a similar role in "explaining" the difference between observed and predicted values.  </details></DD>
<P/>
<h2>Utility</h2>
<P/>
<DT>How did Bob choose Yale? And how Tommy chose 2 hours (according to our discrete approximation)?</DT>
    <DD>Perhaps they considered all the pros and cons of their options and carefully weighed the factors to determine which was best.  Or maybe they threw a dart at the wall.</DD>
<P/>
<DT>Whatever process they used ...</DT>
    <DD>we are not concerned with <em>how</em> the choice was made, but that a chose was made based on a  <em>utility</em> associated with each action.
    <details class="aside"><summary>Optimality and <em>function</em></summary>Of course, much of social science concerns choices that are not optimal, but for our purposes what people do will always be optimal for them given their options and preferences. Also, a teacher of mine emphasized that utility is by definition a function, so saying "utility function" is redundant. It would be like saying "truck vehicle" instead of simply "truck," because by definition a truck is a vehicle.</details></DD>
<P/>
<DT>Let <span class="expressions">\(U(\alpha)\)</span> denote Bob's utility for school <span class="expressions">\(\alpha\)</span>.</DT>
    <DD><span class="expressions">\(U(\alpha)\)</span> is just a real number associated with <span class="expressions">\(\alpha\)</span>. Bob can have utility for options that are not feasible (not in <span class="expressions">\(A\)</span>), such as U(<code>UCLA</code>), but we don't have a complete model until utility is defined for each element of <span class="expressions">\(A\)</span>.  So, we can get fancy and write <span class="expressions">\(U: A\,\to\, \Re\)</span>.  But since <span class="expressions">\(A\)</span> is a discrete set, <span class="expressions">\(U\)</span> is really just four numbers, one for each feasible school.</DD>
<P/>
<DT>For example, Bob would set <span class="expressions">\(\alpha^\star=\)</span> <code>Yale</code> if his utility were:</DT>
<DD><pre>
&alpha;               U(&alpha;)
----------------------
Harvard        -20
Yale            4.2
Queen's        e<sup>1</sup>
McGill          ln(0.0009)
</pre>
Many other utility levels would explain <code>Yale</code> as an optimal choice.</DD>
<P/>
<h2>Multiple Dimensions</h2>
<P/>
<DT>In many cases we want to model choices in more than one dimension.</DT>
  <DD>Bob was making a choice in the <em>school</em> dimension.  But he might also be making decisions in other dimensions, such as <em>major</em>, <em>roommate</em>, etc. Before showing <span class="n">DDP</span> code for Bob's choice let's have him decide his major at the same time. One way to do this is to convert a choice in several (discrete) dimensions into a one dimensional choice by simply listing all the possible combinations. Another way is to make <span class="expressions">\(\alpha\)</span> a vector not a scalar. We illustrate both ways in the next example.</DD>
<P/>
<DT>Example 1B: Bob is choosing both a university <em>and</em> a major. Which major is best might depend on which school he chooses.  </DT>
    <DD>For simplicity, suppose Bob's parents have told him he has to choose either <code>Economics</code> or <code>Physics</code>.  </DD>
    <DD>We could simply expand his choice set as follows: <code>{Harvard-Econ,Harvard-Physics,Yale-Econ,Yale-Physics,Queen's-Econ,Queen's-Econ,McGill-Econ,McGill-Physics}</code>.  Utility is then a number assigned to each of these <code>J=8</code> options.  This collapses a choice in two dimensions into a longer one-dimensional choice.</DD>
    <DD>However, it can be more convenient not to collapse the two dimensions, but to consider them separate but simultaneous.  In this case, we can let school be the row and major the column:
<pre>
BOB'S UTILITY ON THE FEASIBLE MAJOR-SCHOOL CHOICE SET
                    Econ                    Physics
Harvard           -20                       -18
Yale              4.2                      -0.6
Queen's           1.5                        3.2
McGill           -25                        -0.5
</pre></DD>
<DD>Apparently Yale has a good Econ program, but if Bob had not gotten into Yale he would have chosen Queen's and majored in Physics.</DD>
<P/>
<DT>We can also keep the dimensions separate by letting <span class="expressions">\(\alpha\)</span> be a <em>vector</em> of action variables: <span class="expressions">\(\alpha = (a_0, a_1, \dots, a_{D-1})\)</span>.</DT>
<P/>
<DD>In the major-school choice, <code>D=2</code>, because there are two dimensions of choice,  <span class="expressions">\(a_0\)</span> is the major choice, and <span class="expressions">\(a_1\)</span> the major choice.  <details class="aside"><summary>Start at 0?</summary>We start counting at 0 because that is the way counting is done in many computer languages.  So doing so now may avoid some confusion later when we see code.</details>
Although using rows and columns to represent two different action variables is clear, it is not very helpful when there are three or more variables.</DD>
<P/>
<DT>We can combine the list version with the vector version to get something like this:</DT>
<DD><pre>
BOB'S UTILITY USING ACTION VARIABLES
     &alpha;
a<sub>0</sub>            a<sub>1</sub>                U(&alpha;)
--------------------------------------
Econ          Harvard          -20
Physics       Harvard          -18
Econ          Yale              4.2
Physics       Yale             -0.6
Econ          Queen's           1.5
Physics       Queen's           3.2
Econ          McGill           -25
Physics       McGill           -0.5
</pre></DD>
<P/>
<DT>In  <span class="n">DDP</span>:  </DT>
<DD>the action vector is built by adding <a href="Variables.ox.html#ActionVariable">ActionVariable</a>s to it as part of the set-up of the model.  The creation of the list of actions as shown above is done for you by <span class="n">DDP</span> as you add action variables to the action vector. </DD>
<DD>As the user you only have to concern yourself with which variables are chosen and how many different values they take on. You can provide value labels for each action variable you add to the model.  In the example above "Econ" would be a label for an integer option value.</DD>
<P/>
<h2>Full Description of Bob's Problem</h2>
<P/>
As with the first example, each row is an action <span class="expressions">\(\alpha\)</span>, but it is associated with values of two action variables. Labelling the choices certainly helps to understand them, but a computer program that is designed to handle any kinds of choices can hard-code labels like "Harvard" and use it to refer to utility.  Hopefully it is clear that in the abstract the choice over the four universities is just a case of choosing among 4 possibilities.  The labels are helpful to us, but generically the choices can just be numbered 0, 1, 2, and 3.</DD>
<P/>
<DT>The school-major choice can be describe with action variables that are just integers along with labels for each value.</DT>
<dd><pre>
&alpha; =  (a<sub>0</sub> a<sub>1</sub>)
<P/>
Index     Label        Options     Choice Set     Value Labels
-------------------------------------------------------------------------
0          Major             2         0 &hellip; 1       Econ, Physics
1          School           4         0 &hellip; 3       Harvard,Yale,Queen's,McGill
<P/>
A = [0&hellip;1] &times; [0&hellip;3]
a<sub>0</sub>     a<sub>1</sub>          U(&alpha;)
-------------------------
0       0        -20
1       0        -18
0       1         4.2
1       1        -0.6
0       2         1.5
1       2         3.2
0       3        -25
1       3        -0.5
</pre></DD>
<P/>
<DT>Bob's Choice</DT>
<P/>
<DD>So far we just have 8 options with arbitrary values (utilities) assigned to them.  And it turns out that the maximum of those utilities is 4.2 for the action of choosing <code>Yale</code> and <code>Econ</code>.
<span class="equation">$$\eqalign{
\alpha^\star &\equiv \arg\max_{A}U(\alpha) = (0,1) = (\hbox{Econ},\hbox{Yale})\cr
EV &\equiv \max_{A} U(\alpha) = U(\alpha^\star) = 4.2.\cr}$$</span>
</DD>
<P/>
<DT>V and EV</DT>
<P/>
<DD>In microeconomics the highest utility possible is named <em>indirect utility</em>, but in dynamic programming it is usually called the (optimal) <em>value</em> of a state. So, above, <span class="expressions">\(V\)</span> is used to represent the value Bob receives from his choice once optimized.  </DD>
<P/>
<DD>We use <span class="expressions">\(EV\)</span> and not just <span class="expressions">\(V\)</span> in anticipation that there can be elements of uncertainty in the choice.   In particular, the person deciding will know everything up until today when the choice is made, but they will have to make that choice without knowing everything that will happen in the future (tomorrow).  <span class="expressions">\(EV\)</span> will end up being good notation since when, deciding today, they will have to average (take the <b>E</b>expectation of) their optimal choices made tomorrow when they have more information than they have now.
<details class="aside"><summary>Ties</summary><DD>The notation also assumes there are no ties, the optimal choice is unique.  That won't be necessary.  Ties are handled properly by <span class="n">DDP</span>,  but assuming no  ties does make the example simpler.</DD></details></DD>
<P/>
<h2>Coding Bob's Choice in <span class="n">DDP</span></h2>
<P/>
<DT>Bob's choice is simple.</DT>
  <DD>Simply look at the 8 numbers that make up his utility.  </DD>
  <DD>Find the biggest number.  </DD>
  <DD>Look up the labels associated with that action. Finished.  </DD>
<P/>
    <DD>These operations could easily be done by hand, in a spreadsheet, or even in Ox using its built in <a href=""><code>maxc()</code> and <code>maxcindex()</code></a> routines. When you look at the code for Bob's Choice in <span class="n">DDP</span> you will see that it has some elements that are not obvious.  Indeed, it relies on some sophisticated features of the Ox programming language.  Even if you have done some programming in similar languages such as Matlab, Python or R, your reaction may be: that is a lot of complexity for such a simple choice.  And you are right!</DD>
<P/>
    <DD>However, hopefully you see some logic in the complications so that the code is not completely unrelated to the problem as you understand it.  And you might see that some of the complication is there to make real models easier to build than if simple tools only were used.</DD>
<P/>
<details><summary>The Code.</summary>
    <DT>Source: <a href="javascript:popoxcode('../../examples/DDP/BobsChoice.ox');">niqlow/examples/DDP/BobsChoice.ox</a></DT>
    <DD><pre><object width="75%" height="300" type="text/Plain" data="../../examples/DDP/BobsChoice.ox" border="1" ><p style="font-size:14pt"></object>
</pre></dd></details>
<P/>
    <DT>Line-by-line explanation of the code</DT>
    <DT> <code>&#35;import &hellip;</code>:</DT>
    <DD>Import is a way to tell Ox that you are using code that is not part of this file. In this, case the program is importing <span class="n">DDP</span>!
    <details class="aside"><summary>&#35;import and &#35;include</summary> If you want to know more, see <a href="http://www.doornik.com/ox/oxtutlan.html#ox_tutlan_link">Multiple files in Ox</a>. Most examples shown in Ox start with <code>#include "oxstd.h"</code>.  That is done in <span class="n">DDP</span> so it is not necessary to do it explicitly.</details></DD>
<P/>
    <DT><code>class &hellip; { &hellip; }</code>: Class Declaration</DT>
    <DD>A user's model is represented by a <code>class</code>, which is way to combine data and functions that work on the data in one package.  This is called <em>object oriented programming</em>. See <a href="http://www.doornik.com/ox/oxtutlan.html#ox_tutlan_oo">OOP in Ox</a> if you are familiar with objects already and want to know how they work in Ox.</DD>
<P/>
    <details class="aside"><summary>Declare and  Define</summary>In a programming language like Ox your program will have different things in it.  The language has to know what things are in the program, and this is the idea of <em>declaring</em> something.  This is like listing a chapter of a book in a table of contents.  But Ox also has to know what the things are, which is like the contents of the chapter.  This is <em>defining</em>. How things are declared and defined in Ox depends on the kind of thing it is.  And, in some cases your program might declare and define something at the same time not separately.</details>
    <DD>If you are not used to OOP, it can be mysterious and confusing.  And Ox can be used without OOP, but it is the way in which <span class="n">DDP</span> lets the user develop an economic model so it is essential for our purpose.  So here is the basic idea.  What is happening is the program is telling Ox that a new class is going to appear in this program.  A class is a description of a kind of thing (objects).  These lines are describing the class for Ox, which is just a list of the variables (also called <em>members</em> and functions (also called <em>methods</em>) that make up the class.  Every class has a name, and the code gives this class the name <code>BobsChoice</code>.  </DD>
<P/>
    <DD>One of the powerful (but complicated and confusing) features of OOP is that one class can be based on another class that is already defined.  In this case, <code>BobsChoice</code> is based on a class called <a href="Bellman.ox.html#OneStateModel">OneStateModel</a> which is defined in <span class="n">DDP</span>.  The name is meant to imply that this model is really simple.  In real dynamic programs in which there are many states at which the person is making choices. Here Bob makes a choice once, so there is just one state. </DD>
<P/>
    <DD>In turn, <code>OneStateModel</code> is a class based on other classes which are more flexible. The advantage of having a special class for a simple one state model is that some things can be done for the user.</DD>
<P/>
    <DD>The choice involves two action variables, and this class makes room for them with the <code>static decl</code> statement.  The names are short but understandable in the context.  <code>decl</code> is short for <em>declare</em> and does just make room for two things.  What they end up holding is determined by other parts of the program.</DD>
<P/>
    <DD>The <code>static</code> tag is important but at this point it is not necessary to understand why it is there.  It does not have anything to do with the fact that Bob's choice is a static choice.</DD>
<P/>
    <DD>The <code>BobsChoice</code> class also has two functions in it: <code>Decide()</code> and <code>Utility()</code>.  Note that <code>Decide()</code> is declared <code>static</code> but <code>Utility()</code> is not.  Again, this is not important to understand yet.  And like <code>decl</code>, listing functions in the declaration of  a class does not say anything about what they do.  The have to be <em>defined</em> later.</DD>
<P/>
    <DT><code>main(){&hellip;}</code></DT>
    <DD>Every Ox program has to have a function (or routine) called <code>main()</code>. This is where the Ox program actually starts.  Even though the class declaration comes first in the file, it is <code>main()</code> that is the first thing to happen. I have written the code so that <code>main()</code> just does one thing: it asks that <code>Decide()</code> be executed. Once it is finished <code>main()</code> is finished (no other statements  appear inside <code>main()</code>).  Most experienced programmers make their main routines pretty simple. </DD>
<P/>
    <DT><code>BobsChoice::Decide(){&hellip;}</code></DT>
<P/>
    <DD>Above the class declaration said that a routine belonging to <code>BobsChoice</code> and named <code>Decide()</code> would appear, and these lines <em>define</em> what this routine does.  It is the lines of code that are executed when <code>main()</code> refers to the function.  </DD>
<P/>
    <DD><code>Decide()</code> does four things.  That is, it has four statements each ending with <code>;</code>.  The first two say that <code>maj</code> and <code>sch</code> will each contain an <em>action variable</em>.  This routine, <code>ActionVariables()</code>, is part of <span class="n">DDP</span>, so it would not work to call it if we had not imported <span class="n">DDP</span>.</DD>
<P/>
    <DD>Hopefully, you can see that labels attached to the two variables are "major" and "school", respectively. Like nearly all computer languages, Ox asks you to put text inside quotes.  So <code>maj</code>, which is not in quotes is referring to a variable with that name. Ox will make room for because it was <code>decl</code>ared in the class declaration. However, the action variable has a name "major" which is just those characters not a variable or a routine.  </DD>
<P/>
    <DD>In Bob's choice he had only two majors to choose from.  We could write <code>ActionVariable("major",2)</code> and this would mean <code>maj</code> would hold an action variable with two possible values.  This would not given meaningful names (or labels) to the two options: they are just be coded as <code>0</code> and <code>1</code>.  By sending two strings in quotes instead of <code>2</code> this gives each major a meaningful label.  The routine <code>ActionVariable()</code> counts the labels and knows that there are two choices.  They are still coded as <code>0</code> and <code>1</code> but those codes now have the labels "Econ" and "Physics". This will make some output easier to read.</DD>
<P/>
    <DD>The variable <code>sch</code> will contain another action variable that takes on four values (because a list of four labels are sent).</DD>
<P/>
    <DD>The next statement calls a routine with the name <code>Initialize()</code>.  This routine is part of <span class="n">DDP</span> not Ox itself.  As the name implies, it initializes or sets up the problem meant to solve Bob's Choice.  Four things are sent to <code>Initialize()</code>.  The order is important.  The first is a bit mysterious: <code>new BobsChoice()</code>. The second is the number <code>0</code>.  As with some other parts of the code, it is not <em>yet</em> important to understand how these are chosen or used for this problem. In particular, to explain what <code>new BobsChoice()</code> means and why it is there would take us on a tangent that is not necessary for now. </DD>
<P/>
    <DD>However, it is important to see that the variables <code>maj</code> and <code>sch</code> are sent as well.  This is how the <code>OneStateModel</code> in <span class="n">DDP</span> knows what action variables are part of the model.</DD>
<P/>
    <DD>You may think that the line that says <code>maj</code> contains an <code>ActionVariable</code> would add <code>maj</code> to the model automatically.  It would be possible to make it work that way, but for other aspects of DP models (namely state variables) it is easier to separate the creation of a variable from including it in the model. So to be consistent, the same procedure is followed for action variables.</DD>
<P/>
    <DD>The last thing <code>Decide()</code> does is call a routine <a href="Methods.ox.html#VISolve">VISolve</a>(), where "VI" stands for "value iteration."   That is a technique for solving a dynamic programming problem. This one state model is the most simple example of a DP problem, but one part of value iteration is to solve for the optimal choice at each state.  This is where <span class="n">DDP</span> will actually solve Bob's problem.</DD>
<P/>
    <DT><code>BobsChoice::Utility(){ &hellip;}</code></DT>
    <DD>Any DDP problem has a utility, and here is Bob's.  In general, the utility has to return (send back to where the utility was called) a vector in the same order as the actions <span class="expressions">\(\alpha\)</span>.  The use of <code>&lt; -20; &hellip; &gt;</code> is the way Ox lets you hard-code a vector or matrix of numbers.   So the list of utilities above has been copied and written in that format.  We should definitely check that the action variables are organized so that the numbers match up: we wouldn't want Bob to mistakenly go to McGill and major in Physics.</DD>
    <DD>Usually <code>Utility()</code> is a function of actions (and later states) and <em>parameters</em> of the problem.  So Bob's utility is not at all typical.  Later examples in the documentation and exercises will have utilities that are more like what show up in real DP problems.</DD>
<P/>
<h2>Run the program and look at the output.</h2>
<P/>
Getting Ox to run <code>BobsChoice.ox</code> and be able to use <span class="n">DDP</span> is not hard  if you are used to doing this kind of thing.  Otherwise, it can be a pain for the first few times you try something.  Go <a href="">here ???</a> for some help.</p>
<P/>
<details><summary><b>The output you get should look something like this:</b></summary>
<DT>Source: <a href="javascript:popoxcode('../../examples/output/BobsChoice.output.txt');">examples/output/BobsChoice.output.txt</a>.</DT>
<DD><pre><object width="75%" height="300" type="text/plain" data="../../examples/output/BobsChoice.output.txt" border="1" ><p style="font-size:14pt"></object>
</pre></dd></details>
<P/>
<DT>Selected Output</DT>
That output contains a lot of information that is not relevant to this simple case, so here are selected parts of the output:
<details><summary>Selected Output</summary><dd><pre>
-------------------- DP Model Summary ------------------------
4. ACTION VARIABLES
   Number of Distinct action vectors: 8
         major  schoo
    a.N      2      4
<P/>
6. FEASIBLE ACTION SETS
<P/>
    alpha       A[0]
    ----------------------
    (00)          X        -Econ-Harvard
    (10)          X        -Physics-Harvard
    (01)          X        -Econ-Yale
    (11)          X        -Physics-Yale
    (02)          X        -Econ-Queen's
    (12)          X        -Physics-Queen's
    (03)          X        -Econ-McGill
    (13)          X        -Physics-McGill
   &#35;States        1
    ----------------------
    Key: X = row vector is feasible. - = infeasible
<P/>
     Value of States and Choice Probabilities
     ------------------------------------------------------------------------------
    Indx   I   T   A   q   t     r     f       EV      |Choice Probabilities:
       0   1   0   0   0   0     0     0       4.200000 0.000000 0.000000 1.000000 0.000000 0.000000 0.000000 0.000000 0.000000
     ------------------------------------------------------------------------------
</pre></dd></details>
<P/>
<DT>Explanation of Relevant Output</DT>
    <DD>The "DP Model Summary" is produced by <code>Initialize()</code> after everything is set up.  (In models that have more than one state <code>Initialize()</code> only does some of the things done here.  The rest are done by a routine that it calls but typically is called from the user's program.) Part 4 of the summary shows you the action vector <span class="expressions">\(\alpha\)</span>.  It shows that the new action variables assigned to <code>maj</code> and <code>sch</code> were added to the vector and that together they create 8 different options.</DD>
<P/>
    <dd>Part 6 shows you the feasible action set <span class="expressions">\(A\)</span>.  It shows you each action <span class="expressions">\(\alpha\)</span> as two integer values and the labels that go along with them.  Recall that <code>Yale-Econ</code> was the third option in the list and it appears that <span class="expressions">\(A\)</span> was set up so that it matches up with the vector in <code>Utility()</code> (but make sure!). </dd>
    <DD>In real DP problems the actions that are feasible can depend on which state the system is at.  So this output is set up to show you the different values of <span class="expressions">\(A\)</span> in the model.  But with one state only there is one feasible set only.  Thus <span class="expressions">\(A\)</span> is called <code>A[0]</code>, because in some cases there will be <code>A[1]</code> and so forth.</DD>
    <DD><code>VISolve()</code> produces the table of values and choice probabilities.  The first 8 columns really don't apply to this simple model because there is only one state.  The table is designed to show the value of each state in the model.</DD>
    <DD>The parts that matter are the <code>EV</code> and <code>Choice Probabilities</code>.  Recall the <code>EV</code> is the DP version of indirect utility, and we know that the best Bob can do is 4.2.  And the optimal choice is the third element of <span class="expressions">\(A\)</span>, which Bob should chose with probability 1.0.  All the other options are sub-optimal and are chosen with 0 probability.</DD>
<P/>
<h2>Summary</h2>
<P/>
Here is what is important to gather from the example and code above.
<P/>
<OL class="steps">
    <LI>At the heart of a <span class="n">DDP</span> model is discrete choice.  Discrete dynamic programming links together many different discrete choices that are connected by state variables that evolve base on the choices made.</LI>
    <LI>In <span class="n">DDP</span> you build your model by defining a <em>class</em> that is derived from one of the built-in <a href="Bellman.ox.html#Bellman">Bellman</a> classes.  The simplest model to build from is <a href="Bellman.ox.html#OneStateModel">OneStateModel</a>, which is a single discrete choice (one state, no dynamics, etc).</LI>
    <LI>The discrete choice <span class="expressions">\(\alpha\)</span> is a vector of action variables, each taking on a finite number of values.  Your model builds <span class="expressions">\(\alpha\)</span> by creating new <a href="Variables.ox.html#ActionVariable">ActionVariable</a>s and adding them.  With a <a href="Bellman.ox.html#OneStateModel">OneStateModel</a> you send all the action variables to <a href="Bellman.ox.html#OneStateModel___Initialize">Initialize</a>().  In general your code will add action variables yourself after calling <code>Initialize()</code>.</LI>
    <LI>Your model must supply a <code>Utility()</code> which returns a vector of numbers corresponding to the elements of the feasible set <span class="expressions">\(A\)</span>.</LI>
    <LI>The optimal choice for the model can be found by calling <a href="Methods.ox.html#VISolve">VISolve</a>(), which does several things. For a one state model it simply finds <span class="expressions">\(\alpha^\star\)</span>, the maximizing action.  </LI>
</OL>
<P/>
<h2>Exercises</h2>
<OL class="steps">
<LI>Make a couple copies of <code>BobsChoice.ox</code> and experiment with them as follows. (Do not change the original file's contents so you still have it.)</LI>
    <OL class="substeps">
    <LI>Change the utility vector so that two options tie as the optimal choice.  See what happens to the output.</LI>
    <LI>Add a third option to the major choices, <code>Psycho</code>.  Modify utility so that <code>McGill-Psychology</code> is the optimal choice. Run the program and verify the changes.  Try to correct any errors in the code that Ox complains about.</LI>
    <LI>Modify your model to include a third action variable: <code>res</code> which is a binary choice to either live <code>ON</code> campus or <code>OFF</code>.  Tweak the utility so that Yale-Econ is still optimal as is <code>ON</code>.  Run the code and confirm your changes.</LI>
    </OL>
    <LI>Tommy's Choice: Make a copy of the file named <code>TommysChoice.ox</code>.</LI>
    <OL class="substeps">
    <LI>>Change the name of the <code>class</code> defined in the file.  Delete <code>major</code> and rename <code>school</code> to <code>hours</code>.  Use this create the action variable: <code>hours = new ActionVariable("h",13)</code>.  Because it is not useful to label hours in the same way that schools and majors, just send the number of options (no value labels will be created).</li>
    <li>Change the utility so that it is a function of the <code>hours</code> not just a list of arbitrary numbers.  In particular, if <span class="expressions">\(U(h) = -(h-2.2)^2\)</span> then the optimal discrete choice will be <span class="expressions">\(h* = 2\)</span>, which is the 3rd value that <code>hours</code> takes on (0, 1, 2, etc). The vector of values that <code>hours</code> takes on can be accessed as <code>CV(hours)</code>.
    <DD>So, modify <code>Utility()</code> to have this form:
<pre>
TommysChoice::Utility() {
    return -sqr(CV(hours)-2.2);
    }
</pre>
Debug your program  and run it until it produces the correct values of <code>EV</code> and choice probabilities.</DD></li>
<P/>
<LI>Continuing with <code>TommysChoice</code>, change the utility to be <span class="expressions">\(-(h-2.5)^2\)</span>, which induces a tie between 2 and 3.  Inspect the output.</LI>
</OL>
</OL>
<P/>
<a name="SP"><LI>Choice Probabilities</LI></a>
<P/>
<p>The examples above are not particularly interesting models of behaviour.  For one thing, the utility values are arbitrary, whereas a good model would relate utility to observable characteristics of the chooser and the choices.  And, second, suppose Bob did not choose Yale-Econ?  Then our model is incorrect.  This is not surprising because we typically are not modeling a single person's choice and we will never have access to the utility of all the options.  Instead, real discrete models provide a probability of people making choices not a 0/1 outcome.</p>
<P/>
<DT>The <em>logit model</em> of choice includes a continuous random variable in the <em>value</em> of a choice:</DT>
<span class="equation">$$v(\alpha) = U(\alpha) + z_\alpha.$$</span>
<DD>The extra component <span class="expressions">\(z_\alpha\)</span> is assumed to follow the <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Type I Extreme Value (Gumbel) distribution</a>.  Across options <span class="expressions">\(z_\alpha\)</span> values are independent.  The term <span class="expressions">\(U(\alpha)\)</span> is a shortened version of name of the routine that is part the model in <span class="disp">DDP</span>.   The user's coding would be the same <em>and the term <span class="expressions">\(z_\alpha\)</span> is not explicitly added to it.</em></DD>
<P/>
<DT>The <em>probit model</em> of choice makes a different distributional assumption</DT>
    <span class="equation">$$v(\alpha) = U(\alpha) + z_\alpha, \qquad z_\alpha \sim N(0,1).$$</span>
<P/>
<DD>Across options <span class="expressions">\(z_\alpha\)</span> values are independent.  </DD>
<P/>
<DT>Conceptually ...</DT>
 <DD>Bob sees both <code>Utility()</code> (the part we see or assume or estimate) and the vector of values of <span class="expressions">\(z_\alpha\)</span> (the part we don't see but we assume follows a convenient distribution). With his information, Bob still simply chooses the <span class="expressions">\(\alpha\)</span> that maximizes <span class="expressions">\(v(\alpha)\)</span>.  But since we don't see <span class="expressions">\(z_\alpha\)</span>, any of the options might be optimal to Bob.  We get different <em>probabilities</em> of choices being optimal.   The probabilities depend on the observed vector of utilities, to the 0/1 vector of ouptut above would become a  vector of numbers between 0 and 1.  Since <code>Yale-Econ</code> has the highest value of <code>Utility()</code> it will have the greatest probability of being chosen, but the probability will not be 1.0.</DD>
<P/>
<DT>Choice Probabilities under logit</DT>
<DD>Logit:
<span class="equation">$$P(\alpha) = {e^{U(\alpha)}  \over \sum_{\alpha'\in A} e^{U(\alpha')}} = {e^{U(\alpha)-U(\alpha^\star)}  \over \sum_{\alpha'\in A} e^{U(\alpha')-U(\alpha^\star)}}$$</span>
The second version expresses the probability in relative utility loss compared to the optimal choice <span class="expressions">\(\alpha^\star\)</span>.</DD>
<P/>
<DD>Following the literature, <span class="n">DDP</span> generalizes the logit expression to allow the amount of smoothing to be chosen by setting a parameter <span class="expressions">\(\rho\)</span>:
<span class="equation">$$P(\alpha)  = {e^{\rho\left(U(\alpha)-U(\alpha^\star)\right)}  \over \sum_{\alpha'\in A} e^{\rho\left(U(\alpha')-U(\alpha^\star)\right)}}$$</span>
As <span class="expressions">\(\rho\to \infty\)</span> the probability approach the non-smooth values.  As <span class="expressions">\(\rho\to 0\)</span> the smoothing becomes complete and each feasible choice is equally likely.</DD>
<P/>
<details><summary>See Train (2001)</summary>
<a href="">Train (2001)</a> explains all aspects of discrete choice with an emphasis on econometric applications.   Readers are encouraged to refer to Train () for in-depth discussion of the static case.  We start even simpler than Train does.  Eventually the set up starts to look like the material in Train, but we move to dynamic choice before going into the depths of various static choice models that Train does.</details>
<P/>
<DT>Choice Probabilities under Probit</DT>
<DD>Independent Probit:
<span class="equation">$$\eqalign{ P(\alpha) &= Prob( U(\alpha) \ge U(\alpha') ), \forall \alpha'\in A.\cr
&= \prod_{\alpha'\ne\alpha}\ \Phi\left( \rho(U(\alpha)-U(\alpha'))\right)\cr}$$</span>
Again, <span class="n">DDP</span> generalizes the standard probit by varying how important the deterministic component to utility is to choice. The smoothing parameter <span class="expressions">\(\rho = 1/\sigma\)</span> and <span class="expressions">\(\sigma\)</span> is the standard deviation of the difference <span class="expressions">\(z_\alpha-z_{\alpha'}\)</span>.  This smoothing would not be identified if/when <span class="expressions">\(U(\alpha)\)</span> is based on estimated coefficients on variables related to <span class="expressions">\(\alpha\)</span>. </DD>
<P/>
<DD>Correlated Probit:</DD>
<P/>
<h2>Smoothing Choice Probabilities in <span class="n">DDP</span></h2>
<P/>
<DT>Select a Smoothing Method</DT>
<DD>Recall that Bob's Choice was derived from the <a href="Bellman.ox.html#OneStateModel">OneStateModel</a> class of problems. That class is in turn a special case (a derived class) of the <a href="Bellman.ox.html#ExPostSmoothing">ExPostSmoothing</a> class. The method for smoothing for the one state model is made at this point in the code:
<pre>Initialize(new BobsChoice(),0,maj,sch)
</pre></DD>
<P/>
<DD>As you can see from the documentation of <a href="Bellman.ox.html#OneStateModel___Initialize">Initialize</a>(), the second argument is the smoothing method choice. To avoid having to explain everything at once, it was set to 0 in the code, which is the default choice in the general <code>ExPostSmoothing</code> class.</DD>
<P/>
<DD>Another more descriptive way to send 0 is to use a name for zero that is defined in <span class="n">DDP</span> in order to make the internal code easier to follow and debug:
<pre>Initialize(new BobsChoice(),NoSmoothing,maj,sch)
</pre>
Here <code>NoSmoothing</code> is really just a name for the number 0.  It is one of the <a href="DDPShared.ox.html#SmoothingMethods">SmoothingMethods</a> that are really just names for the integers 0, 1, and 2. The code for the ExPostSmoothing class of problems will look at the method sent to it and decide if and how to smooth the resulting choice probabilities.</DD>
<P/>
<DT>Logit Smoothing</DT>
<DD>To make Bob's Choice a logit model, just modify the code to ask for logit smoothing:
<pre>Initialize(new BobsChoice(),LogitKernel,maj,sch)
</pre></DD>
<details><summary>What does "kernel" mean?</summary>  Essentially, a kernel means "smoothing."  You can learn more at <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation">Kernel's wikipedia page</a></details>
<P/>
<DT>Probit Smoothing</DT>
<DD>To make Bob's Choice a probit model, just modify the code to ask for probit smoothing:
<pre>Initialize(new BobsChoice(),GaussKernel,maj,sch)
</pre></DD>
<details><summary>Why Gauss?</summary>Because the normal distribution is also referred to as the Gaussian distribution.</details>
<P/>
<details><summary>Why Ex Post?</summary>
<DD>In the standard discrete choice econometric model the error term <span class="expressions">\(z\)</span> is considered something known/observed by the chooser but not us.  In a more complicated situation, namely a dynamic one, Bob might make other choices before he knows the value of <span class="expressions">\(z\)</span>.  For example, <span class="expressions">\(z\)</span> may be observed when Bob wakes up in the morning but he had to make a choice the day before that was affected by what he does today.  In that case, the value of <span class="expressions">\(z\)</span> is not just smoothing the choice probability for our sake, it is also have a direct effect on other decisions.  </DD>
<DD><span class="n">DDP</span> distinguishes between choice probability smoothing for our own sake and real uncertainty that affects the chooser in a dynamic environment.</DD>
<DD>The case of <em> ex-post smoothing</em> is when <span class="expressions">\(z\)</span> is not a real part of Bob's environment.  He is really just making a deterministic choice based on <span class="expressions">\(U(\alpha)\)</span>.  But after his choice we smooth the probability as if he also add a value of <span class="expressions">\(z\)</span>.  </DD>
<DD>The case of <em>ex-ante</em> smoothing allows <span class="expressions">\(z\)</span> to play a role in other decisions made by Bob so it is a real "structural" error term.  </DD>
</details>
<P/>
<h2>Exercises</h2>
<OL class="steps">
<LI>Make a copy <code>BobsChoice.ox</code>.  Try the 3 different smoothing options discussed above.  Compare results.</LI>
<LI>Multiply all the utilities by 5.  Confirm that this does not change the optional choice when there is no smoothing.  What happens to choice probabilities when they are smoothed compared to the values in the previous exercise?</LI>
</OL>
<P/>
<a name="MS"><LI>Multiple States (Unlinked Choices)</LI></a>
<P/>
<p>Bob and Tommy's choices are one-time discrete choices.  By deriving a model of their choices from the <code>OneStateModel</code> class there was no possibility of dynamics or examining how the situation affects the choice.</p>
<P/>
<p>A <em>state</em> is a situation in which the agent makes a decision.  States differ from each other for various reasons. Utility may differ between states (leading to different decisions).  The feasible action set may differ between states. The effect of choices on what happens in the future may differ. Obviously the next step from a one-state model is a two-state model.  We then double the states again to consider a four-state static choice model.</p>
<P/>
<h2>The State Space</h2>
<P/>
<p>In dynamic programming the choice of action <span class="expressions">\(\alpha\)</span> is contingent on the state the chooser is in.  To store and solve models efficiently, <span class="n">DDP</span> will distinguish between types of states.  But a generic state will be denoted <span class="expressions">\(\theta\)</span>.  And the set of states the problem might be in is denoted <span class="expressions">\(\Theta\)</span>. As with feasible actions, the state space is constructed by adding state variables to it.   We illustrate this with simple extension of Bob's Choice.</p>
<P/>
<DT>Example 1C.  Bob is still deciding between schools and majors.  However, it is more complicated. He may or may not receive a scholarship to Queen's for which he is applied for.</DT>
<P/>
    <DD>Obviously Bob will make his decision based on whether he gets the scholarship or not, so the choice is <em>contingent</em> upon that.  We want the model to capture both choices.  One reason to do this is that we want to apply the model to data in which some people get scholarships and some don't.  The values of utility above will be the values without a scholarship.  Then in the event of getting the scholarship the utility of choosing Queen's goes up.</DD>
<P/>
    <DD>Scholarships are measured in dollars, but Bob's utility from getting and accepting a scholarship may be different that the monetary value.  However, to keep things simple we will assume that the utilities already listed are in money-equivalent values so we can add the value of the scholarship to them in order to compute net utility for Bob.  Specifically, the Queen's scholarship has a value of &#36;2,200 and utility is measured in thousands of dollars. So in the case of getting the scholarship and going to Queen's utility increases by 2.2.</DD>
<P/>
<h2>Coding Bob's New Choice</h2>
<P/>
<DT>State Variables</DT>
<P/>
    <DD>The <a href="Variables.ox.html#StateVariable">StateVariable</a> class is designed to represents state variables in a dynamic program.  Like action variables, the user's code creates state variable, stores them in a <code>static</code> member of the model class, and then adds them to the model. <span class="n">DDP</span> then incorporates them into the state space and the solution methods. </DD>
<P/>
    <DD>Unlike action variables, which are mainly different only in the number of values they can take on, state variables differ from each in a lot of ways. So <span class="n">DDP</span> provides a number of different kinds of state variables to add to a model as well as the possibility of defining your own type if it does not exist already. However, in this simple static situation, the base <code>StateVariable</code> class will suffice.  So Bob's award status will be of that class.</DD>
<P/>
    <DD>The simple <a href="Bellman.ox.html#OneStateModel">OneStateModel</a> class is no longer appropriate because Bob either gets the award or not.  So this model of Bob's choice is derived from a different class of problems.  Again, since it is a very simple situation, the base <a href="Bellman.ox.html#Bellman">Bellman</a> class will do the trick.  </DD>
<P/>
<DT>Static Time</DT>
<P/>
    <DD>The Bellman class does not restrict the model to be static, so unlike <code>OneStateModel</code> the code will have to specify this feature of the environment. But this will take one simple line of code.  Below the role of time is introduced in earnest.</DD>
<P/>
<DT>Utility</DT>
    <DD>Finally, the new model of Bob's choice will have to modify <code>Utility()</code> to account for scholarships.  As with the first example, this may seem much more complicated than necessary, and it is for this simple static two-state environment.  But the approach to doing this will handle any kind of utility with limited programming complexity.</DD>
<P/>
<details><summary><b>The Code</b></summary>
<DT>Source: <a href="javascript:popoxcode('../../examples/DDP/BobsChoiceB.ox');">examples/DDP/BobsChoiceB.ox</a></DT>
<DD><pre><object width="75%" height="300" type="text/plain" data="../../examples/DDP/BobsChoiceB.ox" border="1">
<p style="font-size:14pt"></p></object>
</pre></DD></details>
<P/>
<DT>Line-by-line explanation of the (new and modified) code</DT>
<P/>
<DT><code>class BobsChoiceB : Bellman {</code>
    <DD>We could continue to use the same name but for clarity this  model will be called <code>BobsChoiceB</code> The base class is no longer <code>OneStateModel</code> but <code>Bellman</code>.</DD>
<P/>
<DT><code>static const decl Uv =  &lt; -20; -18;  4.2; -0.6;  1.5;  3.2; -25; -0.5&gt;;  //Added</code>
    <DD>The vector of utilities has been moved from the utility to a constant member of the class.  Now the value of the scholarship can be  added to <code>Uv</code> in order to compute overall utility.</DD>
<P/>
<DT><code>static decl Qsch, </code></DT>
<DD>A variable to hold the scholarship state has been added to the model.</DD>
<P/>
<DT>Changes to <code>Decide</code></DT>
    <DD>We still need to <code>Initialize</code> the model, but the Bellman version of <code>Initialize</code> is different than the very special OneStateModel version.  It does not allow you to list actions.  <code> SetClock(StaticProgram)</code> says that this is a static (but possibly multi-state) model.</DD>
<P/>
    <DD><code>Actions(maj,sch)</code>:  Except in specialized models like OneStateModel, action variables are added to the model by sending them to <a href="DP.ox.html#DP___Actions">Actions</a>().  </DD>
    <DD><code>Qsch = new StateVariable("Qsch",2)</code>:  this creates a basic state variable that takes on two values.</DD>
    <DD><code>EndogenousStates(Qsch);</code>: As mentioned earlier, different kinds of states are tracked in <span class="n">DDP</span> and here we have added <code>Qsch</code> to the <em>endogenous</em> state space.
    <DD><code>CreateSpaces()</code>: In the first example this routine is called inside <a href="Bellman.ox.html#OneStateModel___Initialize">Initialize</a>().  But in all other kinds of models, the user calls this method themselves once they have added all the elements to model.  Each kind of Bellman model has its own version of <a href="DP.ox.html#DP___CreateSpaces">CreateSpaces</a>(), and the one that is called is the one that the model is derived from (here <code>Bellman</code>'s own base version).  Sometimes arguments can or have to be sent to <code>CreateSpaces</code>, but often the default values apply and nothing is sent.</DD>
<P/>
<DT>Utility</DT>
    <DD>Utility now returns a different vector depending on whether <code>Qsch</code> equals 0 or 1. <code>Qsch</code> is the state variable and takes on 2 values, 0 and 1.  We set utility up so that a value of 1 indicates the Queen's scholarship was received and its value should be added to the utility of Queen's programs when choosing where to go.  The underlying code will call <code>Utility</code> for all values of all states added to the model.  </DD>
<P/>
    <DD>The value of <code>Qsch</code> when called can be accessed a couple different ways.  One way is directly:  <code>Qsch.v</code> will hold either 0 or 1 when Utility is called.  Or the state variable can be sent to <a href="Shared.ox.html#CV">CV</a>() which will access the value return it.  <code>CV()</code> is a flexible way of incorporating state variables, parameters and even constants in your model in such way that it looks the same regardless of the kind of thing it is.</DD>
<P/>
    <DD>The trickier thing to understand at this point is accessing values of the school choice, which is an action variable not a state variable.  Looking at the list of labels associated with <code>sch</code>, Queen's is the 3rd value, or <code>2</code> since we start actions at 0.  Ox's <code>.==</code> operator will compare a vector to a value and return a vector of 0s and 1s for whether corresponding element equals the value.</DD>
<P/>
    <DD>If this is unclear, you can add a <code>print()</code> statement above <code>return</code> to see what <code>CV(Qsch)*(CV(sch).==2)</code> contributes to utility.</DD>
<P/>
<h3>Run the program and look at the output.</h3>
<P/>
<details><summary><b>The output you get should look something like this:</b></summary>
<DT>Source: <a href="javascript:popoxcode('../../examples/output/BobsChoiceB.output.txt');">../../examples/output/BobsChoiceB.output.txt</a>.</DT>
<dd><pre>
<object width="75%" height="300" type="text/plain" data="../../examples/output/BobsChoiceB.output.txt" border="1" ></object>
</pre></dd></details>
<P/>
<h2>Restricted Feasible Actions</h2>
<P/>
<DT>Now, suppose we also want the model to consider what happens if Yale does or does not accept Bob. </DT>
<P/>
    <DD>We handle this by creating a model with two states, one in which Yale is in the feasible set of schools and another at which it is not.</DD>
<P/>
<DT>State-Dependence in <span class="expressions">\(A\)</span> </DT>
<P/>
    <DD>An important feature of many dynamic programming models: the feasible set depends on the state the action is being taken at. <span class="n">DDP</span> provides a method for creating different feasible sets. Given a state <span class="expressions">\(\theta\)</span>, we can specify that the feasible action set is <span class="expressions">\(A(\theta)\)</span>.  If we let plain old <span class="expressions">\(A\)</span> denote the unrestricted set (as above), then <span class="expressions">\(A(\theta) \subseteq A\)</span>. Recall that each action vector <span class="expressions">\(\alpha\)</span> is represented in the code as a row in a matrix.  Now different states will have different matrices.</DD>
<P/>
<h2>Coding Bob's New Choice in <span class="n">DDP</span></h2>
<P/>
<DT><code>FeasibleActions()</code></DT>
    <DD>To restrict feasible actions, the user provides a <em>method</em> that returns a column vector of 0s and 1s.  This column vector depends on the current state of the model. A 1 indicates the action (row) is feasible at the state, 0 means infeasible.  The user's method is called by <span class="n">DDP</span> inside <a href="DP.ox.html#DP___CreateSpaces">CreateSpaces</a>() for each possible state.   </DD>
<P/>
<details><summary><b>The Code</b></summary>
<DT>Source: <a href="javascript:popoxcode('../../examples/DDP/BobsChoiceC.ox');">niqlow/examples/DDP/BobsChoiceC.ox</a></DT>
<DD><pre><object width="75%" height="300" type="text/plain" data="../../examples/DDP/BobsChoiceC.ox" border="1" ></object>
</pre></DD></details>
<P/>
<DT>Line-by-line explanation of the (new and modified) code</DT>
<DD><code>Yacc</code> is added to the class so that it can track the case of Yale accepting Bob or not.</DD>
<DD>The class declares that there will be a new method named <code>FeasibleActions()</code>.  The name is important because this new method will replace one that already exists.  If the name is not correct then this replacement will not happen.</DD>
<DD>The new binary state variable is created and added to the model just like <code>Qsch</code>.</DD>
<DD>The actions are still which school to go to and which major to study.  But if <code>Yacc=0</code> Yale is not feasible and any row of <span class="expressions">\(A\)</span> with that school should be removed at that state.  Yale is school 1.  If <code>Yacc=1</code> all the options are feasible. If <code>Yacc=0</code> then only cases when sch is <em>1</em> are feasible.  The logical expression for feasibility is "Yacc <em>or</em> sch=1".  However, we have to use <code>CV()</code> to get the value of variables.  And Ox uses double equal signs to test equality. It uses double vertical lines to denote "or."  So the return value <code>(CV(Yacc)==1)  .||  (CV(sch).!=1);</code> is a translation of the logic.  The table below shows the two different return values. </DD>
<P/>
<DD><pre>
VECTOR OF 0s AND 1s RETURNED BY FEASIBLEACTIONS
<P/>
     &alpha;               Yacc=0        Yacc=1
------------------------------------------------
Econ          Harvard        1              1
Physics       Harvard        1              1
Econ          Yale           0              1
Physics       Yale           0              1
Econ          Queen's        1              1
Physics       Queen's        1              1
Econ          McGill         1              1
Physics       McGill         1              1
</pre></DD>
<P/>
<DD>Finally, utility must return values that correspond to the feasible action set.  The hard-coded <code>uv</code> vector includes rows for Yale.  It cannot be used without removing those rows.  The function <code>OnlyFeasible()</code> can be used to do this without repeating the logical expression in <code>FeasibleAction</code> to find and delete the Yale rows.</DD>
<P/>
<h2>Run the program and look at the output.</h2>
<P/>
<details><summary><b>The output you get should look something like this:</b></summary>
<DT>Source: <a href="javascript:popoxcode('../../examples/output/BobsChoiceC.output.txt');">examples/output/BobsChoiceB.output.txt</a>.</DT>
<dd><pre>
<object width="75%" height="300" type="text/plain" data="../../examples/output/BobsChoiceB.output.txt" border="1" ><p style="font-size:14pt"></p></object>
</pre></dd></details>
<h2>Summary</h2>
Here is what is important to gather from the examples:
<OL class="steps">
<LI>Discrete dynamic programming links together many different discrete choices that are made at different states.</LI>
<LI>A static problem with more than one state generalizes the <a href="Bellman.ox.html#OneStateModel">OneStateModel</a> introduced first but is a special case of sequential decisions made over time.</LI>
<LI>More generally then in the one-state example, your code will call routines/methods to first  <em>initialize</em> the DP environment, then add state variables and action variables to the model then <em>create the spaces</em> implied by the items added to the model.  Then the model can be solved.</LI>
<LI>Unlike the simple one state set up, you build the action vector <span class="expressions">\(\alpha\)</span> by sending action variables to <a href="DP.ox.html#DP___Actions">Actions</a>() which will add them to the model.  This is done between initializing and creating spaces.</LI>
<LI>In <span class="n">DDP</span> timing is controlled by a clock and a static problem is specified by setting the clock as <code>StaticProgram</code>.  This is done between initializing and creating spaces.</LI>
<LI>The discrete state vector <span class="expressions">\(\theta\)</span> is a vector of state variables, each taking on a finite number of values.  Your model builds <span class="expressions">\(\theta\)</span> by creating new state variable objects and adding them to your model using <a href="DP.ox.html#DP___EndogenousStates">EndogenousStates</a>(). </LI>
<LI>Different states in the model can affect the utility of actions. The value of state variables can affect the set of feasible actions. </LI>
<LI>Restrictions of actions is possible by providing a <code>FeasibleActions()</code> routine with your model.  It takes as an input argument the matrix of all possible actions, <span class="expressions">\(A\)</span>, and returns a column of 0s and 1s indicating which rows are feasible at the current state.</LI>
<LI>As with <code>Utility()</code>, the <span class="n">DDP</span> code will always set the value of state variables before calling <code>FeasibleActions()</code>.  The value of a state variable is stored in its <code>.v</code> data member, or your code can send the state variable to <a href="Shared.ox.html#CV">CV</a>() which will return the value.</LI>
</OL>
<P/>
<h2>Exercises</h2>
<OL class="steps">Make a couple copies of <code>BobsChoiceB.ox</code> and <code>BobsChoiceC.ox</code> and experiment with them as follows. (Do not change the original file's contents.)
<LI>Suppose Bob has also applied for a Justin Bieber Scholarship which is worth &#36;100 <em>if</em> Bob attends a Canadian university.  Add a state variable to version B to account for this possibility as well as the Queen's scholarship.  Now there will be 2 state variables and four possible states.</LI>
<LI>Suppose McGill's Physics department does <em>not</em> have an undergraduate degree, so McGill-Physics is not feasible but McGill-Econ is.  Modify <code>FeasibleActions</code> in version C so that this option is excluded for both states of <code>Yacc</code>.  </LI>
</OL>
<P/>
<a name="SC"><LI>Choices Linked over Time (Dynamic States)</LI></a>
<P/>
So far the environment has multiple discrete choices but no dynamics.  Now we introduce dynamics.  We will use Tommy's choice to introduce dynamics.   That is, we will model how many hours Tommy studies for an exam in the period leading up to the exam.</p>
<P/>
<DT>Tommy Choices Redux</DT>
<DD>Tommy chooses among <span class="expressions">\(J=13\)</span> different discrete numbers of hours each night.  Nights are different (and thus his choice) because the opportunity cost of studying goes up on the weekend.</DD>
<DD>For now we continue a very simple (trivial) reason Tommy cares about studying. Later we add to the model that he knows how studying affects his expected score. </DD>
<P/>
<h2>Discounting and the Overall Objective</h2>
<P/>
In the static examples earlier utility is the objective and there is nothing that ties together multiple states.  Once we introduce sequential choice it becomes necessary to describe how choices and their utilities are aggregated over time.  The power of dynamic programming is to tie together simple static choices in order to explain sequential choice.</p>
<P/>
<DT>Time</DT>
    <DD>Let <var>t</var> denote time, takes on discrete values. The first decision occurs at <var>t=0</var>.  Suppose the chooser at time 0 is trying to decide among different sequences of choices.  A particular sequence of choices is <span class="expressions">\(\alpha \equiv \alpha_0, \alpha_0, \dots\)</span>.  For the moment, the total number of periods is left unspecified. It could be finite or infinite (the decisions keep going forever).  And, for the moment, we suppress any other state variables that may be part of the model other than <var>t</var>.</DD>
<P/>
    <DT>Then, <span class="n">DDP</span> and the basic DP framework itself assume that the chooser places value on that sequence equal to</DT>
<span class="equation">$$V(\alpha)  = \sum_{t=0,1,\dots}\  \delta^t U(\alpha_t; t).$$</span>
    <DD>The new and fundamental parameter appearing in <span class="expressions">\(V()\)</span> is the <em>discount factor <span class="expressions">\(\delta\)</span></em>, which typically is in the range <span class="expressions">\([0,1)\)</span>.
    <details class="aside"><summary>Notes</summary>
    <DD>If the decision horizon is finite then <span class="expressions">\(\delta=1\)</span> is permissible.  If <span class="expressions">\(\delta=0\)</span> the person puts zero weight on future utility.  The decider only cares about the current utility even though current decisions might affect the future.</DD>
    <DD>This formulation allows utility to depend on time as well as the action chosen at time.  But soon we will move <var>t</var> into the state vector <span class="expressions">\(\theta\)</span> and it will simply be a specialized state variable not completely set apart from other state variables.</DD></details>
    </DD>
<P/>
<h2>The Clock and Value Function Iteration</h2>
<P/>
Dynamic programming accounts for a choice made today affects outcomes in the future along with the current appreciation (utility) of the choice.  The linear separability of <span class="expressions">\(V()\)</span> (and geometric discounting) makes it possible to solve the overall dynamic problem by breaking it into many connected static problems.</p>
<P/>
<DT>Transitions</DT>
    <DD>The only required time distinction in DP is between now (today) and later (tomorrow). It is common in the literature to use &prime; to denote things that will happen tomorrow.  These notes follow that convention.  So if <var>t</var> is a state variable, then <var>t</var> alone is its value today when a choice is being made.  The value it takes on tomorrow is the denoted <var>t&prime;</var>. A state variable moving from today to its value tomorrow is called a <em>transition</em>.</DD>
<P/>
<DT>Stationary and Non-Stationary Clocks</DT>
    <DD>There are two basic clocks in DP.  The first is a stationary clock in which today is the same as tomorrow.  This means that the decision horizon is infinite, because if decisions end sometime in the future then tomorrow is one period closer to the end then today.  Therefore they are not the same. The second is a special kind of non-stationary world which ends after fixed number of periods.  This is usually called a <em>finite horizon</em> model.  However, <span  class="n">DDP</span> makes distinctions between different kinds of finite horizon clocks.  So what is usually called a finite horizon is here called <em>normal aging</em>.</DD>
<P/>
<DT>Normal Aging</DT>
    <DD>In normal aging, time starts at <code>0</code> and ends at some fixed last time, denoted <code>T-1</code>.  That is, <code>T</code> is the number of periods of choice. In normal aging, the time next period is always one period later than the last (until T-1).  That is,
    $<span class="expressions">\(t' = t + 1,\quad   0 \le t \lt T.\)</span>
    This is the <em>transition</em> of a normal aging clock.  Each day you get another day older.  Note this is really defining a function,  <var>t'(t) = t+1</var>.  Technically, the transition is undefined for when <var>t=T-1</var> because that is the last period and the world ends immediately after.</DD>
<P/>
<DT>Infinite Horizon (stationary)</DT>
    <DD>A stationary simply says tomorrow is just like today:
    <span class="equation">$$t' = t$$</span>
    Again, tomorrow is different from today but when the chooser wakes up tomorrow it is just like today.  However, what state the chooser is in tomorrow may be different than the state he was in today.
    <details class="aside"><summary>A perfect analogy</summary>In <em>Ground Hog Day</em> in which Bill Murray knows he will wake up and relive the same day, apparently forever, but his state can be different each day he wakes up.  He can learn to play the piano and every day he wakes up a little better.  And each day he makes choices that affect his utility in the future.  It is a perfect analogy to a stationary decision environment and also a perfect movie.</details>
    An the important feature of time is that if there is a yesterday (as in normal aging) it can never come again.  Any state that occur in the future cannot have a value of time less than <var>t</var>.  </DD>
<P/>
<h2>Clocks in <span class="n">DDP</span></h2>
<P/>
<h2>State Variables that Depend on Current and Past Events <span class="n">DDP</span></h2>
<P/>
In BobsChoiceB there were two states.  Either Bob has a scholarship or not and this changes his decision.  Dynamic programs can account for state variables that depend on what has happened in the past.  For example a very common state variable counts how many times a choice has been made before the current period.  If <span class="expressions">\(a\)</span> is binary choice then utility might depend on how many times <span class="expressions">\(a=1\)</span> before now.
<span class="equation">$$A_t = {\sum}_{s=0}^{t-1} a_s.$$</span>
Note that this does not clearly define the initial value <span class="expressions">\(A_0.\)</span>  Typically the model would set this to 0, but it could be initialized to a different value.  For dynamic programming it does not matter how the current value <span class="expressions">\(A_t\)</span> came about.  What matters is how current decisions will alter the effect <span class="expressions">\(A\)</span> in the future.  This is what is meant by the <em>transition of <span class="expressions">\(A\)</span></em>.  We can state this simply as
<span class="equation">$$A^\prime = A + a.$$</span>
That is, next period's value of <span class="expressions">\(A\)</span> will be what it is today plus the value of the current decision.  If <span class="expressions">\(a=1\)</span> then <span class="expressions">\(A^\prime = A + 1.\)</span>  Otherwise it is unchanged.</p>
<P/>
<h2>Terminal States</h2>
<P/>
In some environments decision-making ends when a particular state is reached.  These are called terminal states.  A value of state variable can be made terminal using <a href="Variables.ox.html#StateVariable___MakeTerminal">MakeTerminal</a>().
<P/>
<LI><a name="MD">Multiple Deciders</a></LI>
<P/>
<em>Not completed yet: You can see information about this in <a href="Variables.ox.html#Fixed">Time Invariant or Grouping Variables </a></em>
<P/>
<!--<LI><a name="Next">Where to go next</a></LI>-->
<P/>
</OL>
<P/>
<HR/>
<P/>
<a name="tech"><H1>Technical Overview of <span class="n">DDP</span></H1></a>
<P/>
This section provides the mathematical description of the fundamentals of <span class="n">DDP</span>. This is the first document on the technical side.  For further help on using and extending your model see also:
<DD><a href="Methods.ox.html">Methods</a> for how to solve the model</DD>
<DD><a href="Data.ox.html">Data</a> for how to simulate data and read external data in for estimation</DD>.
<P/>
<p>This is useful for someone who already knows a great deal about dynamic programming and has some familiarity with Ox or similar languages. If you want a much more basic introduction, start back at the <a href="#guide">beginner's guide</a> You can also start with <a href="GetStarted.html">GetStarted</a> for a demonstration of coding and return here.</p>
<P/>
<OL class="contents">Contents
<LI><a href="#NC">Notation and Conventions</a></LI>
<LI><a href="#EDP">Elements of DP in <span class="n">DDP</span></a></LI>
<LI><a href="#ES">Extensions and Specializations</a></LI>
<LI><a href="#VS"><span="n">DDP</span> terminology versus other surveys and methods articles</a></LI>
<LI><a href="#DS">Designing and solving <code>MyModel</code></a></LI>
<!--<LI><a href="#PS">Prediction and Simulation</a></LI>
<LI><a href="#Data">Data</a></LI>
<LI><a href="#Estim">DDP Estimation</a></LI>-->
</OL>
<P/>
<OL class="body"><a name="NC"><LI>Notation and Conventions</LI></a>
<OL class="chapter">
<LI>Fonts, case and alphabets</LI>
<details><summary>Examples</summary><pre>
The discount factor:        <span class="expressions">\(\delta\)</span>
Single action variable:     <span class="expressions">\(a\)</span>
Vector of actions:          <span class="expressions">\(\alpha = (a_0,a_1,\dots)\)</span>
Number of values:           <span class="expressions">\(a\)</span>.N
Size of a vector:           <span class="expressions">\(\alpha\)</span>.N
Distinct vectors            <span class="expressions">\(\alpha\)</span>.D
Endogenous state space:     <span class="expressions">\(\Theta\)</span>
Utility:                    <span class="expressions">\(U()\)</span>
</pre></details>
<DT><em>Lower case Greek</em> (e.g. <span class="expressions">\(\alpha\)</span>, <span class="expressions">\(\beta\)</span>) denotes</DT>
      <DD>Vectors of discrete variables (i.e. exogenous states <span class="expressions">\(\epsilon\)</span>) <em>or</em> scalar parameters, quantities variables with a continuous range (i.e. discount factor <span class="expressions">\(\delta\)</span>).</DD>
<DT><em>Lower case Roman</em> (a, b, &hellip;)  denotes</DT>
     <DD>Individual discrete variables (i.e. action <code>a</code>) <em>or</em> variable properties of an object (i.e. current value <code>a.v</code>). A generic element of a vector will usually use the Roman letter corresponding to the vector's Greek name and without subscript.  A subscript is used when ordering is important.</DD>
<DT><em>Upper case Greek</em> (e.g. <span class="expressions">\(A\)</span>, <span class="expressions">\(\Theta\)</span>)  denotes</DT>
     <DD>Parameter vectors <em>or</em>spaces (sets of discrete vectors).</DD>
<DT><em>Upper case Roman</em> (A, B, C, &hellip;) denotes</DT>
      <DD>Functions, often including empty brackets and arguments suppressed (i.e. U()). <em>or</em> fixed properties of an object (i.e. <code>a.N</code>) </DD>
<P/>
<LI>Objects and the property operator <var>.</var></LI>
<P/>
In mathematical notation, <span class="expressions">\(a_N\)</span> might denote the number of values <span class="expressions">\(a\)</span> takes on. This works well when <span class="expressions">\(a\)</span> has only one property. In <span class="n">DDP</span>, a variable like <span class="expressions">\(a\)</span> has several properties so using subscripts and superscripts to indicate them can become clumsy.  And, properties can have properties.  Instead of using <span class="expressions">\(a<_N\)</span>, the notation <b><code>a.N</code></b> is the property N associated with the object <code>a</code>. The binary <code>.</code> operator is how properties (members) are accessed in Ox.
<DD><q>o.p</q> retrieves from the object <var>o</var> the aspect or property <var>p</var>.</dd>
<P/>
The <a href="Shared.ox.html#Quantity">Quantity</a> class represents <a href="Shared.ox.html#Discrete">Discrete</a> variables and continuous <a href="Shared.ox.html#Parameter">Parameter</a>s. The usual notions of variables and parameters in mathematical models are derived from the base type <a href="Shared.ox.html#Quantity">Quantity</a>. Quantities are either <em>discrete</em> (actions and states) or <em>continuous</em> (parameters). Quantity objects are typically added to one or more lists using routines built into <span class="n>niqlow</span>.  The vector notation is used to match the literature but vectors in standard DDP are actually lists of <a href="Shared.ox.html#Quantity">Quantity</a> objects.</p>
<P/>
<DT>Ranges:  0 &hellip; m.</DT>
    <DD>Although it is natural to start counting from <em>1</em>, starting from <em>0</em> has some advantages. It is common in C-like languages such as Ox. Typically in <span class="n">DDP</span>, <b>0</b> is the lowest and first value a discrete variable takes on. Starting at 0 means a variable <var>n</var> that takes on N values has a range <span class="expressions">\(n=0,1,\dots,N-1\)</span>. This is the default range for discrete variables.</DD>
<P/>
<LI>Special Functions and Operators</LI>
<P/>
<DT>Unary decrement operator &oline; (postfix)</DT>
<P/>
<DD>Counting N values starting from 0 means the last possible value is N-1, a bit of notational clutter that can be confusing.  To avoid the clutter define the unary decrement operator &oline; as J&oline;  &equiv;  J - 1. For example, 5&oline; = 4. So a variable <code>n</code> with <code>n.N</code> values would have the range <var>n = 0, &hellip; ,(n.N)&oline;</var>.</dd>
<P/>
<DT>&prime; : next value(s) of a state variable (postfix)</DT>
<DD>If <var>s</var> is a state variable then values it does or can take on next period are denoted <var>s&prime;</var>.</DD>
<P/>
<DT>The indicator function I{}.</DT>
        <span class="equation">$$I{x} = \cases{ 1 & x is TRUE\cr
                         0 & otherwise.\cr}$$</span>
<P/>
<DT>Cartesian product of discrete sets, <span class="expressions">\(\times\)</span> is the matrix of all possible vectors of the sets.</DT>
<DD>Let a<sub>0</sub> and a<sub>1</sub> be two discrete variables that each take on values 0 and 1.  Then</DD>
<P/>
<DD><span class="expressions">\(a_0 \in \{ 0, 1 \},  a_1 \in \{ 0, 1 \}\)</span> then <span class="expressions">\(a_0 \times a_1\)</span> equals
<pre>
               0         0
               1         0
               0         1
               1         1
</pre></DD>
<P/>
<LI>OOP Lingo</LI>
<P/>
<acronym title="Object oriented programming ">OOP</acronym> has jargon.  This jargon is used here in an effort to be accurate in describing the code but it may be obscure to people unfamiliar with OOP. In Ox, a <code>class</code> and a <code>struct</code> are both a <em>class</em> as usually defined.
    <details class="aside"><summary>What's the difference?</summary>The difference between <code>struct</code> and <code>class</code> is simply whether elements of an object are by default directly accessible from the outside (i.e. <em>public</em>) or not (<em>private</em>): yes in a <code>struct</code>, no in a <code>class</code>. <span class="n">DDP</span> is designed for convenience not reliability, so everything class is declared <code>struct</code>, but the term <em>class</em> is used in this documentation.</details>
A <em>class</em> is a bundle of data and functions to operate on the data.  The data are called <em>members</em> of the class and the functions are called <em>methods</em>, although Ox documentation also refers to these as data members and function members, respectively. Multiple copies of a class can be created while a program runs.  Each copy is called an <em>object</em> or <em>instance</em> of the class.   The key is that the methods work with the data of the object without needing to <em>pass</em> the data to it as with non-OOP languages or constructs.</p>
<P/>
<p>Members and methods of  class are either <em>static</em> or <em>automatic</em>.  This distinction is extremely important in the design of <span class="n">DDP</span>.  Static members/methods are shared by all objects of a class, whereas automatic members/methods are specific to the instance.  <span class="n">DDP</span> conserves memory by storing as much information in static variables as possible. The word <em>automatic</em> does not ever appear, it is implicit.  If the tag <code>static</code> does not appear in the declaration of the item then it is automatic.</p>
<P/>
<LI><em>MyModel</em>, <em>DPPparent</em> and other placeholder names</LI>
<P/>
    <details class="aside"><summary>Geek talk:</summary> <code>MyModel</code> and <code>MyCode</code> are <q>metasyntatic variable</q> like <code>foo</code>.</details>
<P/>
<p> A user builds a DDP model by adding components to it: states and actions and the functions related to them. <span class="n">DDP</span> cannot know how many items will be added of each type. How can the model be ready to store whatever the user chooses?  The answer: the user of <span class="n">DDP</span> constructs a model as a class <em>derived</em> from one of the built-in models, called a DDP for <q>Derived Dynamic Program</q>.   The user's class <em>inherits</em> the built-in properties of its base model. So <span class="n">DDP</span> can solve the model and produce output for it even though it does not know the details of the user's model until the program starts executing.</p>
<P/>
DDP Models are derived from the <a href="Bellman.ox.html#Bellman">Bellman</a> class, which in turn is derived from the base <a href="DP.ox.html#DP">DP</a> class. If the user wants to call his/her model <var>MyModel</var>, they would have something like this
<DD><pre>class MyModel : DPparent {
    &vellip;
    }
</pre></DD>
For example, if you want to model the choice over makes of car and include extreme-value shocks in the value of choices you would declare a class like this:
<DD><pre>class Make : ExtremeValue {
    &vellip;
    }
</pre></DD>
So the generic <code>MyModel</code> is specifically <code>Make</code> and <code>DPparent</code> is specifically <code>ExtremeValue</code>.</p>
<P/>
<p>Which Bellman-derived class that <code>MyModel</code> is based on is called <code>DPparent</code> in these notes. <code>MyModel::</code> is prefixed to items that the user provides or customizes.  <code>DPparent</code> is prefixed to items related to the parent.  Other predefined or default items either have no prefix or are prefixed by <code>DP::</code>. The convention of using <code>MyModel</code> avoids having to write repeatedly <q>the user's version of &hellip; DP::x</q>  Instead, <code>MyModel::x</code> suffices.</p>
<P/>
<DT><em>MyCode</em></DT>
<P/>
A user must define elements of <code>MyModel</code> in Ox, and they must write an Ox program that executes some tasks in the proper order.  For example, the code must always call the <code>Initialize()</code> method for <code>DPparent</code> before adding things to the model.  So the Ox code that executes these tasks is collectively called <code>MyCode</code> in these notes.  Another way to think of it: <code>MyModel</code> is a translation of the pen-and-paper aspects of your model and <code>MyCode</code> are the instructions to implement the model, solve it and use it.</p>
<P/>
<LI>Some Properties of DDP Objects</li>
<P/>
The interpretation of a property can depend on the kind of object on the left side of ".".  Here are some of the key properties of objects in <span class="n">DDP</span>.  Note that the association of this variable names to a property is a convention in <span class="n">DDP</span>.  It is not inherent in Ox, and there may be exceptions even within <span class="n">DDP</span>.
<P/>
<DT>N : cardinality.</DT>
    <DD>A discrete object (such as states and actions) has a cardinality/range. <em>d.N</em> is the number of distinct values <var>d</var> takes on, and generically these are the range 0,1, &hellip; ,(d.N)&oline;. A vector, such as <span class="expressions">\(\alpha\)</span>, has a size <var>N</var>, which denotes the <em>length</em> of the vector.</DD>
<P/>
<DT>D : dimensionality</DT>
    <DD>A vector <code>x</code> of discrete variables has a length x.N. But it creates a space of possible values (the Cartesian product) equal to the product of the individual variable cardinalities. So <code>x.D</code> is the size of the Cartesian space of a vector <code>x</code>.
    <span class="equation">$$x.D  \equiv {\prod}_{i= 0}^{x.N^-} x_i.N.$$</span>
    A space in <span class="n">DDP</span>, say <span class="expressions">\(\Theta\)</span>, is usually a set of vectors of the space of possible vectors.  The number of points in a space is then <span class="expressions">\(\Theta\)</span>.D.</DD>
<P/>
<DT>Rows and columns of matrices</DT>
    <DD>Since matrices are typically representing a vector space, <var>A.D</var> is the number of rows and <var>A.N</var> is the number of columns.</DD>
<P/>
<DT>i : position.</DT>
<P/>
    <DD>Many objects appear in a vector or a list.  The objects position in the list is <var>i</var>.  So a<sub>i</sub> is a name for the ith action variable in &alpha;. We can write a<sub>i</sub>.i = i. This redundancy turns out to be very important in some cases.</DD>
<P/>
<DT>v : current value.</DT>
<P/>
    <DD>In math, <var>a</var> usually means the value of the variable <var>a</var>.  This notation is inadequate when variables have multiple properties and when the notation is meant to reflect some elements of the computer program. Instead, the current value of a variable or vector is the property <em>v</em>. So if <var>a</var> is a discrete variable <code>a.v</code> is the value of <var>a</var>, which can only be one of the values 0 &hellip; (a.N)&oline;.</DD>
<P/>
    <DD>An important function in <span class="n">niqlow</span> is <a href="Shared.ox.html#CV">CV</a>() which will return the current value of objects sent to it.  So typically <code>MyCode</code> does not need to reference  <code>.v</code> directly. This is explained in detail below.</DD>
<P/>
<DT>.actual  : the actual values</DT>
<P/>
    <DD><code>MyModel</code> may need discrete values to correspond to another set of values (possibly not even integer values).  Which values are mapped to may depend on parameters that are changing between solutions of the model. </DD>
<P/>
    <DD>If <code>MyModel</code> creates an action or state variable <var>x</var> from a derived the class then it can also provide an <code>Update()</code> routine to reset and store the vector of <em>actual</em>.</DD>
<P/>
    <DD>The default is that <code>x.actual = 0... (x.N)&oline;</code>, the range of <code>x.v</code>.</DD>
<P/>
<li>Accessing Values of Quantity Objects</li>
<P/>
<code>MyCode</code> can get the current value of an object simply using <code>x.v</code> (and the actual value as <code>x.actual[v]</code>, but this is not recommended.  <code>MyCode</code> should <em>never</em> modify these values because DDP ensures they have the correct values at all times.</p>
<P/>
<p>One reason to avoid direct reference to <code>.v</code> is that <code>MyCode</code> will change as it develops. For example, early on some quantity <var>x</var> may not be an action or state variable, but simply a fixed number. So <code>MyModel</code> can access its current value as simply <code>x</code>. However, as the model takes shape <var>x</var> may be changed to a variable.  But now <code>x</code> is a complicated object not a number.  The user would have to go through <code>MyCode</code> and <code>x</code> to <code>x.v</code>. It is also possible to want to change a number into a function, <code>x()</code> that computes and returns a value.</p>
<P/>
<span class="n">DDP</span> provides functions to access current and actual values that will work even when the container changes from a simple variable to a Quantity object or function.  </p>
<P/>
<DT><code><a href="Shared.ox.html#CV">CV</a>()</code></DT>
<P/>
    <DD><code>CV(x)</code> is a routine in <span class="n">niqlow</span> that you can send almost anything to and it will return the value of the object. It examines the argument <code>x</code>, and if it is a <a href="Shared.ox.html#Quantity">Quantity</a> object with an element <code>v</code> then <code>CV(x)</code> will return it.   That is <code>CV(x) = x.v</code>. What makes it very useful is that it will also return the value of other things that are not
    Quantity objects.  If you send a real number (called a <code>double</code> in Ox) as <code>x</code> then <code>CV()</code> simply returns the value.  If you send a function to <code>CV()</code> then it will call the function and return its value. Thus, you do not need to change your code as some concept changes during programming if you use <code>CV(x)</code>.</DD>
<P/>
<DT><code><a href="Shared.ox.html#AV">AV</a>()</code></DT>
<P/>
    <DD>Discrete quantities like state variables always take on values from 0 to some upper bound <code>N-1</code>.  However, in the model each of those discrete values may map into different values in the model.  The property <code>actual</code> contains these model-relevant values. <code>AV(x)</code> acts like <code>CV()</code> but it returns <code>x.actual[x.v]</code>.</DD>
<P/>
    <DD>By default this equals <code>CV(x)</code> unless you specify different actual values.  So in the default case <code>AV(x) = CV(x)</code>. In this way <code>a-&gt;Update()</code> is only called once for each variable on each model solution to reset <code>.actual</code>.  If <code>.actual</code> were not a vector <code>x-&gt;Update()</code> would have to be called every time <code>x.v</code> changed.</DD>
<P/>
</OL>
<P/>
<a name="EDP"><LI>Elements of DP in <span class="n">DDP</span></LI></a>
<P/>
First, the simplest and most general notation is introduced to describe a DDP.  However, code that simply matched the general form of a DP model would quickly overwhelm memory or computing capacity. <span class="n">DDP</span> saves memory and calculation by letting <code>MyModel</code> categorize elements and specialize the environment.
<P/>
<OL class="chapter">
<li><dfn id="DP-def">Definition of a DP Model</dfn> </li>
<P/>
<DT>Primitives: basic elements of a complete DP model.</DT>
    <OL class="steps">
    <LI><span class="expressions">\(\alpha \in A\)</span>: The (finite) set of possible actions</LI>
    <LI><span class="expressions">\(\theta \in \Theta\)</span>: The (discrete) state space</LI>
    <LI><span class="expressions">\(P(\theta^{\,\prime}\,|\,\alpha,\theta)\)</span>: Conditional transition to the next state, including any notion of a time dimension.</LI>
    <LI><span class="expressions">\(U(\alpha,\theta)\)</span>: Utility/return/payoff of an action conditional on the state</LI>
    <li><span class="expressions">\(\zeta_\alpha\)</span>: shock added to the value of a choice in order to smooth choice probabilities.</li>
    <LI><span class="expressions">\(E\left[\sum_{t=0,\dots}\delta^t\,U(\alpha_t,\theta_t)\right]\)</span>: additively separable objective with foresight; <span class="expressions">\(\delta\)</span> is the discount factor.</LI>
    </OL>
<P/>
<DT>Aspects of the <dfn id="DP-solution">DP solution</dfn>.</DT>
    <OL class="steps">
    <LI><span class="expressions">\(v(\alpha,\theta):\quad\)</span> Value of current choice given future optimal choices.</LI>
    <LI><span class="expressions">\(V(\theta):\quad\)</span> Value of arriving at state \theta, accounting for current optimal choice</LI>
    <LI><span class="expressions">\(EV(\theta^{\,\prime}|\alpha,\theta):\quad\)</span> Expected value entering next period (after integrating out optimal values across IID random terms)</LI>
    <LI><span class="expressions">\(P^\star(\alpha|\theta):\quad\)</span> Conditional choice probabilities (after integrating out IID random terms).</LI>
    </OL>
<P/>
<DT><dfn id="BE-dfn">Bellman's Equation: Solution of a DP</dfn></DT>
<P/>
<DD>Bellman's equation is really a combination of four equations that jointly relate the aspects above to the primitive elements of the model.</DD>
<P/>
<span class="equation">$$\eqalign{
v(\alpha,\theta)\quad &\equiv \quad U(\alpha,\theta) + \delta EV\left(\theta^{\,\prime}|\alpha,\theta\right),\quad \forall \alpha\in A, \theta \in \Theta\cr
EV(\theta^{\,\prime}|\alpha,\theta)\quad &= \quad \sum_{\theta^{\,\prime}\in\Theta} P(\theta^{\,\prime};\alpha,\theta) V(\theta^{\,\prime})\cr
V\left(\theta\right)\quad  &\equiv\quad  \max_{\alpha\in A}\quad v(\alpha,\theta)\cr
P^\star\left(\alpha,\theta\right)\quad  &\equiv\quad   Prob\left[\ \alpha \in \arg\max_{\alpha\in A}\quad v(\alpha,\theta )\ \right]\cr}$$</span>
<P/>
<li>Encoding Abstract Elements</li>
<P/>
    This section shows how each of the elements of the general DP framework is represented in <span class="n">DDP</span>.  The user's code (known here as <code>MyCode</code>) will build the model up dynamically as the code executes.  Then when all the elements of the model has been defined the code will call <code>DPparent::CreateSpaces()</code>, which will construct the action set and state space.  After this, <code>MyCode</code> can solve the model and use using tools described elsewhere.</p>
<P/>
<details><summary><b>Framework</b><br/> The shell for the model.</summary>
<pre>class MyModel : DPparent {
     // declare static members to hold action and state variables objects
     // declare required and optional methods
    static Initialize();
    }
&vellip;
MyModel::Initialize() {
    DPparent::Initialize(new MyModel(),&hellip;);
       // define actions and states (create objects)
       // add them to the model
    DPparent::CreateSpaces(&hellip;);
    &vellip;
    }
</pre></details></dd>
<P/>
<DT>Notes</DT>
<DD>Elements of the model are added between the call to <code>DPparent::Initialize()</code> and <code>DPparent::CreateSpaces()</code>.  </DD>
<DD>There is no requirement that <code>MyModel</code> provide an <code>Initialize()</code> function.  However, it is convenient to do this so that all the model creation steps occur together within a (static) method, and that method has direct access to elements of <code>MyModel</code> and, through inheritance, all the methods and data members within <class span="n">DDP</class>.</DD>
<DD>[Subtle/Non-Intuitive Alert!]  Every DP model sends a <b>new</b> copy of itself to the parent <code>Initialize()</code> method.  What this means and why it is done is not easy to explain at this point. <a href="#">See below.</a></DD>
<DD>The <code>&hellip;</code> inside <code>()</code> means that some other required arguments need to be sent or optional arguments can be sent, depending on the parent of <code>MyModel</code>.</DD>
<P/>
<OL class="section">
<P/>
<li>Action Variables</li>
<P/>
<blockquote><b><code>MyCode</code> builds the action <span class="expressions">\(\alpha\)</span> by adding action variables to it using <a href="DP.ox.html#DP___Actions">Actions</a>().</b></blockquote>
<P/>
At a minimum, an <a href="Variables.ox.html#ActionVariable">ActionVariable</a> is defined by its <a href="Shared.ox.html#Quantity___L">L</a>abel and the number of distinct values it takes on, <a href="Shared.ox.html#Discrete___N">N</a>.  <span class="n">DDP</span> tracks values of <em>a</em> as 0 &hellip; N&oline;.</p>
<P/>
<details><summary><b>Example</b><br/> Define a binary choice variable <var>d</var> and add it to <span class="expressions">\(\alpha\)</span>.</summary>
<pre>class MyModel : DPparent {
    &vellip;
    static decl d;                         // NEW
    &vellip;
    static Initialize();
    }
&vellip;
MyModel::Initialize() {
    DPparent::Initialize(new MyModel(),&hellip;);
    &vellip;
    d = new ActionVariable("choice",2);   // NEW
    Actions(a);                           // NEW
    &vellip;
    CreateSpaces();
    }
</pre></dd></details>
<P/>
<DT>See <a href=".\Variables.ox.html#ActionVariables">Full Action Variable and Action Vector Documentation</a>.</DT>
<P/>
<li>State Variables and Blocks</li>
<blockquote><b><code>MyCode</code> builds the state space by adding adding state variables and state blocks to <code>MyModel</code>.</b></blockquote>
<P/>
    As with <span class="expressions">\(\alpha\)</span>, the state of the DP model <span class="expressions">\(\theta\)</span> is built up by adding state variables to it.  In the basic notation above, <span class="expressions">\(\theta\)</span> is simply a point in a set, but in DDP it will be a vector of individual state variables. Unlike action variables, state variables evolve and how they evolve affects what needs to be stored and computed for them.   The transition <span class="expressions">\(P(\theta^\prime; \alpha,\theta)\)</span> emerges from the individual transitions of the state variable added to the state.</p>
<P/>
In <span class="n">DDP</span>, state variables are classified as either <em>autonomous</em> or <em>coevolving</em> depending on how they enter the state transition &Rho;().
<DT><a href="Variables.ox.html#Autonomous">Autonomous</a> Variables</DT>
    <DD>If <code>s</code> is autonomous, then its transition is independent of all other transitions <em>and</em> the transitions of all other variables is independent of  <code>s</code>.  This means the <code>s</code> transition enters the overall &Rho;() independently.  The transition for <code>s</code> can still depend on the current action and current state. The transition is specified by making the state variable an instance (object) of one of the built-in autonomous state variables adding it to <code>MyModel</code>.</DD>
<P/>
<DT><a href="Variables.ox.html#Coevolving">Coevolving</a> Variables and <a href="Variables.ox.html#StateBlock">StateBlock</a>s</DT>
    <DD>If the transition of a variable depends on the transition of one or more other states then it is coevolving and must be placed in a <a href="Variables.ox.html#StateBlock">StateBlock</a>. If a variable is coevolving then its  <a href="Variables.ox.html#StateBlock">StateBlock</a> is responsible for determining the transitions.  A block is itself independent (autonomous) of all other autonomous state variables and blocks.</DD>
<P/>
<DT>See <a href=".\StateVariable.ox.html">State Variable and Block Documentation</a>.</DT>
<P/>
<details><summary><b>Example</b><br/> Define a state variable <var>m</var> that takes on the value of action variable <var>d</var> chosen last period.</summary>
<pre>class MyModel : DPparent {
    &vellip;
    static decl d;
    static decl m;                                //NEW
    &vellip;
    static Initialize();
    }
&vellip;
MyModel::Initialize() {
    DPparent::Initialize(new MyModel(),&hellip;);
    &vellip;
    d = new ActionVariable("choice",2);
    Actions(a);
    m = new LaggedAction("prevd",d);             //NEW
    EndogenousStates(m);                         //NEW
    &vellip;
    CreateSpaces();
    }
</pre></dd></details>
<P/>
<LI name="clock">Transitions and the Clock</LI>
    <blockquote><b>&Rho;() is generated automatically by the state variables and blocks added to the state vectors.<br/><code>MyModel</code> always includes a time-keeping state block,</b></blockquote>
<P/>
<DT>The state space <span class="expressions">\(\Theta\)</span> is built up by <span class="n">DDP</span> from the state variables added to the model.</DT>
  <DD>Each state variables or block <var>q</var> is an object of a class that defines all aspects of behaviour including its interaction with actions and other state variables as well its transition from the current state <span class="expressions">\(q\)</span> to the state next period, <span class="expressions">\(q^{\prime}\)</span>. Therefore, the overall transition <span class="expressions">\(P(\theta^{ \prime};\alpha,\theta)\)</span> is constructed by <span class="n">DDP</span> rather than specify independently of the state and action spaces.</DD>
<P/>
<DT>The abstract DDP model defined above has an implicit concept of <q>today</q>, the current state <span class="expressions">\(\theta\)</span>, and of <q>tomorrow</q>, the next state <span class="expressions">\(\theta^{ \prime}\)</span>.</DT>
  <DD>But since, at its solution, Bellman's equation holds at each point in <span class="expressions">\(\Theta\)</span> there is no need to further specify timing within the model.  However, if a monotonic time variable is an element of the state vector, then Bellman's equation can be solve sequentially backwards in time, reducing temporary storage and computational requirements. Because timing is key to the efficient solution algorithm  <span class="n">DDP</span> always has the concept of a model <em>clock</em>.  The literatures includes many models that have generalized notions of time that still exhibit monotonicity.  To account for these notions the clock is always a <a href="Variables.ox.html#StateBlock">StateBlock</a> with at least two state variables in it.</DD>
<P/>
<details><summary><b>Example</b><br/>Set the clock to a finite horizon of 40 periods.</summary>
<pre>class MyModel : DPparent {
    &vellip;
    static decl d;
    static decl m;
    &vellip;
    static Initialize();
    }
&vellip;
MyModel::Initialize() {
    DPparent::Initialize(new MyModel(),&hellip;);
    SetClock(NormalAging,40);                //NEW
    &vellip;
    d = new ActionVariable("choice",2);
    Actions(a);
    m = new LaggedAction("prevd",d);
    EndogenousStates(m);
    &vellip;
    CreateSpaces();
    }
</pre></dd></details>
<P/>
<li>Utility</li>
    <blockquote><b><code>MyModel::Utility()</code> should return utility as a vector for the feasible matrix at the current state.</b> </blockquote>
<P/>
<DT><code>MyModel::Utility()</code></DT>
    <DD>The one period utility/return/payoff, <span class="expressions">\(U(\alpha;\epsilon,\eta,\theta)\)</span>.  It must be called <code>Utility()</code>, because it replaces a <em>virtual</em> method, <code>DP::U</code>.  It returns the utility as a <em>vector</em>: one element for each feasible action &theta;.A. You might expect <code>MyModel::Utility()</code> would require arguments to pass the value of state variables.  However, using the object-oriented approach to representing the model means that the values are available because <span class="n">DDP</span> will set the value of members of <code>MyModel</code> before calling U(). </DD>
<P/>
<details><summary><b>Example</b><br/>Define the Utility to equal an indicator for whether the current action equals last period's action.</summary>
<pre>class MyModel : DPparent {
    &vellip;
    static decl d;
    static decl m;
    Utility();                          //NEW
    &vellip;
    static Initialize();
    }
&vellip;
MyModel Utility(); {                   //NEW
    return CV(d) .== CV(m);            //NEW
    }                                  //NEW
&vellip;
MyModel::Initialize() {
    DPparent::Initialize(new MyModel(),&hellip;);
    SetClock(NormalAging,40);
    &vellip;
    d = new ActionVariable("choice",2);
    Actions(a);
    m = new LaggedAction("prevd",d);
    EndogenousStates(m);
    &vellip;
    CreateSpaces();
    }
</pre></dd>
</details>
<P/>
<li>Foresight</li>
    <blockquote><b><code>MyModel</code> should set the discount factor using <a href="DP.ox.html#DP___SetDelta">SetDelta</a>().</b></blockquote>
<P/>
<DT><span class="expressions">\(\delta\in [0,1)\)</span>: the discount factor.</DT>
<P/>
    <DD>The default value is <code>DP::delta=0.95</code>. MyModel::&delta; is either a fixed real value or a <a href="Shared.ox.html#Parameter">Parameter</a>, which allows it to depend on outside variables and/or to be estimated within a nested solution algorithm. <a href="DP.ox.html#DP___SetDelta">SetDelta</a>() can be used to set the value, passing either a real number or a <a href="Shared.ox.html#Parameter">Parameter</a>. Unlike some other elements of the model, the discount factor can be set or changed after <code>CreateSpaces()</code> has been called.</DD>
<P/>
<details><summary><b>Example</b><br/>Set &delta;, the discount factor, to 0.99.</summary>
<pre>class MyModel : DPparent {
    &vellip;
    static decl d;
    static decl m;
    Utility();
    &vellip;
    static Initialize();
    }
MyModel Utility(); {
    return CV(d) .== CV(m);
    }
&vellip;
MyModel::Initialize() {
    DPparent::Initialize(new MyModel(),&hellip;);
    SetClock(NormalAging,40);
    &vellip;
    d = new ActionVariable("choice",2);
    Actions(a);
    m = new LaggedAction("prevd",d);
    EndogenousStates(m);
    &vellip;
    CreateSpaces();
    SetDelta(0.99);                            // NEW
    &vellip;
    }
</pre></dd></details>
<P/>
</OL></OL>
<P/>
<a name="ES"><li>Extensions and Specializations</LI></a>
<P/>
    The <span class="n">DDP</span> code so far captures the basic elements of the general <a href="#DP-defn">DP framework.</a>  Solving the model to derive <a href="#BE-defn">Bellman's equation</a> is  discussed <a href="Methods.ox.html">separately</a>. This section adds more structure to the generic framework and shows how to build this structure into <code>MyModel</code>.</p>
<P/>
<OL class="chapter">
<li>Five State Vector<em>S</em></li>
    <blockquote><b>Sort state variables by their role in the transition &Rho;().</b></blockquote>
<P/>
<h3>The generic state space <span class="expressions">\(\theta\in \Theta\)</span> generalizes to <span class="expressions">\((\zeta,\epsilon,\eta,\theta,\gamma) \in (Z,E,H,\Theta,\Gamma)\)</span></h3>
<P/>
Following the literature special or restricted state variables are placed in different state vectors. The single vector <span class="expressions">\(\theta\)</span> is replaced by multiple vectors holding specially behaved state variables: <span class="expressions">\(\eta\)</span>, <span class="expressions">\(\epsilon\)</span>, <span class="expressions">\(\theta\)</span>,  or <span class="expressions">\(\gamma\)</span>.  The most general kinds of state variables are placed in a vector still denoted <span class="expressions">\(\theta\)</span>.</p>
<P/>
Segregation state variables into different vectors can reduce memory and computing since only the information required for restricted state variables are stored.. These distinctions matter for how <a href="DP.ox.html#DP">DP</a> solves <code>MyModel</code>, but from the point of view of <code>MyModel</code> a state variable is just a state variable regardless of which category it is placed. A generic state variable that is not associated with a particular vector is denoted <code>s</code>.</p>
<P/>
<details class="aside"><summary>Placeholder state variables</summary>
<DD>Any of the state vectors may be empty in <code>MyModel</code> except <span class="expressions">\(\theta\)</span> which always has a <a href="Variables.ox.html#Clock">Clock</a>.</DD>
<DD>If a vector is empty in <code>MyModel</code> then <span class="n">DDP</span> places a special <a href="Variables.ox.html#Fixed">Fixed</a> state variable that takes on only the value 0.  This has no effect on the size of the state space but it greatly simplifies the internal coding of algorithms.</DD>
<DD>These placeholders simplify the internal coding of the problem greatly and do nothing to expand the state space.  They do appear in output so they take up some space on the screen.</DD>
</details>
<P/>
<dfn id="SV-defn">The 5 State Vectors</dfn>
<OL class="section">
<P/>
<LI><span class="expressions">\(\theta\)</span>: Endogenous state vector</LI>
<P/>
    <DD>A generic element of <span class="expressions">\(\theta\)</span> is denoted <var>q</var>. Anything that <code>MyCode</code> places in the other state vectors could be in the <span class="expressions">\(\theta\)</span> (but not vice versa). State variables are added to <span class="expressions">\(\theta\)</span> by sending them to the function <a href="DP.ox.html#DP___EndogenousStates">EndogenousStates</a>(). Like any DDP model <span class="expressions">\(\theta\)</span> is a semi-Markov process in which the transition to <span class="expressions">\(\theta^{ \prime}\)</span> depends potentially on all current state variables and the action <span class="expressions">\(\alpha\)</span>.</DD>
<P/>
    <DD>The other state vectors separate out variables which evolve in a simpler or more restrictive way. <span class="expressions">\(\theta\)</span> always contains a single <a href="Variables.ox.html#StateBlock">StateBlock</a> derived from <a href="Variables.ox.html#Clock">Clock</a>.  See <a href="#clock">Clock Block</a> below.</DD>
<P/>
<LI><span class="expressions">\(\eta\)</span>: Semi-Exogenous state vector</LI>
<P/>
    <DD>A generic element of &eta; is denoted <var>h</var>. &eta; is a place for restricted endogenous variables whose transition probabilities are be independent of all other variables. Any element of &eta; is an IID process.  It is semi-exogenous because the current value <em>can</em> influence the transition of the exogenous state variables, <span class="expressions">\(\theta\)</span>. Semi-exogenous states are added to &eta; using <a href="DP.ox.html#DP___SemiExogenousStates">SemiExogenousStates</a>().</DD>
<P/>
<LI><span class="expressions">\(\epsilon\)</span>: Exogenous state vector</LI>
<P/>
    <DD>A generic element of <span class="expressions">\(\epsilon\)</span> is denoted <var>e</var>.  <span class="expressions">\(\epsilon\)</span> can include only  (fully) exogenous state variables and is thus more specialized than a semi-exogenous variable.  These variables are not only IID but they cannot direclty influence the transition of any other state of the system.   Elements of <span class="expressions">\(\epsilon\)</span> satisfy Rust's Conditional Independence property. States are added to <span class="expressions">\(\epsilon\)</span> using <a href="DP.ox.html#DP___ExogenousStates">ExogenousStates</a>().</DD>
<P/>
<LI><span class="expressions">\(\gamma\)</span>: Grouping (random and fixed effect) state vector</LI>
    <DD>A generic element of <span class="expressions">\(\gamma\)</span> is denoted <span class="expressions">\(g\)</span> States are added to <span class="expressions">\(\gamma\)</span> using <a href="DP.ox.html#DP___GroupVariables">GroupVariables</a>(). A grouping variable is equivalent to either a <em>random effect</em> or a <em>fixed effect</em> in a panel model.  Either way, it does not vary within the life of an agent following the DP, but from our point of view it is random for an agent. Because grouping variables do not vary within a solution it is wasteful to create space for each of their values and the other states in <span class="expressions">\(\theta\)</span>.  Instead, <span class="n">DDP</span> resolves the model for each value of <span class="expressions">\(\gamma\)</span>.  It stores differing choice probabilities across random effects for a given fixed effect value, so that integration across random effects can be carried out.</DD>
<P/>
    <DD><b><em>Warning:</em> if random effect elements of <span class="expressions">\(\gamma\)</span> affect the transition <span class="expressions">\(P()\)</span>. you have to change the <a href="DDPShared.ox.html#Flags___UpdateTime">UpdateTime</a> in your model</b> Because <span class="expressions">\(\gamma\)</span> is fixed during a solution, it is often <em>not</em> listed as an argument of U() and endogenous outcomes such as <span class="expressions">\(V(\theta)\)</span>.  However, <code>MyModel</code> can treat elements of <span class="expressions">\(\gamma\)</span> like other states.</DD>
<P/>
<LI><span class="expressions">\(\zeta\)</span>: Continuous value shock vector</LI>
    <blockquote><b>Continuous states are placed in <span class="expressions">\(\zeta\)</span> and can only affect U() not <span class="expressions">\(P()\)</span>.</b></blockquote>
<P/>
    <DD>The most specialized random elements are those that are IID, do not influence the transition of other state variables <em>and</em> enter utility as an additively separable shock. These are often random variables with infinite support that smooth choice probabilities. Elements of <span class="expressions">\(\zeta\)</span> are not really stored.  Rather the distribution of <span class="expressions">\(\zeta\)</span> affects the specification of Bellman's equation, specifically the expression for <span class="expressions">\(EV(\theta^{ \prime})\)</span>. If any such shocks are in the model, then a solution method must be available to deal with them explained below in <a href="#">Solution Method</a>. Any continuous shocks <span class="expressions">\(\zeta\)</span> are not included in <span class="expressions">\(U()\)</span> as the user codes it.  Instead, the distribution of continuous shocks is accounted for by the algorithm to compute <span class="expressions">\(EV(\theta^{ \prime})\)</span>.</DD>
<P/>
</OL>
<P/>
<img src="icons/statevectors.png" width="60%">
<P/>
<li>Feasible Actions at a state: <span class="expressions">\(\theta\)</span>.A</li>
<P/>
    <blockquote><b><code>MyModel</code> can account for limits on choice conditional on the endogenous state.</b></blockquote>
<P/>
    <h3>The generic action space <span class="expressions">\(\alpha \in A\)</span> generalizes to <span class="expressions">\(\alpha \in A(\theta)\)</span>.</h3>
<P/>
The matrix of possible actions, <code>&Alpha;</code>, was defined above as the Cartesian product of the ranges of all action variables.  However, in many cases <code>MyModel</code> may rule out certain actions as not logically possible at a particular state. Or some actions are ruled infeasible for convenience to avoid calculations that are relatively unimportant to the overall goal of the model.</p>
<P/>
<em>Feasibility</em> is a property <code>MyModel</code> imposes on <span class="expressions">\(\alpha\)</span>. The model rules out some actions given the interpretation of <span class="expressions">\(\alpha\)</span>. In dynamic programming, the set of feasible actions can depend on the current state, <span class="expressions">\(\theta\)</span>.  In typical math notation it would be natural to write this as <span class="expressions">\(A(\theta)\)</span>, where <span class="expressions">\(A()\)</span> is now a matrix-valued function of the state. Instead, write feasibility as a <em>property</em> of the state</p>
<P/>
The feasible actions at <span class="expressions">\(\theta\)</span> is a matrix property: <span class="expressions">\(\forall \theta \in \Theta\)</span>,   <span class="expressions">\(\theta.A  \subseteq  A\)</span>. <span class="n">DDP</span> does not allow exogenous states to affect the choice set.  So <code>MyModel</code> must assign a variable that affects feasible actions to <span class="expressions">\(\theta\)</span> even if its transition would otherwise qualify for exogenous or semi-exogenous status. A different way to handle infeasible choices is to have <code>MyModel::U()</code> return numeric -&infin; as the utility for any infeasible <span class="expressions">\(\alpha\)</span>.  This option is always open for use in <code>MyModel</code>, but it does not the size of the static optimization problem and is not as close to the standard notation.</p>
<P/>
By default all possible actions are feasible at all <span class="expressions">\(\theta\)</span> because the built-in <a href="Bellman.ox.html#Bellman___FeasibleActions">FeasibleActions</a>() specifies this. If <code>MyModel</code> does not say otherwise, <span class="expressions">\(\theta.A \equiv A\)</span> for all endogenous states. <code>MyModel</code> can restrict choices by providing a replacement for the <em>virtual</em> method <code>Bellman::FeasibleActions()</code>. <code>MyModel::FeasibleActions</code> returns a column vector which indicates that <span class="expressions">\(\alpha\)</span> is feasible or not:
<DD><pre>FeasibleActions() returns a vector of length A.D containing I{A.i&in;&theta;.A}, i = 0 &hellip; (A.D)&oline;.
</pre></DD>
The default method returns a vector of 1s equal in size to the rows of the unrestricted action space. This means that <code>MyModel</code> can define feasibility without knowing everything about the model. Indeed, another user may be deriving their model from yours, adding additional choice variables that you did not anticipate.  Even so, your feasibility conditions can still be imposed regardless of the presence of other columns and rows of <var>A</var>.</p>
<P/>
Typically there is a small number of different feasible sets relative to the size of the state space.  In this case, storing a matrix at each <span class="expressions">\(\theta\)</span> is wasteful.  So <span class="n">DDP</span> stores a list (<code>OxArray</code>) of different feasible sets.  Rather than storing <span class="expressions">\(\theta\)</span>.A it only stores an index <span class="expressions">\(\theta\)</span>.j into a list of feasible sets. In <span class="n">DDP</span>, the list of feasible matrices is simply <code>A</code>.  And the index <span class="expressions">\(\theta\)</span>.j into the list at a state is <code>Aind</code>. <code>MyModel</code> accesses the current feasible matrix as <code>Alpha::A[Aind]</code>. The first matrix, <code>Alpha::A[0]</code> is <em>always</em> the possible matrix <var>A</var>.  If <code>MyModel</code> does not specify feasible actions, then <code>Aind =  0</code>.</p>
<P/>
<b>Note</b>: the elements of the <var>A</var> list are the <em>actual</em> value of actions and are updated at the start of each value solve.  If an action variable does not have its own <a href="Shared.ox.html#Discrete___Update">Update</a>() routine defined then the actual values are simply the default range 0 &hellip; (a.N)&oline;.</p>
<P/>
In <span class="n">DDP</span> the key functions, U() and &Rho;() act on a single point in the state space at a time.  So the current value of a state variable is placed in the <code>.v</code> property of the Ox variable representing it. On the other hand, both U() and &Rho;() are 'vectorized' in actions: they must operate on the whole feasible matrix at once.  Action variables have the <code>.v</code> property, but it is not used for them.  Their current values are in a column of the <span class="expressions">\(\theta\)</span>.A matrix, which is <code>A[Aind]</code> in the code.  The previously described functions <span class="expressions">\(CV()\)</span> and <span class="expressions">\(AV()\)</span> work with action variables as well, returning the column of the current/actual action matrix at <span class="expressions">\(\theta\)</span>.
<details class="aside"><summary>For example</summary> Consider a model that has two choices: work hours and whether to volunteer or not.   Then at some state <span class="expressions">\(\theta\)</span>.A may look like this, along with the return value of <code>a(work)</code>.
<DD><pre>     A[Aind]     |
work      vol    |  CV(work)
 -------------------------
 0         0      |    0
 1         0      |    1
 2         0      |    2
 0         1      |    0
 1         1      |    1
 2         1      |    2
</pre></DD></details></p>
<P/>
<a name="TS"><LI>Terminal States and Terminal Values of State Variables</LI></a>
<P/>
    <blockquote><b><code>MyModel</code> can make values of a state variable terminal by calling <a href="Variables.ox.html#StateVariable___MakeTerminal">MakeTerminal</a>().</b></blockquote>
<P/>
<dfn id="Terminal-State-dfn">Some dynamic programs end if and when certain states are encountered.
Such states are called <em>terminal states</em>. </dfn> There are three features of terminal state:
<OL class="steps">
<LI>There is no more choice from that period on.  Often a model is set up so that it terminates the program when an action is taken.  That is coded in <span class="n">DDP</span> as a transition to a next state that is terminal.</LI>
<LI>Transitions from terminal states are undefined (or ignored).</LI>
<LI>The value of a terminal state is exogenous to the model and not solved as part of <a href="#BE-defn">Bellman's equation</a>.  In <span class="n">DDP</span> the terminal value is returned as the utility of the state (not a function of the non-existent choice).</LI>
</OL>
<P/>
<dfn id="Terminal-Value-dfn">If a value of a state variable makes a state terminal it is called a <em>terminal value</em>.</dfn> <span class="n">DDP</span> considers termination a property of value(s) of an endogenous state variable which the whole state <span class="expressions">\(\theta\)</span> inherits.
<DD><pre>   q.T is a subset of the possible values of q that terminate decision making.
</pre></DD>
By default: q.T = &empty; for built-in state variables. <code>MyModel</code> makes values terminal by applying <code>q-&gt;<a href="Variables.ox.html#StateVariable___MakeTerminal">MakeTerminal</a>()</code> to it.  Only endogenous state variables, those in <span class="expressions">\(\theta\)</span>, can have terminal values since other state vectors are either IID or invariant.</p>
<P/>
The set of terminal states is defined as <span class="expressions">\(\overline{\Theta}\)</span>. A state is terminal if any of the endogenous state variables currently equal a terminal value.
<span class="equation">$$\theta.T\quad =\quad I\{ \hbox{for some }k, \theta.v_k \in q_k.T \}.$$</span>
The convention in <span class="n">DDP</span> is that at a terminal state there is no choice, and <code>MyModel</code> must provide a value for the state via U(). Because of this convention, <code>MyModel::FeasibleActions()</code> is <em>not</em> called at a terminal state.  Instead, for <span class="expressions">\(\theta \in  \overline{\Theta}\)</span>, <span class="expressions">\(\theta\)</span>.A is automatically equal to the first row of <span class="expressions">\(A\)</span>.</p>
<P/>
Let <span class="expressions">\(\overline{V}(\theta)\)</span>, for <span class="expressions">\(\theta \in \overline{\Theta}\)</span> be the exogenous value of arriving at a terminal state <span class="expressions">\(\theta\)</span>. <code>MyModel::Utility()</code> returns this value.  <span class="n">DDP</span> sets it as <span class="expressions">\(V(\theta)\)</span> directly.</p>
<P/>
<li>Trimming The State Space <span class="expressions">\(\Theta\)</span> of Unreachable States</li>
<P/>
    <blockquote><b><code>MyModel</code> can trim the state space by providing a <code>MyModel::Reachable()</code> routine.  It returns <code>TRUE</code> for states that can be reached from initial conditions and <code>FALSE</code> otherwise. </b></blockquote>
<P/>
Following the notion of possible versus feasible actions above, the Cartesian product of all possible values of the endogenous state variables is defined as the <em>possible state space</em>:
    <span class="equation">$$ \Omega \quad\equiv\quad \prod_{q_k \in\theta} \{ 0 \dots q_{_{k.N^-}} \}$$</span>
The current value of a state is always equal to some row in <span class="expressions">\(\Omega: \theta.v \in \Omega\)</span>.</p>
<P/>
Just because a state is possible (<span class="expressions">\(\theta\in\Omega\)</span>) does not mean the DP can ever get there.  Of course, if the DP was solved and then started at exactly <span class="expressions">\(\theta\)</span> it would be reached.  But finite horizon dynamic programs must have initial conditions specified.  And those initial conditions can mean that some states in <span class="expressions">\(\Omega\)</span> will never be reached.  And if they can't be reached then they do not need to be stored and included in the solutions to <a href="#BE-defn">Bellman's Equation</a>. So in some models, especially those with a finite horizon, &Omega; contains many endogenous states that cannot be reached from possible initial states of the user's situation.
    <details class="aside"><summary>The term Reachable</summary><DT>The term <em>reachable</em> is used for two reasons.  </DT><DD>First, in <span class="n">DDP</span> the term <em>feasible</em> applies to action sets not states.  </DD><DD>And, second, reachable emphasizes the fact that it depends on what initial states are assumed not logical inconsistency.)</DD></details></p>
<P/>
As with feasibility of actions, <em>reachability</em> of a state is not a mechanical property.  Rather it depends on the model and how it will be used.  Since, in <span class="n">DDP</span>, an endogenous state <span class="expressions">\(\theta\)</span> is not just a vector of numbers but rather an object with many properties attached it, it is important for efficiency that <span class="n">DDP</span> only create and process objects for reachable states.</p>
<P/>
The property <span class="expressions">\(\theta\)</span>.R equals 1 if <code>MyModel</code> specifies that <span class="expressions">\(\theta\)</span> is reachable.  Otherwise <span class="expressions">\(\theta\)</span>.R = 0. The state space <span class="expressions">\(\Theta\)</span> is the set of reachable states within the set of all possible states.  It emerges from the property <span class="expressions">\(\theta\)</span>.R of each logically possible state:
<span class="equation">$$\Theta\quad \equiv\quad  \bigl\{  \theta \in \Omega\ :\ \theta.R = 1  \bigr\}$$</span>
    <details><summary>Storage Details</summary>Note that <code>R</code> is a conceptual  property only.  During computation objects are only created for states with &theta.R=1.  So the actual test is whether a possible state contains an object or just the number 0 as a placeholder. Storing a 0 for possible but unreachable states still requires allocation of a single <code>oxvalue</code>, but it avoids storage of vectors and matrices associated with each reachable state (such as the utility vector, the optimal choice probability matrix, etc.).</details></p>
<P/>
As shown above, <code>MyModel</code> must call <code>DPparent::CreateSpaces()</code>, which sets up the list of feasible action matrices and creates the state space <span class="expressions">\(\Theta\)</span>.  It must traverse (loop over) the possible state space &Omega; at least once  It is during this traversing that it is determined whether the point <span class="expressions">\(\theta\)</span> is reachable or not.  <code>CreateSpaces()</code> uses two methods to determine reachability:  <em>inherent reachability</em> of included state variables and the user supplied routine that <em>asserts reachability</em> or not.</p>
<P/>
Some state variables generate <em>inherently</em> unreachable states. Above it was emphasized that reachability depends on the whole model, but including some kinds of state variables in your model will generate unreachable states in non-stationary environments (such as a finite decision horizon). As an example, a <a href="Variables.ox.html#Counter">Counter</a> state variable counts how many times an action or state has occurred in the past.  If the counter starts at 0 in a finite horizon model then the only reachable states are ones in which this state variable's current value is less than or equal to the value of the clock, <a href="DDPShared.ox.html#I___t">t</a>.  This is true regardless on other state variables added to the model, which actions are feasible, etc.  It depends solely on the clock type, initial conditions and the presence of this variable in the model. </p>
<P/>
So some trimming of the state space can be done automatically based on state variables in the endogenous vector <em>and</em> the model's <a href="Variables.ox.html#Clock">Clock</a> block.  By design, state variable objects do not know directly about the model's clock, because all the discrete variable classes are defined and used by the <a href="DP.ox.html#DP">DP</a> class.  So checking for reachable requires sending the clock to the state variable. The <a href="Variables.ox.html#StateVariable">StateVariable</a> class includes a virtual method named <a href="Variables.ox.html#StateVariable___IsReachable">IsReachable</a>() which takes a single argument that will be the model's clock block.  The default function returns <code>TRUE</code>.  That is, by default, no state variable says a point in the state space is unreachable.  However, some kinds of state variables that do generate unreachable states (such as counters) will supply a replacement copy of <code>IsReachable()</code>.  </p>
<P/>
At each <em>possible</em> state in the space &Omega; <code>CreateSpaces</code> will call <code>IsReachable()</code> for each state variable in the endogenous state vector <span class="expressions">\(\theta\)</span>. If any of them return <code>FALSE</code> that point is marked as unreachable. (This can be turned off when creating the state variable using the optional <code>Prune</code> argument. If no state variables claim the current state is inherently unreachable then the user supplied method is called which can mark states unreachable for reasons not inherent to the clock and state variables themselves.</p>
<P/>
<code>MyCode</code> must first call <code>DPparent::Initialize()</code>, then add variables to the model, then call <a href="DP.ox.html#DP___CreateSpaces">CreateSpaces</a>.   An object of <code>MyModel</code> is sent to <code>DP::Initialize()</code> which will <q>clone</q> it for each reachable point in the state space.  As with <code>Bellman::FeasibleActions</code>, which must have that name because it is virtual function, this function must have the name <code>Reachable()</code>.</p>
<P/>
The base <code>Bellman</code> class has a method  <code><a href="Bellman.ox.html#Bellman___Reachable">Reachable</a>()</code> which simply returns TRUE.  So if <code>MyModel</code> does not provide its own version of <code>Reachable</code> all possible states are asserted as reachable. However, remember that some state variables may have inherently unreachable states in finite horizon models, and these conditions will be checked regardless of whether <code>MyModel</code> provides its own <code>Reachable()</code>.)</p>
<P/>
If <code>MyModel</code> provides a replacement for the virtual <code>Reachable</code> it must return TRUE or FALSE depending on whether the current values of state variables are a reachable state. Inside <code>CreateSpaces()</code> and during the loop over the possible state space <span class="expressions">\(\Omega\)</span>, and if no state variables assert inherent unreachability of their current value, then <code>MyModel::Reachable()</code> is called. </p>
<P/>
<code>MyModel::Reachable()</code> indicates <span class="expressions">\(\theta\)</span> is reachable by returning TRUE.  Otherwise it should return FALSE to indicate something about the current values of all endogenous state variables makes this point unreachable.  So <code>Reachable()</code> returns 1   if <span class="expressions">\(\theta.R = 1\)</span> and 0 if <span class="expressions">\(\theta.R = 0\)</span>.</p>
<P/>
<DT>How should <span class="n">DDP</span> traverse <span class="expressions">\(\Theta\)</span>?</DT>
    <DD><code>CreateSpaces()</code> traverses &Omega; once in order to create <span class="expressions">\(\Theta\)</span>.  Thereafter the user has an option for how to  traverse <span class="expressions">\(\Theta\)</span>. If &Omega; is much larger than <span class="expressions">\(\Theta\)</span> then it makes sense to store <span class="expressions">\(\Theta\)</span> as a list of feasible states (actually a matrix of state vectors, <span class="expressions">\(\theta\)</span>.v).  <span class="n">DDP</span> will only create a space (a list) of states equal to the size of <span class="expressions">\(\Theta\)</span>.  It then loops over the list of state vectors which map directly into <span class="expressions">\(\Theta\)</span>. Otherwise,  <span class="n">DDP</span> can traverse <span class="expressions">\(\Theta\)</span> by looping over the possible values of each endogenous state variable, but ignoring at states that are not reachable. </DD>
<P/>
<a name="newMyModel"><DT><code>MyModel</code> returns a new <code>MyModel</code>???</DT></a>
<P/>
    <DD>Recall that <code>MyModel</code> is a <em>class</em> derived from some DDP, denoted <code>DDPparent</code>.  A DDP is designed to represent both the overall model <em>and</em> an endogenous state <span class="expressions">\(\theta\)</span>.  <span class="n">DDP</span> creates a copy (an object) of <code>MyModel</code> for each reachable <span class="expressions">\(\theta\)</span>.  It places them on a list named <code>Theta</code>.   To conserve memory, only a limited number of variables (properties) are specific to each object for different <span class="expressions">\(\theta\)</span>'s.  These are what Ox calls <em>automatic</em> variables. Most properties (class members) for <code>MyModel</code> are <em>static</em> members.  They are shared by all objects of type <code>MyModel</code>.  These are properties of the overall model. To conserve space, the Ox variables (members) in <code>MyModel</code> that hold actions and states should by declared <code>static</code> (see code above).  Otherwise, if variables are automatic new storage for them is created at each point in <span class="expressions">\(\theta\)</span> even though <span class="n">DDP</span> processes one <span class="expressions">\(\theta\)</span> at a time.  By storing elements as static and then updating their current value (<code>.v</code> property) storage for large state spaces is reduced dramatically. </DD>
<P/>
<li>Multiple Dynamic Programs: The Group Space &Gamma;</li>
<P/>
    <blockquote><b><code>MyModel</code> can require several solutions to a DP model that differ only by shifts in <span class="expressions">\(U()\)</span> or <span class="expressions">\(P()\)</span>.</b></blockquote>
<P/>
Group variables are like random or fixed effects in econometrics.  They are fixed and non-random from an agent's point of view, but from our point of view they vary across agents. Group variables are not involved in the creation of <span class="expressions">\(\Theta\)</span>, which is reused for the solution of the model for each <span class="expressions">\(\gamma\)</span>. Instead, the group space <span class="expressions">\(\Gamma\)</span> is created from the Cartesian product of all possible values of the group variables.
<span class="equation">$$\Gamma\quad\equiv\quad\prod_{k=0\dots\gamma.N^-}\ \bigl\{\,0\dots\, \gamma.N^-\,\bigr\}$$</span>
<details class="aside"><summary>Example</summary>
<dd>There are two group variables, <code>&gamma; = (g,d)</code>, where g is gender, so g.N&oline;=1 and d is degree status so
d.N&oline; =2, (no high school degree, high school, some college).  Then <code>&Gamma; = {0,1}&times;{0,1,2} = </code>
<pre>0 0
1 0
0 1
1 1
0 2
1 2
</pre></dd></details>
<!--<pre>&Gamma; &equiv; &times;{ 0 &hellip; (g<sub>k</sub>.N)&oline; }, for k= 0 &hellip; (&gamma;.N)&oline;.
</pre></dd>-->
Each group has a probability
<span class="equation">$$P_g(\gamma)\quad=\quad \prod_{k=0\dots \gamma.N^-}\ p(g_k.v)$$</span>
This assumes group variables are iid.  You can use fixed and random effect blocks to include correlated groups.
<!--<pre>&Rho;<sub>g</sub>(&gamma;) = &prod; <sub>k=0&hellip;(&gamma;.N)&oline;</sub>  p(g<sub>k</sub>.v)
</pre></DD>-->
<!--There is no mechanism to mark some groups as unreachable.<-->
<P/>
<DT>Choice Probabilities: <span class="expressions">\(P*\)</span></DT>
<P/>
    <DD><span class="n">DDP</span> solves the DP model for each group vector <span class="expressions">\(\gamma \in \Gamma\)</span>. At each <span class="expressions">\(\theta \in \Theta\)</span>, <span class="expressions">\(P*( \alpha\ ;\ \epsilon, \eta, \theta, \gamma )\)</span> is stored as a matrix in <span class="expressions">\(\alpha\)</span> and <span class="expressions">\(\epsilon \times \eta\)</span> and a list (OxArray) in <span class="expressions">\(\gamma\)</span>. </DD>
<P/>
</OL>
<P/>
<a name="VS"><LI><span="n">DDP</span> terminology versus other surveys and methods articles</LI></a>
<P/>
<OL class="chapter">
Most contributions to the DDP literature adopt some idiosyncratic notation or terminology, and the current document is no exception.  Here is a translation of the current notation into that used in surveys and papers that have become a standard.  Features that are similar are not listed and neither are elements of alternative notation that is somehow more general than that used here.  The DDP notation is followed by &rarr; and the alternative notation in <var>italic</var> face.  A brief explanation and/or possible reason why the DDP notation is preferable is then given in [&nbsp;].</p>
<P/>
A key reason the notation differs here is because it is used to describe a framework for designing a DDP and solving it efficiently. Most other notation is used to describe a specific model or to describe models generally without reference to restrictions that can be used for efficiency.</p>
<P/>
<LI>Aguirregabiria &amp; Mira (JoE 2010)</LI>
<DT>Actions:
<DD><code>&alpha; &nbsp;&nbsp;&rarr;&nbsp;&nbsp;</code> <var>(a)</var>  [Here, vectorized actions do not retain dimensions of choice]
<DD>A.D &nbsp;&nbsp;&rarr;&nbsp;&nbsp; <var>J</var>
<DD>a &nbsp;&nbsp;&rarr;&nbsp;&nbsp; <var>a<sub>it</sub></var> </DD>
<DT>Clock:
<DD>t &in; &theta; &nbsp;&nbsp;&rarr;&nbsp;&nbsp; <var>t as a subscript</var>. [Here, subscripts are used for other properties and t is in the state already]
<DT>Discrete States:
<DD>&theta; &nbsp;&nbsp;&rarr;&nbsp;&nbsp; <var>x<sub>it</sub></var>.  [Here, allow Greek vectors to be distinguished from Roman variables]
<DD>&epsilon; and &eta; &nbsp;&nbsp;&rarr;&nbsp;&nbsp; <var>empty vectors</var> [Here, discrete exogenous state variables save space and computation]
<DT>Transitions:
<DD>&Rho;(&theta;&prime;|&alpha;,&theta;) &nbsp;&nbsp;&rarr;&nbsp;&nbsp; <var>f<sub>x</sub>(x<sub>i,t+1</sub>|a<sub>it</sub>,x<sub>it</sub>)</var> [Here, distinguish upper case functions from lower case vectors and variables]
<DT>Continuous States:
<DD>&zeta; = <var>(&epsilon;<sub>it</sub>)</var>, always with size A.D [Here, &zeta; may have lower dimension that the action space.]
<DD>utility is always additively separable (their assumption AS).
<DT>Bellman:
<DD>EV(&theta;) &nbsp;&nbsp;&rarr;&nbsp;&nbsp; <var><span class="o">V</span>(x<sub>it</sub>)</var> [mnemonic for <b>E</b>xpected <b>V</b>alue]
<DD>&Rho;*(<code>&alpha;</code>|&theta;) &nbsp;&nbsp;&rarr;&nbsp;&nbsp; <var>P(a|x,&theta;)</var> [Distinguished from primitive &Rho;() even when arguments are suppressed.]
<DT> Miscellaneous <DD>&delta; &nbsp;&nbsp;&rarr;&nbsp;&nbsp; <var>&beta;</var> [Here, mnemonic for <b>d</b>iscount factor.] </DD>
<P/>
<li>Keane, Todd &amp; Wolpin (Handbook of Labor Economics 2011)</li>
<DT>Actions;
<DD><code>&alpha;</code> = (a<sub>0</sub>&hellip; a<sub>&alpha;.N&oline;</sub>) &nbsp;&nbsp;&rarr;&nbsp;&nbsp; <var>(d<sup>00&hellip;1</sup> d<sup>10&hellip;0</sup> &hellip; d<sup>11&hellip;1</sup>)</var>
 <DD>&alpha;.D &nbsp;&nbsp;&rarr;&nbsp;&nbsp; <var> the length of the superscript</var>;
 <DD><var>d<sup>&hellip;</sup>.N = 2, and &sum; d<sup>&hellip;</sup> = 1.</var>
<DT>Discrete States:
<DD>&theta; &nbsp;&nbsp;&rarr;&nbsp;&nbsp; <var>&Omega;&oline;<sub>it</sub></var>
<DD>&epsilon; and &eta; &nbsp;&nbsp;&rarr;&nbsp;&nbsp; <var>empty vectors</var>
<DT>Continuous States:
<DD>&zeta;&nbsp;&nbsp;&rarr;&nbsp;&nbsp;<var>S(&Omega;&oline;<sub>it</sub>)</var> [loses self-standing vector status]
<DT>Bellman:
<DD>Choice Probabilities: &Rho;*(&alpha;|&theta;) &nbsp;&nbsp;&rarr;&nbsp;&nbsp; <var>Pr( d<sup>&hellip;</sup>=1 |&Omega;&oline;<sub>it</sub>)</var>
<DD>v(&alpha;|&eta;,&epsilon;,&theta;,&gamma;) &nbsp;&nbsp;&rarr;&nbsp;&nbsp; <var>V<sup>&hellip;</sup></var></DD>
<P/>
</DD>
</OL>
<a name="DS"><li>Designing and writing <code>MyModel</code></li></a>
<P/>
<OL class="chapter">
<li>Overview</li>
 Before programming, state <code>MyModel</code> in the notation used above (which combines standard mathematical statement of DP and some peculiarities of <span class="n">DDP</span>.)
<DL>
<DT>Determine the timing in your model (stationary, finite horizon, etc.).</DT>
<P/>
<DT>List the <a href="ActionVariable.ox.html">action variables</a> in <span class="expressions">\(\alpha = (a_0,a_1,\dots)\)</span>.</DT>
    <DD>Many papers vectorize the action variables into one long choice.  But DDP makes it easy to separate different dimensions of choice so that each variable is meaningful.</DD>
<DT>List and classify <a href="StateVariable.ox.html">state variables</a> and state blocks.</DT>
    <DD>For each state variable/block, either find a predefined class that it matches (up to the value of parameters to the creator routine for the class) or derive a new transition for it. If two or more variables are conditionally correlated (coevolving), place them in a state block and derive a transition. Specify the dependency of the transition on the current values of other state variables and actions.  </DD>
<DT>Based on these dependencies assign each state variable to  <span class="expressions">\(\epsilon\)</span>, <span class="expressions">\(\eta\)</span> and <span class="expressions">\(\theta\)</span>.</DT>
<DT>Terminal values: if values of endogenous states end decision-making</DT>
    <DD>Express as a value or set of values of variables in &theta; that terminate decisions. Use the method <a href="Variables.ox.html#StateVariable___MakeTerminal">MakeTerminal</a>() to mark values as terminating. This implicitly defines <span class="expressions">\(\overline{\Theta}\)</span></DD>
<DT>Reachability if not every endogenous state can be reached</DT>
    <DD>Express &theta;.R as a logical/boolean value depending on the values of variables in &theta;. This implicitly defines <span class="expressions">\(\Theta\)</span></DD>
<DT>Fixed group variables in &gamma;</DT>
    <DD>For each group variable, find a predefined random effect variable that matches it or derive a new type of group variable.</DD>
<DT>Constraints on choice: if not every action can be taken at every state</DT>
    <DD>Express feasible actions <span class="expressions">\(\theta\)</span>.A, as a logical/boolean indicator for whether <span class="expressions">\(\alpha\)</span> is feasible (in <span class="expressions">\(\theta\)</span>.A) depending on values of variables in <span class="expressions">\(\theta\)</span>.</DD>
<DT><span class="expressions">\(\zeta\)</span> : if continuous variables enter choice value</DT>
    <DD>Specify the distribution of  <span class="expressions">\(\zeta\)</span> and the solution method associated with it.
<DT>Utility <span class="expressions">\(U()\)</span></DT>
    <DD>Expressed as a vector of values, one number for each row of <span class="expressions">\(\theta\)</span>.A. (each value of <code>&alpha;</code>). Values depend on current values of state variables in all the vectors, but not &zeta; unless the model will be solved with reservation values.</DD>
</DL>
<P/>
<LI>Steps</LI>
<OL class="steps">
<LI>Choose the DDP <code>struct</code> that  <code>MyModel</code> is derived from, referred to as <code>DDPparent</code>.</LI>
        <DD>See <a href="Methods.ox.html">Methods</a> for the solving and smoothing methods available.</DD>
<P/>
<LI>Write the declaration and definition of <code>MyModel</code> and any other derived elements needed in the model.</LI>
<P/>
        <DD>See <a href="Variables.ox.html">Variables</a> to create custom state variables,state blocks and action variables. Decide if you want to use the <code>#import</code> or <code>#include</code> approach to using <code>MyModel</code> in an Ox program.
        <pre> #import "MyModel"
</pre>
    requires two separate files: <code>MyModel.h</code> and <code>MyModel.ox</code>
        <pre> #include "MyModel.ox"
</pre>
    requires one file <code>MyModel.ox</code> which includes what would be in the header and ox file.  You can have a separate <code>MyModel.h</code> file, but you may need to use <code>conditional define directives</code> to avoid multiple inclusions.</DD>
<P/>
        <DD>The Header Material (to go in a file such as <code>MyModel.h</code>). For each derived element, write a <code>struct</code> declaration.</DD>
<P/>
        <DD>The .ox Material. For each derived element, define the required and optional methods required of <code>MyModel</code> and its components.      Put this material in <code>MyModel.ox</code>.</DD>
<P/>
<LI>Write an Ox program that <code>includes</code> or <code>imports</code> the definitions of the elements then builds up the model and solves it</LI>.
</OL>
<LI>Steps the program should execute in building the model</LI>
<OL class="steps">
<LI>Call <code>DDPparent::Initailize()</code> for the base of <code>MyModel</code></LI>
<LI>Set the model clock with <a href="DP.ox.html#DP___SetClock">SetClock</a>().</LI>
    <DD>Some DDPs require a <code>new</code> clock variable be created first and sent to <code>Initialize()</code>.  For these methods you do not call <code>SetClock</code>; it will be called by <code>Initalize()</code>.</DD>
<LI>Create <code>new</code> instances for the action variables and the state variables in the model.</LI>
<LI>Add the action and state variables to the model using DP methods such as <a href="DP.ox.html#DP___Actions">Actions</a>()</LI>
<LI>Call <a href="Bellman.ox.html#Bellman___CreateSpaces">CreateSpaces</a>(), sending it a static routine that indicates states are reachable and whether <span class="expressions">\(\Theta\)</span> should be traverse with a loop or a list</LI>
    <hr><blockquote>Items above are done once while the program runs.  They can be repeated only after calling<a href="Bellman.ox.html#Bellman___Delete">Delete</a>() which disposes of the elements of the previous model.  Items below can be done repeatedly during the life of the program once the steps above are done.</blockquote><hr>
<LI>Set parameters of the model, including the discount factor <span class="expressions">\(\delta\)</span>.</LI>
</OL>
<P/>
</OL>
<P/>
</OL>

<dl><dt class="author">Author:</dt><dd class="author">&copy; 2011-2019 <a href="http://econ.queensu.ca/~ferrall">Christopher Ferrall</a></dd></dd>
</dl>
<div class="footer">
Generated by <a href="http://oxdoc.sourceforge.net">oxdoc 1.1-beta</a> &copy Copyright 2005-2014 by Y. Zwols<br>
Math typesetting by <a href="http://www.mathjax.org/">Mathjax</a>
</div>
