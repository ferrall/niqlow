Output of DDP Tests:Outcomes-Simulation
---------------------------
DP::Intialize is complete. Action and State spaces are empty.
 Log file name is: logs/DP-DDP-7-06-2021-12-42-10.log
-------------------- DP Model Summary ------------------------
0. USER BELLMAN CLASS
    Test7 | Rust  | Exteme Value  | Bellman
1. CLOCK
    1. Ergodic
2. STATE VARIABLES
              |eps   |eta |theta -clock        |gamma
              s11    s21      x      t     t'      r      f
       s.N      1      1     12      1      1      1      1


     Transition Categories (not counting placeholders and variables with N=1)
                 NonRandom       Random   Coevolving    Augmented       Timing    Invariant
     #Vars               0            1            0            0            0            0

3. SIZE OF SPACES

                       Number of Points
    Exogenous(Epsilon)                1
    SemiExogenous(Eta)                1
   Endogenous(Theta_t)               12
                 Times                1
         EV()Iterating               12
      ChoiceProb.track               12
         Random Groups                1
 Dynamic Random Groups                1
          Fixed Groups                1
   Total Groups(Gamma)                1
       Total Untrimmed               12

4. ACTION VARIABLES
   Number of Distinct action vectors: 2
             a
    a.N      2


5. TRIMMING AND SUBSAMPLING OF THE ENDOGENOUS STATE SPACE (Theta)
                           N
    TotalReachable        12
         Terminal          0
     Approximated          0
Index of first state by t (t=0..T-1)
      0     11


6. FEASIBLE ACTION SETS
 
     i    [a]        A[0]   
     ---------------------------
     000 (0)           X        
     001 (1)           X        
        #States       12
     --------------------------
         Key: X = row vector is feasible. - = infeasible

-------------------- End of Model Summary ------------------------
Phase :Initializing Increment:       0.01. Cumulative:         0.01

>>>>>>Value Iteration Starting

>>>>>>Value Iteration Finished.  Succeed: 1
Phase :Solving Increment:       0.65. Cumulative:         0.65

     Value of States, Ergodic Distn, and Choice Probabilities 
    Indx   T   A   x   t     r     f       EV      |  Erg.Distn  |Choice Probabilities:         
      11   1   0  11   0     0     0      -68.214190  0.0000507 0.219181 0.780819
      10   1   0  10   0     0     0      -68.161549  0.0002258 0.259221 0.740779
       9   1   0   9   0     0     0      -68.087471  0.0009979 0.312113 0.687887
       8   1   0   8   0     0     0      -67.990681  0.0036367 0.375573 0.624427
       7   1   0   7   0     0     0      -67.864030  0.0108956 0.449854 0.550146
       6   1   0   6   0     0     0      -67.696144  0.0268122 0.534879 0.465121
       5   1   0   5   0     0     0      -67.469979  0.0543180 0.629026 0.370974
       4   1   0   4   0     0     0      -67.160298  0.0914520 0.727823 0.272177
       3   1   0   3   0     0     0      -66.731054  0.1238224 0.822813 0.177187
       2   1   0   2   0     0     0      -66.135275  0.1469612 0.902346 0.097654
       1   1   0   1   0     0     0      -65.321500  0.1578754 0.956722 0.043278
       0   1   0   0   0     0     0      -64.246718  0.0509028 0.985226 0.014774
     ------------------------------------------------------------------------------
Ptrans 
      0.39190     0.016961     0.038271     0.069440      0.10667      0.14538      0.18228      0.21560      0.24471      0.26958      0.29031      0.30600
      0.59530      0.40070     0.058133      0.10548      0.16203      0.22084      0.27689      0.32750      0.37172      0.40950      0.44099      0.46482
     0.012800      0.57009      0.35488    0.0022680    0.0034839    0.0047485    0.0059535    0.0070419    0.0079927    0.0088049    0.0094820    0.0099945
      0.00000     0.012246      0.53717      0.32246      0.00000      0.00000      0.00000      0.00000      0.00000      0.00000      0.00000      0.00000
      0.00000      0.00000     0.011550      0.48982      0.28523      0.00000      0.00000      0.00000      0.00000      0.00000      0.00000      0.00000
      0.00000      0.00000      0.00000     0.010532      0.43327      0.24652      0.00000      0.00000      0.00000      0.00000      0.00000      0.00000
      0.00000      0.00000      0.00000      0.00000    0.0093161      0.37446      0.20962      0.00000      0.00000      0.00000      0.00000      0.00000
      0.00000      0.00000      0.00000      0.00000      0.00000    0.0080515      0.31841      0.17630      0.00000      0.00000      0.00000      0.00000
      0.00000      0.00000      0.00000      0.00000      0.00000      0.00000    0.0068465      0.26780      0.14719      0.00000      0.00000      0.00000
      0.00000      0.00000      0.00000      0.00000      0.00000      0.00000      0.00000    0.0057581      0.22358      0.12232      0.00000      0.00000
      0.00000      0.00000      0.00000      0.00000      0.00000      0.00000      0.00000      0.00000    0.0048073      0.18580      0.10159      0.00000
      0.00000      0.00000      0.00000      0.00000      0.00000      0.00000      0.00000      0.00000      0.00000    0.0039951      0.15763      0.21918

     0.062977     0.050903
      0.15486      0.15788
      0.14394      0.14696
      0.12080      0.12382
     0.088433     0.091452
     0.054318     0.054318
     0.026812     0.026812
     0.010896     0.010896
    0.0036367    0.0036367
   0.00099787   0.00099787
   0.00022583   0.00022583
   5.0696e-05   5.0696e-05
Tracking all actions, endogenous state and auxiliary variables
... finished.
