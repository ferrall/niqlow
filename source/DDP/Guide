/**
<H1 id="guide">Beginner's Guide</H1>

<h2>Contents</h2>
<OL class="contents">
<LI><a href="OV">Overview</a></LI>
<LI><a href="#DC">A Single Discrete Choice</a></LI>
<LI><a href="#SP">Smoothed Choice Probabilities</a></LI>
<LI><a href="#MS">More than One Unlinked Choice (Multiple Static States)</a></LI>
<LI><a href="#SC">Choices Linked over Time (Dynamic States)</a></LI>
<LI><a href="#MD">Multiple Deciders</a></LI>
<LI><a href="#Next">Where to go next</a></LI>
</OL>

<OL class="body">

<LI id="OV">Overview [in the form of a FAQ]</LI>

<OL class="faq">
<LI>What is Dynamic Programming (DP)?</LI>

<DD>DP  is a framework for choices when:</DD>
<UL>
<LI>choices are sequential over time and choices can be made contingent on what has happened in the past that lead up to the current situation.</LI>
<li>the value of choices now depends on what may happen in the future</li>
<LI>choices now affect what happens in the future, and therefore anticipating future choices is crucial for understanding choices today. </LI>
</UL>

<LI>Does Dynamic Programming Include Simpler Situations as Special Cases?</LI>
<UL>
<LI>A single choice in a single state is the most special case of DP. So static optimization can be considered a special case of DP.</LI>
<LI>When there are multiple choices but they are not linked together through time  it is also a special case.</LI>
<LI>If there multiple choices linked together through time but the agent does not care about the future then this again is a special case of DP.  It is the case of myopic decision-making.</LI>
</UL>
<LI>What is the <span class="n">DDP</span> component of <span class="n">niqlow</span></LI>

    <DD><span class="n">DDP</span> is the component of <span class="n">niqlow</span> that lets a user design and solve a discrete dynamic programming. It is written in Ox and requires Ox to run.  The user <em>imports</em> <span class="n">DDP</span> into their Ox program to use it.</DD>
<LI>Why <em>Discrete</em> Dynamic Programming (DDP)?</LI>

     <DD>DDP is a term used in <span class="n">niqlow</span> that applies to models that have discrete (as opposed to continuous) choices and/or states.  A model of decisions with discrete states (including as a very special case a single state/decisin) but continuous choices typically involves solving first order conditions or carrying non-linear optimization. <span class="n">DDP</span> does not incorporate continuous choice directly.</DD>

<LI>What about continuous choice?</LI>
     <DD>Optimization over continually varying controls (and solving non-linear systems) is the topic of the <span class="n"><a href="../FiveO/default.html">FiveO</a></span> component <span class="n">niqlow</span>. Methods that embed continuous choice inside a discrete state spaces is therefore a hybrid of the two parts of <span class="n">niqlow</span>. </DD>

<LI>What does this first part of the document do?</LI>
    <DD>It presents DDP from scratch starting with simple static choices then building up to dynamics, and shows how the key concepts of DDP are represented in <span class="n">DDP</span>.</DD>
<LI>What does part 1 of the document <em>not</em> do?</LI>

    <DD>It does <em>not</em> teach computer programming or the Ox programming language. It also does <em>not</em> present the general DDP framework nor the technical elements of <span class="n">DDP</span> in a systematic way. If you are already comfortable with Ox and with dynamic programming, then you might find this document too slow.  The <a href="#tech">second part</a> below may be a better place to start (and that includes links to examples the presume more background knowledge than what is presented here).</DD>

<LI>What about estimation?</LI>

    <DD>Within <span class="n">niqlow</span>, <em>estimation</em> refers to finding statistical estimates (guesses) of parameters of the model using data that is assumed to be generated by the model. A parameter is a quantity that enters the model and whose value is determined outside the model (an exogenous or so-called structural parameter). Parameters are typically continuous values and so guessing or estimating them is typically carried out using algorithms in <span class="n"><a href="../FiveO/default.html">FiveO</a></span>. <span class="n">DDP</span> includes methods to handle data generated by the model or created outside but assumed to be generated from the model. This document does not concern itself with DDP data and estimation, which is discussed in <a href="Data.ox.html">Data</a>.</DD>
</OL>


<LI id="DC">Discrete Choice</LI>

<p>We start with  <em>choice</em> without states and dynamics.  We start with such a simple  static choice framework that it might seem trivial. The computer code shown to implement the model will appear to be overkill.  Bear with this simple start because you will see that all the elements of dynamic programming in many areas of economics can be handled easily within the framework.</p>


<h2>The choice set</h2>

<p>Choice is simply picking from a set of options.  We will let $A$ denote the finite set of possible actions to choose among.   And we will let an arbitrary element of $A$ be denoted $\alpha$.  We will work through a couple basic choices:</p>

<DT>Example 1A. Bob wants to go to university. Four have accepted him. These four places make up his discrete choice set:</DT>
<DD>$A = $ {<code>Harvard, Yale, Queen's, McGill</code>}.  </DD>
<DD>We sometimes need a symbol for the number of options available and will use <code>J</code> for that purpose.  So for Bob, <code>J=4</code>. Of course there are other schools but Bob either did not apply or did not get in to them.  They  are <em>infeasible</em> choices. $A$ is his feasible choice set.</DD>
<DD>While $\alpha$ stands for any of the feasible actions, at some point Bob makes a choice. The school he chooses becomes special, and we distinguish the choice made from the options considered. If Bob chooses to go to Yale we would write $\alpha^\star = $ <code>Yale</code>.   In these notes things marked with $\star$ are typically related to the optimal choice. </DD>

In some cases we might use discrete choice to approximate a continuous choice, so here is another example:
<DT>Example 2A.  Tommy is choosing how much time to study for a test the next day.</DT>

    <DD>So the choice might be any number between 0 and, say, 12 hours. It would then be natural (or convenient or intuitive) to think of the choice set as $A=[0,12]$, to let $\alpha$ denote any real number in that range, and to characterize the optimal choice with a first-order condition o marginal utility, $U^{\,\prime}(\alpha).$  However, for various reasons, we stick with a discrete choice set to model Tommy's choice.  To be specific, we might think of him as studying for a number of hours: $A=$ {<code>0,1,2,&hellip;,12</code>}.   In this case $J=13$.  </DD>

    <DD><details><summary>What if Tommy were an actual person (say in a data set) and studied 2.2 hours?  </summary>
    Let's write $\hat\alpha$ = 2.2 to denote the observed choice.  This would suggest that Tommy did not choose among whole hours. </p>
    <p>This difference between actual and model behaviour might not matter to us (this choice might be part of a much bigger model).  So we might approximate by rounding to the nearest element of $A$ so that
    $\alpha^\star=$<code>2</code> would explain Tommy's choice.</p>
    <p>This is getting ahead of ourselves, but you might be wondering about accepting 2 to stand for an observed choice of 2.2.  One way in which empirical DDP work deals with this kind of discrepancy is to assume that there is <em>measurement error</em> in the data.</p>
    <p>So the difference $d =$ <code>2.2-2</code> would be attributed to the measurement error (i.e. Tommy says he worked 2.2 hours but he really worked exactly 2 hours).  This can bother empirical people who are used to working with, say, regression models.  But the error in a regression model plays a similar role in "explaining" the difference between observed and predicted values.</p>  </details></DD>

<h2>Utility</h2>

<DT>How did Bob choose Yale? And how Tommy chose 2 hours (according to our discrete approximation)?</DT>
    <DD>Perhaps they considered all the pros and cons of their options and carefully weighed the factors to determine which was best.  Or maybe they threw a dart at the wall.</DD>

<DT>Whatever process they used ...</DT>
    <DD>we are not concerned with <em>how</em> the choice was made, but that a chose was made based on a  <em>utility</em> associated with each action.
    <details class="aside"><summary>Optimality and <em>function</em></summary>Of course, much of social science concerns choices that are not optimal, but for our purposes what people do will always be optimal for them given their options and preferences. Also, a teacher of mine emphasized that utility is by definition a function, so saying "utility function" is redundant. It would be like saying "truck vehicle" instead of simply "truck," because by definition a truck is a vehicle.</details></DD>

<DT>Let $U(\alpha)$ denote Bob's utility for school $\alpha$.</DT>
    <DD>$U(\alpha)$ is just a real number associated with $\alpha$. Bob can have utility for options that are not feasible (not in $A$), such as U(<code>UCLA</code>), but we don't have a complete model until utility is defined for each element of $A$.  So, we can get fancy and write $U: A\,\to\, \Re$.  But since $A$ is a discrete set, $U$ is really just four numbers, one for each feasible school.</DD>

<DT>For example, Bob would set $\alpha^\star=$ <code>Yale</code> if his utility were:</DT>
<DD><pre>
&alpha;               U(&alpha;)
----------------------
Harvard        -20
Yale            4.2
Queen's        e<sup>1</sup>
McGill          ln(0.0009)
</pre>
Many other utility levels would explain <code>Yale</code> as an optimal choice.</DD>

<h2>Multiple Dimensions</h2>

<DT>In many cases we want to model choices in more than one dimension.</DT>
  <DD>Bob was making a choice in the <em>school</em> dimension.  But he might also be making decisions in other dimensions, such as <em>major</em>, <em>roommate</em>, etc. Before showing <span class="n">DDP</span> code for Bob's choice let's have him decide his major at the same time. One way to do this is to convert a choice in several (discrete) dimensions into a one dimensional choice by simply listing all the possible combinations. Another way is to make $\alpha$ a vector not a scalar. We illustrate both ways in the next example.</DD>

<DT>Example 1B: Bob is choosing both a university <em>and</em> a major. Which major is best might depend on which school he chooses.  </DT>
    <DD>For simplicity, suppose Bob's parents have told him he has to choose either <code>Economics</code> or <code>Physics</code>.  </DD>
    <DD>We could simply expand his choice set as follows: <br/> <code>{Harvard-Econ,Harvard-Physics,Yale-Econ,Yale-Physics,Queen's-Econ,Queen's-Econ,McGill-Econ,McGill-Physics}</code>.
    <br/>
    Utility is then a number assigned to each of these <code>J=8</code> options.  This collapses a choice in two dimensions into a longer one-dimensional choice.</DD>
    <DD>However, it can be more convenient not to collapse the two dimensions, but to consider them separate but simultaneous.  In this case, we can let school be the row and major the column:
<pre>
BOB'S UTILITY ON THE FEASIBLE MAJOR-SCHOOL CHOICE SET
                    Econ                    Physics
Harvard           -20                       -18
Yale              4.2                      -0.6
Queen's           1.5                        3.2
McGill           -25                        -0.5
</pre></DD>
<DD>Apparently Yale has a good Econ program, but if Bob had not gotten into Yale he would have chosen Queen's and majored in Physics.</DD>

<DT>We can also keep the dimensions separate by letting $\alpha$ be a <em>vector</em> of action variables: $\alpha = (a_0, a_1, \dots, a_{D-1})$.</DT>

<DD>In the major-school choice, <code>D=2</code>, because there are two dimensions of choice,  $a_0$ is the major choice, and $a_1$ the major choice.  <details class="aside"><summary>Start at 0?</summary>We start counting at 0 because that is the way counting is done in many computer languages.  So doing so now may avoid some confusion later when we see code.</details>
Although using rows and columns to represent two different action variables is clear, it is not very helpful when there are three or more variables.</DD>

<DT>We can combine the list version with the vector version to get something like this:</DT>
<DD><pre>
BOB'S UTILITY USING ACTION VARIABLES
     &alpha;
a<sub>0</sub>            a<sub>1</sub>                U(&alpha;)
--------------------------------------
Econ          Harvard          -20
Physics       Harvard          -18
Econ          Yale              4.2
Physics       Yale             -0.6
Econ          Queen's           1.5
Physics       Queen's           3.2
Econ          McGill           -25
Physics       McGill           -0.5
</pre></DD>

<DT>In  <span class="n">DDP</span>:  </DT>
<DD>the action vector is built by adding `ActionVariable`s to it as part of the set-up of the model.  The creation of the list of actions as shown above is done for you by <span class="n">DDP</span> as you add action variables to the action vector. </DD>
<DD>As the user you only have to concern yourself with which variables are chosen and how many different values they take on. You can provide value labels for each action variable you add to the model.  In the example above "Econ" would be a label for an integer option value.</DD>


<h2>Full Description of Bob's Problem</h2>

As with the first example, each row is an action $\alpha$, but it is associated with values of two action variables. Labelling the choices certainly helps to understand them, but a computer program that is designed to handle any kinds of choices can hard-code labels like "Harvard" and use it to refer to utility.  Hopefully it is clear that in the abstract the choice over the four universities is just a case of choosing among 4 possibilities.  The labels are helpful to us, but generically the choices can just be numbered 0, 1, 2, and 3.</DD>

<DT>The school-major choice can be describe with action variables that are just integers along with labels for each value.</DT>
<dd><pre>
&alpha; =  (a<sub>0</sub> a<sub>1</sub>)

Index     Label        Options     Choice Set     Value Labels
-------------------------------------------------------------------------
0          Major             2         0 &hellip; 1       Econ, Physics
1          School           4         0 &hellip; 3       Harvard,Yale,Queen's,McGill

A = [0&hellip;1] &times; [0&hellip;3]
a<sub>0</sub>     a<sub>1</sub>          U(&alpha;)
-------------------------
0       0        -20
1       0        -18
0       1         4.2
1       1        -0.6
0       2         1.5
1       2         3.2
0       3        -25
1       3        -0.5
</pre></DD>

<DT>Bob's Choice</DT>

<DD>So far we just have 8 options with arbitrary values (utilities) assigned to them.  And it turns out that the maximum of those utilities is 4.2 for the action of choosing <code>Yale</code> and <code>Econ</code>.
$$\eqalign{
\alpha^\star &\equiv \arg\max_{A}U(\alpha) = (0,1) = (\hbox{Econ},\hbox{Yale})\cr
EV &\equiv \max_{A} U(\alpha) = U(\alpha^\star) = 4.2.\cr}$$
</DD>

<DT>V and EV</DT>

<DD>In microeconomics the highest utility possible is named <em>indirect utility</em>, but in dynamic programming it is usually called the (optimal) <em>value</em> of a state. So, above, $V$ is used to represent the value Bob receives from his choice once optimized.  </DD>

<DD>We use $EV$ and not just $V$ in anticipation that there can be elements of uncertainty in the choice.   In particular, the person deciding will know everything up until today when the choice is made, but they will have to make that choice without knowing everything that will happen in the future (tomorrow).  $EV$ will end up being good notation since when, deciding today, they will have to average (take the <b>E</b>expectation of) their optimal choices made tomorrow when they have more information than they have now.
<details class="aside"><summary>Ties</summary><DD>The notation also assumes there are no ties, the optimal choice is unique.  That won't be necessary.  Ties are handled properly by <span class="n">DDP</span>,  but assuming no  ties does make the example simpler.</DD></details></DD>

<h2>Coding Bob's Choice in <span class="n">DDP</span></h2>

<DT>Bob's choice is simple.</DT>
  <DD>Simply look at the 8 numbers that make up his utility.  </DD>
  <DD>Find the biggest number.  </DD>
  <DD>Look up the labels associated with that action. Finished.  </DD>

    <DD>These operations could easily be done by hand, in a spreadsheet, or even in Ox using its built in <a href=""><code>maxc()</code> and <code>maxcindex()</code></a> routines. When you look at the code for Bob's Choice in <span class="n">DDP</span> you will see that it has some elements that are not obvious.  Indeed, it relies on some sophisticated features of the Ox programming language.  Even if you have done some programming in similar languages such as Matlab, Python or R, your reaction may be: that is a lot of complexity for such a simple choice.  And you are right!</DD>

    <DD>However, hopefully you see some logic in the complications so that the code is not completely unrelated to the problem as you understand it.  And you might see that some of the complication is there to make real models easier to build than if simple tools only were used.</DD>

<details><summary>Click to see the code.</summary>
    <DT>Source: <a href="javascript:popoxcode('../../examples/DDP/BobsChoice.ox');">niqlow/examples/DDP/BobsChoice.ox</a></DT>
    <DD><pre><object width="75%" height="300" type="text/Plain" data="../../examples/DDP/BobsChoice.ox" border="1" ><p style="font-size:14pt"></object>
</pre></dd></details>

    <DT>Line-by-line explanation of the code</DT>
    <DT> <code>&#35;import &hellip;</code>:</DT>
    <DD>Import is a way to tell Ox that you are using code that is not part of this file. In this, case the program is importing <span class="n">DDP</span>!
    <details class="aside"><summary>&#35;import and &#35;include</summary> If you want to know more, see <a href="http://www.doornik.com/ox/oxtutlan.html#ox_tutlan_link">Multiple files in Ox</a>. Most examples shown in Ox start with <code>#include "oxstd.h"</code>.  That is done in <span class="n">DDP</span> so it is not necessary to do it explicitly.</details></DD>

    <DT><code>class &hellip; { &hellip; }</code>: Class Declaration</DT>
    <DD>A user's model is represented by a <code>class</code>, which is way to combine data and functions that work on the data in one package.  This is called <em>object oriented programming</em>. See <a href="http://www.doornik.com/ox/oxtutlan.html#ox_tutlan_oo">OOP in Ox</a> if you are familiar with objects already and want to know how they work in Ox.</DD>

    <details class="aside"><summary>Declare and  Define</summary>In a programming language like Ox your program will have different things in it.  The language has to know what things are in the program, and this is the idea of <em>declaring</em> something.  This is like listing a chapter of a book in a table of contents.  But Ox also has to know what the things are, which is like the contents of the chapter.  This is <em>defining</em>. How things are declared and defined in Ox depends on the kind of thing it is.  And, in some cases your program might declare and define something at the same time not separately.</details>
    <DD>If you are not used to OOP, it can be mysterious and confusing.  And Ox can be used without OOP, but it is the way in which <span class="n">DDP</span> lets the user develop an economic model so it is essential for our purpose.  So here is the basic idea.  What is happening is the program is telling Ox that a new class is going to appear in this program.  A class is a description of a kind of thing (objects).  These lines are describing the class for Ox, which is just a list of the variables (also called <em>members</em> and functions (also called <em>methods</em>) that make up the class.  Every class has a name, and the code gives this class the name <code>BobsChoice</code>.  </DD>

    <DD>One of the powerful (but complicated and confusing) features of OOP is that one class can be based on another class that is already defined.  In this case, <code>BobsChoice</code> is based on a class called `OneStateModel` which is defined in <span class="n">DDP</span>.  The name is meant to imply that this model is really simple.  In real dynamic programs in which there are many states at which the person is making choices. Here Bob makes a choice once, so there is just one state. </DD>

    <DD>In turn, <code>OneStateModel</code> is a class based on other classes which are more flexible. The advantage of having a special class for a simple one state model is that some things can be done for the user.</DD>

    <DD>The choice involves two action variables, and this class makes room for them with the <code>static decl</code> statement.  The names are short but understandable in the context.  <code>decl</code> is short for <em>declare</em> and does just make room for two things.  What they end up holding is determined by other parts of the program.</DD>

    <DD>The <code>static</code> tag is important but at this point it is not necessary to understand why it is there.  It does not have anything to do with the fact that Bob's choice is a static choice.</DD>

    <DD>The <code>BobsChoice</code> class also has two functions in it: <code>Decide()</code> and <code>Utility()</code>.  Note that <code>Decide()</code> is declared <code>static</code> but <code>Utility()</code> is not.  Again, this is not important to understand yet.  And like <code>decl</code>, listing functions in the declaration of  a class does not say anything about what they do.  The have to be <em>defined</em> later.</DD>

    <DT><code>main(){&hellip;}</code></DT>
    <DD>Every Ox program has to have a function (or routine) called <code>main()</code>. This is where the Ox program actually starts.  Even though the class declaration comes first in the file, it is <code>main()</code> that is the first thing to happen. I have written the code so that <code>main()</code> just does one thing: it asks that <code>Decide()</code> be executed. Once it is finished <code>main()</code> is finished (no other statements  appear inside <code>main()</code>).  Most experienced programmers make their main routines pretty simple. </DD>

    <DT><code>BobsChoice::Decide(){&hellip;}</code></DT>

    <DD>Above the class declaration said that a routine belonging to <code>BobsChoice</code> and named <code>Decide()</code> would appear, and these lines <em>define</em> what this routine does.  It is the lines of code that are executed when <code>main()</code> refers to the function.  </DD>

    <DD><code>Decide()</code> does four things.  That is, it has four statements each ending with <code>;</code>.  The first two say that <code>maj</code> and <code>sch</code> will each contain an <em>action variable</em>.  This routine, <code>ActionVariables()</code>, is part of <span class="n">DDP</span>, so it would not work to call it if we had not imported <span class="n">DDP</span>.</DD>

    <DD>Hopefully, you can see that labels attached to the two variables are "major" and "school", respectively. Like nearly all computer languages, Ox asks you to put text inside quotes.  So <code>maj</code>, which is not in quotes is referring to a variable with that name. Ox will make room for because it was <code>decl</code>ared in the class declaration. However, the action variable has a name "major" which is just those characters not a variable or a routine.  </DD>

    <DD>In Bob's choice he had only two majors to choose from.  We could write <code>ActionVariable("major",2)</code> and this would mean <code>maj</code> would hold an action variable with two possible values.  This would not given meaningful names (or labels) to the two options: they are just be coded as <code>0</code> and <code>1</code>.  By sending two strings in quotes instead of <code>2</code> this gives each major a meaningful label.  The routine <code>ActionVariable()</code> counts the labels and knows that there are two choices.  They are still coded as <code>0</code> and <code>1</code> but those codes now have the labels "Econ" and "Physics". This will make some output easier to read.</DD>

    <DD>The variable <code>sch</code> will contain another action variable that takes on four values (because a list of four labels are sent).</DD>

    <DD>The next statement calls a routine with the name <code>Initialize()</code>.  This routine is part of <span class="n">DDP</span> not Ox itself.  As the name implies, it initializes or sets up the problem meant to solve Bob's Choice.  Four things are sent to <code>Initialize()</code>.  The order is important.  The first is a bit mysterious: <code>new BobsChoice()</code>. The second is the number <code>0</code>.  As with some other parts of the code, it is not <em>yet</em> important to understand how these are chosen or used for this problem. In particular, to explain what <code>new BobsChoice()</code> means and why it is there would take us on a tangent that is not necessary for now. </DD>

    <DD>However, it is important to see that the variables <code>maj</code> and <code>sch</code> are sent as well.  This is how the <code>OneStateModel</code> in <span class="n">DDP</span> knows what action variables are part of the model.</DD>

    <DD>You may think that the line that says <code>maj</code> contains an <code>ActionVariable</code> would add <code>maj</code> to the model automatically.  It would be possible to make it work that way, but for other aspects of DP models (namely state variables) it is easier to separate the creation of a variable from including it in the model. So to be consistent, the same procedure is followed for action variables.</DD>

    <DD>The last thing <code>Decide()</code> does is call a routine `VISolve`(), where "VI" stands for "value iteration."   That is a technique for solving a dynamic programming problem. This one state model is the most simple example of a DP problem, but one part of value iteration is to solve for the optimal choice at each state.  This is where <span class="n">DDP</span> will actually solve Bob's problem.</DD>

    <DT><code>BobsChoice::Utility(){ &hellip;}</code></DT>
    <DD>Any DDP problem has a utility, and here is Bob's.  In general, the utility has to return (send back to where the utility was called) a vector in the same order as the actions $\alpha$.  The use of <code>&lt; -20; &hellip; &gt;</code> is the way Ox lets you hard-code a vector or matrix of numbers.   So the list of utilities above has been copied and written in that format.  We should definitely check that the action variables are organized so that the numbers match up: we wouldn't want Bob to mistakenly go to McGill and major in Physics.</DD>
    <DD>Usually <code>Utility()</code> is a function of actions (and later states) and <em>parameters</em> of the problem.  So Bob's utility is not at all typical.  Later examples in the documentation and exercises will have utilities that are more like what show up in real DP problems.</DD>

<h2>Run the program and look at the output.</h2>

Getting Ox to run <code>BobsChoice.ox</code> and be able to use <span class="n">DDP</span> is not hard  if you are used to doing this kind of thing.  Otherwise, it can be a pain for the first few times you try something.  Go <a href="">here ???</a> for some help.</p>

<details><summary><b>The output you get should look something like this:</b></summary>
<DT>Source: <a href="javascript:popoxcode('../../examples/output/BobsChoice.output.txt');">examples/output/BobsChoice.output.txt</a>.</DT>
<DD><pre><object width="75%" height="300" type="text/plain" data="../../examples/output/BobsChoice.output.txt" border="1" ><p style="font-size:14pt"></object>
</pre></dd></details>

<DT>Selected Output</DT>
That output contains a lot of information that is not relevant to this simple case, so here are selected parts of the output:
<details><summary>Selected Output</summary><dd><pre>
-------------------- DP Model Summary ------------------------
4. ACTION VARIABLES
   Number of Distinct action vectors: 8
         major  schoo
    a.N      2      4

6. FEASIBLE ACTION SETS

    alpha       A[0]
    ----------------------
    (00)          X        -Econ-Harvard
    (10)          X        -Physics-Harvard
    (01)          X        -Econ-Yale
    (11)          X        -Physics-Yale
    (02)          X        -Econ-Queen's
    (12)          X        -Physics-Queen's
    (03)          X        -Econ-McGill
    (13)          X        -Physics-McGill
   &#35;States        1
    ----------------------
    Key: X = row vector is feasible. - = infeasible


     Value of States and Choice Probabilities
     ------------------------------------------------------------------------------
    Indx   I   T   A   q   t     r     f       EV      |Choice Probabilities:
       0   1   0   0   0   0     0     0       4.200000 0.000000 0.000000 1.000000 0.000000 0.000000 0.000000 0.000000 0.000000
     ------------------------------------------------------------------------------
</pre></dd></details>

<DT>Explanation of Relevant Output</DT>
    <DD>The "DP Model Summary" is produced by <code>Initialize()</code> after everything is set up.  (In models that have more than one state <code>Initialize()</code> only does some of the things done here.  The rest are done by a routine that it calls but typically is called from the user's program.) Part 4 of the summary shows you the action vector $\alpha$.  It shows that the new action variables assigned to <code>maj</code> and <code>sch</code> were added to the vector and that together they create 8 different options.</DD>

    <dd>Part 6 shows you the feasible action set $A$.  It shows you each action $\alpha$ as two integer values and the labels that go along with them.  Recall that <code>Yale-Econ</code> was the third option in the list and it appears that $A$ was set up so that it matches up with the vector in <code>Utility()</code> (but make sure!). </dd>
    <DD>In real DP problems the actions that are feasible can depend on which state the system is at.  So this output is set up to show you the different values of $A$ in the model.  But with one state only there is one feasible set only.  Thus $A$ is called <code>A[0]</code>, because in some cases there will be <code>A[1]</code> and so forth.</DD>
    <DD><code>VISolve()</code> produces the table of values and choice probabilities.  The first 8 columns really don't apply to this simple model because there is only one state.  The table is designed to show the value of each state in the model.</DD>
    <DD>The parts that matter are the <code>EV</code> and <code>Choice Probabilities</code>.  Recall the <code>EV</code> is the DP version of indirect utility, and we know that the best Bob can do is 4.2.  And the optimal choice is the third element of $A$, which Bob should chose with probability 1.0.  All the other options are sub-optimal and are chosen with 0 probability.</DD>

<h2>Summary</h2>

Here is what is important to gather from the example and code above.

<OL class="steps">
    <LI>At the heart of a <span class="n">DDP</span> model is discrete choice.  Discrete dynamic programming links together many different discrete choices that are connected by state variables that evolve base on the choices made.</LI>
    <LI>In <span class="n">DDP</span> you build your model by defining a <em>class</em> that is derived from one of the built-in `Bellman` classes.  The simplest model to build from is `OneStateModel`, which is a single discrete choice (one state, no dynamics, etc).</LI>
    <LI>The discrete choice $\alpha$ is a vector of action variables, each taking on a finite number of values.  Your model builds $\alpha$ by creating new `ActionVariable`s and adding them.  With a `OneStateModel` you send all the action variables to `OneStateModel::Initialize`().  In general your code will add action variables yourself after calling <code>Initialize()</code>.</LI>
    <LI>Your model must supply a <code>Utility()</code> which returns a vector of numbers corresponding to the elements of the feasible set $A$.</LI>
    <LI>The optimal choice for the model can be found by calling `VISolve`(), which does several things. For a one state model it simply finds $\alpha^\star$, the maximizing action.  </LI>
</OL>


<LI id="SP">Choice Probabilities</LI>

<p>The examples above are not particularly interesting models of behaviour.  For one thing, the utility values are arbitrary, whereas a good model would relate utility to observable characteristics of the chooser and the choices.  And, second, suppose Bob did not choose Yale-Econ?  Then our model is incorrect.  This is not surprising because we typically are not modeling a single person's choice and we will never have access to the utility of all the options.  Instead, real discrete models provide a probability of people making choices not a 0/1 outcome.</p>

<DT>The <em>logit model</em> of choice includes a continuous random variable in the <em>value</em> of a choice:</DT>
$$v(\alpha) = U(\alpha) + z_\alpha.$$
<DD>The extra component $z_\alpha$ is assumed to follow the <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Type I Extreme Value (Gumbel) distribution</a>.  Across options $z_\alpha$ values are independent.  The term $U(\alpha)$ is a shortened version of name of the routine that is part the model in <span class="disp">DDP</span>.   The user's coding would be the same <em>and the term $z_\alpha$ is not explicitly added to it.</em></DD>

<DT>The <em>probit model</em> of choice makes a different distributional assumption</DT>
    $$v(\alpha) = U(\alpha) + z_\alpha, \qquad z_\alpha \sim N(0,1).$$

<DD>Across options $z_\alpha$ values are independent.  </DD>

<DT>Conceptually ...</DT>
 <DD>Bob sees both <code>Utility()</code> (the part we see or assume or estimate) and the vector of values of $z_\alpha$ (the part we don't see but we assume follows a convenient distribution). With his information, Bob still simply chooses the $\alpha$ that maximizes $v(\alpha)$.  But since we don't see $z_\alpha$, any of the options might be optimal to Bob.  We get different <em>probabilities</em> of choices being optimal.   The probabilities depend on the observed vector of utilities, to the 0/1 vector of ouptut above would become a  vector of numbers between 0 and 1.  Since <code>Yale-Econ</code> has the highest value of <code>Utility()</code> it will have the greatest probability of being chosen, but the probability will not be 1.0.</DD>

<DT>Choice Probabilities under logit</DT>
<DD>Logit:
$$P(\alpha) = {e^{U(\alpha)}  \over \sum_{\alpha'\in A} e^{U(\alpha')}} = {e^{U(\alpha)-U(\alpha^\star)}  \over \sum_{\alpha'\in A} e^{U(\alpha')-U(\alpha^\star)}}$$
The second version expresses the probability in relative utility loss compared to the optimal choice $\alpha^\star$.</DD>

<DD>Following the literature, <span class="n">DDP</span> generalizes the logit expression to allow the amount of smoothing to be chosen by setting a parameter $\rho$:
$$P(\alpha)  = {e^{\rho\left(U(\alpha)-U(\alpha^\star)\right)}  \over \sum_{\alpha'\in A} e^{\rho\left(U(\alpha')-U(\alpha^\star)\right)}}$$
As $\rho\to \infty$ the probability approach the non-smooth values.  As $\rho\to 0$ the smoothing becomes complete and each feasible choice is equally likely.</DD>

<details><summary>See Train (2001)</summary>
<a href="">Train (2001)</a> explains all aspects of discrete choice with an emphasis on econometric applications.   Readers are encouraged to refer to Train () for in-depth discussion of the static case.  We start even simpler than Train does.  Eventually the set up starts to look like the material in Train, but we move to dynamic choice before going into the depths of various static choice models that Train does.</details>

<DT>Choice Probabilities under Probit</DT>
<DD>Independent Probit:
$$\eqalign{ P(\alpha) &= Prob( U(\alpha) \ge U(\alpha') ), \forall \alpha'\in A.\cr
&= \prod_{\alpha'\ne\alpha}\ \Phi\left( \rho(U(\alpha)-U(\alpha'))\right)\cr}$$
Again, <span class="n">DDP</span> generalizes the standard probit by varying how important the deterministic component to utility is to choice. The smoothing parameter $\rho = 1/\sigma$ and $\sigma$ is the standard deviation of the difference $z_\alpha-z_{\alpha'}$.  This smoothing would not be identified if/when $U(\alpha)$ is based on estimated coefficients on variables related to $\alpha$. </DD>

<DD>Correlated Probit:</DD>


<h2>Smoothing Choice Probabilities in <span class="n">DDP</span></h2>

<DT>Select a Smoothing Method</DT>
<DD>Recall that Bob's Choice was derived from the `OneStateModel` class of problems. That class is in turn a special case (a derived class) of the `ExPostSmoothing` class. The method for smoothing for the one state model is made at this point in the code:
<pre>Initialize(new BobsChoice(),0,maj,sch)
</pre></DD>

<DD>As you can see from the documentation of `OneStateModel::Initialize`(), the second argument is the smoothing method choice. To avoid having to explain everything at once, it was set to 0 in the code, which is the default choice in the general <code>ExPostSmoothing</code> class.</DD>

<DD>Another more descriptive way to send 0 is to use a name for zero that is defined in <span class="n">DDP</span> in order to make the internal code easier to follow and debug:
<pre>Initialize(new BobsChoice(),NoSmoothing,maj,sch)
</pre>
Here <code>NoSmoothing</code> is really just a name for the number 0.  It is one of the `SmoothingMethods` that are really just names for the integers 0, 1, and 2. The code for the ExPostSmoothing class of problems will look at the method sent to it and decide if and how to smooth the resulting choice probabilities.</DD>

<DT>Logit Smoothing</DT>
<DD>To make Bob's Choice a logit model, just modify the code to ask for logit smoothing:
<pre>Initialize(new BobsChoice(),LogitKernel,maj,sch)
</pre></DD>
<details><summary>What does "kernel" mean?</summary>  Essentially, a kernel means "smoothing."  You can learn more at <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation">Kernel's wikipedia page</a></details>

<DT>Probit Smoothing</DT>
<DD>To make Bob's Choice a probit model, just modify the code to ask for probit smoothing:
<pre>Initialize(new BobsChoice(),GaussKernel,maj,sch)
</pre></DD>
<details><summary>Why Gauss?</summary>Because the normal distribution is also referred to as the Gaussian distribution.</details>

<details><summary>Why Ex Post?</summary>
<DD>In the standard discrete choice econometric model the error term $z$ is considered something known/observed by the chooser but not us.  In a more complicated situation, namely a dynamic one, Bob might make other choices before he knows the value of $z$.  For example, $z$ may be observed when Bob wakes up in the morning but he had to make a choice the day before that was affected by what he does today.  In that case, the value of $z$ is not just smoothing the choice probability for our sake, it is also have a direct effect on other decisions.  </DD>
<DD><span class="n">DDP</span> distinguishes between choice probability smoothing for our own sake and real uncertainty that affects the chooser in a dynamic environment.</DD>
<DD>The case of <em> ex-post smoothing</em> is when $z$ is not a real part of Bob's environment.  He is really just making a deterministic choice based on $U(\alpha)$.  But after his choice we smooth the probability as if he also add a value of $z$.  </DD>
<DD>The case of <em>ex-ante</em> smoothing allows $z$ to play a role in other decisions made by Bob so it is a real "structural" error term.  </DD>
</details>

<h2>Exercises</h2>
<OL class="steps">
<LI>Make a copy <code>BobsChoice.ox</code>.  Try the 3 different smoothing options discussed above.  Compare results.</LI>
<LI>Multiply all the utilities by 5.  Confirm that this does not change the optional choice when there is no smoothing.  What happens to choice probabilities when they are smoothed compared to the values in the previous exercise?</LI>
</OL>

<LI id="MS">Multiple States (Unlinked Choices)</LI>

<p>Bob and Tommy's choices are one-time discrete choices.  By deriving a model of their choices from the <code>OneStateModel</code> class there was no possibility of dynamics or examining how the situation affects the choice.</p>

<p>A <em>state</em> is a situation in which the agent makes a decision.  States differ from each other for various reasons. Utility may differ between states (leading to different decisions).  The feasible action set may differ between states. The effect of choices on what happens in the future may differ. Obviously the next step from a one-state model is a two-state model.  We then double the states again to consider a four-state static choice model.</p>

<h2>The State Space</h2>

<p>In dynamic programming the choice of action $\alpha$ is contingent on the state the chooser is in.  To store and solve models efficiently, <span class="n">DDP</span> will distinguish between types of states.  But a generic state will be denoted $\theta$.  And the set of states the problem might be in is denoted $\Theta$. As with feasible actions, the state space is constructed by adding state variables to it.   We illustrate this with simple extension of Bob's Choice.</p>

<DT>Example 1C.  Bob is still deciding between schools and majors.  However, it is more complicated. He may or may not receive a scholarship to Queen's for which he is applied for.</DT>

    <DD>Obviously Bob will make his decision based on whether he gets the scholarship or not, so the choice is <em>contingent</em> upon that.  We want the model to capture both choices.  One reason to do this is that we want to apply the model to data in which some people get scholarships and some don't.  The values of utility above will be the values without a scholarship.  Then in the event of getting the scholarship the utility of choosing Queen's goes up.</DD>

    <DD>Scholarships are measured in dollars, but Bob's utility from getting and accepting a scholarship may be different that the monetary value.  However, to keep things simple we will assume that the utilities already listed are in money-equivalent values so we can add the value of the scholarship to them in order to compute net utility for Bob.  Specifically, the Queen's scholarship has a value of &#36;2,200 and utility is measured in thousands of dollars. So in the case of getting the scholarship and going to Queen's utility increases by 2.2.</DD>

<h2>Coding Bob's New Choice</h2>

<DT>State Variables</DT>

    <DD>The `StateVariable` class is designed to represents state variables in a dynamic program.  Like action variables, the user's code creates state variable, stores them in a <code>static</code> member of the model class, and then adds them to the model. <span class="n">DDP</span> then incorporates them into the state space and the solution methods. </DD>

    <DD>Unlike action variables, which are mainly different only in the number of values they can take on, state variables differ from each in a lot of ways. So <span class="n">DDP</span> provides a number of different kinds of state variables to add to a model as well as the possibility of defining your own type if it does not exist already. However, in this simple static situation, the base <code>StateVariable</code> class will suffice.  So Bob's award status will be of that class.</DD>

    <DD>The simple `OneStateModel` class is no longer appropriate because Bob either gets the award or not.  So this model of Bob's choice is derived from a different class of problems.  Again, since it is a very simple situation, the base `Bellman` class will do the trick.  </DD>

<DT>Static Time</DT>

    <DD>The Bellman class does not restrict the model to be static, so unlike <code>OneStateModel</code> the code will have to specify this feature of the environment. But this will take one simple line of code.  Below the role of time is introduced in earnest.</DD>

<DT>Utility</DT>
    <DD>Finally, the new model of Bob's choice will have to modify <code>Utility()</code> to account for scholarships.  As with the first example, this may seem much more complicated than necessary, and it is for this simple static two-state environment.  But the approach to doing this will handle any kind of utility with limited programming complexity.</DD>


<details><summary><b>The Code</b></summary>
<DT>Source: <a href="javascript:popoxcode('../../examples/DDP/BobsChoiceB.ox');">examples/DDP/BobsChoiceB.ox</a></DT>
<DD><pre><object width="75%" height="300" type="text/plain" data="../../examples/DDP/BobsChoiceB.ox" border="1">
<p style="font-size:14pt"></p></object>
</pre></DD></details>

<DT>Line-by-line explanation of the (new and modified) code</DT>

<DT><code>class BobsChoiceB : Bellman {</code>
    <DD>We could continue to use the same name but for clarity this  model will be called <code>BobsChoiceB</code> The base class is no longer <code>OneStateModel</code> but <code>Bellman</code>.</DD>

<DT><code>static const decl Uv =  &lt; -20; -18;  4.2; -0.6;  1.5;  3.2; -25; -0.5&gt;;  //Added</code>
    <DD>The vector of utilities has been moved from the utility to a constant member of the class.  Now the value of the scholarship can be  added to <code>Uv</code> in order to compute overall utility.</DD>

<DT><code>static decl Qsch, </code></DT>
<DD>A variable to hold the scholarship state has been added to the model.</DD>

<DT>Changes to <code>Decide</code></DT>
    <DD>We still need to <code>Initialize</code> the model, but the Bellman version of <code>Initialize</code> is different than the very special OneStateModel version.  It does not allow you to list actions.  <code> SetClock(StaticProgram)</code> says that this is a static (but possibly multi-state) model.</DD>

    <DD><code>Actions(maj,sch)</code>:  Except in specialized models like OneStateModel, action variables are added to the model by sending them to `DP::Actions`().  </DD>
    <DD><code>Qsch = new StateVariable("Qsch",2)</code>:  this creates a basic state variable that takes on two values.</DD>
    <DD><code>EndogenousStates(Qsch);</code>: As mentioned earlier, different kinds of states are tracked in <span class="n">DDP</span> and here we have added <code>Qsch</code> to the <em>endogenous</em> state space.
    <DD><code>CreateSpaces()</code>: In the first example this routine is called inside `OneStateModel::Initialize`().  But in all other kinds of models, the user calls this method themselves once they have added all the elements to model.  Each kind of Bellman model has its own version of `DP::CreateSpaces`(), and the one that is called is the one that the model is derived from (here <code>Bellman</code>'s own base version).  Sometimes arguments can or have to be sent to <code>CreateSpaces</code>, but often the default values apply and nothing is sent.</DD>

<DT>Utility</DT>
    <DD>Utility now returns a different vector depending on whether <code>Qsch</code> equals 0 or 1. <code>Qsch</code> is the state variable and takes on 2 values, 0 and 1.  We set utility up so that a value of 1 indicates the Queen's scholarship was received and its value should be added to the utility of Queen's programs when choosing where to go.  The underlying code will call <code>Utility</code> for all values of all states added to the model.  </DD>

    <DD>The value of <code>Qsch</code> when called can be accessed a couple different ways.  One way is directly:  <code>Qsch.v</code> will hold either 0 or 1 when Utility is called.  Or the state variable can be sent to `CV`() which will access the value return it.  <code>CV()</code> is a flexible way of incorporating state variables, parameters and even constants in your model in such way that it looks the same regardless of the kind of thing it is.</DD>

    <DD>The trickier thing to understand at this point is accessing values of the school choice, which is an action variable not a state variable.  Looking at the list of labels associated with <code>sch</code>, Queen's is the 3rd value, or <code>2</code> since we start actions at 0.  Ox's <code>.==</code> operator will compare a vector to a value and return a vector of 0s and 1s for whether corresponding element equals the value.</DD>

    <DD>If this is unclear, you can add a <code>print()</code> statement above <code>return</code> to see what <code>CV(Qsch)*(CV(sch).==2)</code> contributes to utility.</DD>

<h3>Run the program and look at the output.</h3>

<details><summary><b>The output you get should look something like this:</b></summary>
<DT>Source: <a href="javascript:popoxcode('../../examples/output/BobsChoiceB.output.txt');">../../examples/output/BobsChoiceB.output.txt</a>.</DT>
<dd><pre>
<object width="75%" height="300" type="text/plain" data="../../examples/output/BobsChoiceB.output.txt" border="1" ></object>
</pre></dd></details>


<h2>Restricted Feasible Actions</h2>

<DT>Now, suppose we also want the model to consider what happens if Yale does or does not accept Bob. </DT>

    <DD>We handle this by creating a model with two states, one in which Yale is in the feasible set of schools and another at which it is not.</DD>

<DT>State-Dependence in $A$ </DT>

    <DD>An important feature of many dynamic programming models: the feasible set depends on the state the action is being taken at. <span class="n">DDP</span> provides a method for creating different feasible sets. Given a state $\theta$, we can specify that the feasible action set is $A(\theta)$.  If we let plain old $A$ denote the unrestricted set (as above), then $A(\theta) \subseteq A$. Recall that each action vector $\alpha$ is represented in the code as a row in a matrix.  Now different states will have different matrices.</DD>

<h2>Coding Bob's New Choice in <span class="n">DDP</span></h2>

<DT><code>FeasibleActions()</code></DT>
    <DD>To restrict feasible actions, the user provides a <em>method</em> that returns a column vector of 0s and 1s.  This column vector depends on the current state of the model. A 1 indicates the action (row) is feasible at the state, 0 means infeasible.  The user's method is called by <span class="n">DDP</span> inside `DP::CreateSpaces`() for each possible state.   </DD>

<details><summary><b>The Code</b></summary>
<DT>Source: <a href="javascript:popoxcode('../../examples/DDP/BobsChoiceC.ox');">niqlow/examples/DDP/BobsChoiceC.ox</a></DT>
<DD><pre><object width="75%" height="300" type="text/plain" data="../../examples/DDP/BobsChoiceC.ox" border="1" ></object>
</pre></DD></details>

<DT>Line-by-line explanation of the (new and modified) code</DT>
<DD><code>Yacc</code> is added to the class so that it can track the case of Yale accepting Bob or not.</DD>
<DD>The class declares that there will be a new method named <code>FeasibleActions()</code>.  The name is important because this new method will replace one that already exists.  If the name is not correct then this replacement will not happen.</DD>
<DD>The new binary state variable is created and added to the model just like <code>Qsch</code>.</DD>
<DD>The actions are still which school to go to and which major to study.  But if <code>Yacc=0</code> Yale is not feasible and any row of $A$ with that school should be removed at that state.  Yale is school 1.  If <code>Yacc=1</code> all the options are feasible. If <code>Yacc=0</code> then only cases when sch is <em>1</em> are feasible.  The logical expression for feasibility is "Yacc <em>or</em> sch=1".  However, we have to use <code>CV()</code> to get the value of variables.  And Ox uses double equal signs to test equality. It uses double vertical lines to denote "or."  So the return value <code>(CV(Yacc)==1)  .||  (CV(sch).!=1);</code> is a translation of the logic.  The table below shows the two different return values. </DD>

<DD><pre>
VECTOR OF 0s AND 1s RETURNED BY FEASIBLEACTIONS

     &alpha;               Yacc=0        Yacc=1
------------------------------------------------
Econ          Harvard        1              1
Physics       Harvard        1              1
Econ          Yale           0              1
Physics       Yale           0              1
Econ          Queen's        1              1
Physics       Queen's        1              1
Econ          McGill         1              1
Physics       McGill         1              1
</pre></DD>

<DD>Finally, utility must return values that correspond to the feasible action set.  The hard-coded <code>uv</code> vector includes rows for Yale.  It cannot be used without removing those rows.  The function <code>OnlyFeasible()</code> can be used to do this without repeating the logical expression in <code>FeasibleAction</code> to find and delete the Yale rows.</DD>


<h2>Run the program and look at the output.</h2>

<details><summary><b>The output you get should look something like this:</b></summary>
<DT>Source: <a href="javascript:popoxcode('../../examples/output/BobsChoiceC.output.txt');">examples/output/BobsChoiceB.output.txt</a>.</DT>
<dd><pre>
<object width="75%" height="300" type="text/plain" data="../../examples/output/BobsChoiceB.output.txt" border="1" ><p style="font-size:14pt"></p></object>
</pre></dd></details>
<h2>Summary</h2>
Here is what is important to gather from the examples:
<OL class="steps">
<LI>Discrete dynamic programming links together many different discrete choices that are made at different states.</LI>
<LI>A static problem with more than one state generalizes the `OneStateModel` introduced first but is a special case of sequential decisions made over time.</LI>
<LI>More generally then in the one-state example, your code will call routines/methods to first  <em>initialize</em> the DP environment, then add state variables and action variables to the model then <em>create the spaces</em> implied by the items added to the model.  Then the model can be solved.</LI>
<LI>Unlike the simple one state set up, you build the action vector $\alpha$ by sending action variables to `DP::Actions`() which will add them to the model.  This is done between initializing and creating spaces.</LI>
<LI>In <span class="n">DDP</span> timing is controlled by a clock and a static problem is specified by setting the clock as <code>StaticProgram</code>.  This is done between initializing and creating spaces.</LI>
<LI>The discrete state vector $\theta$ is a vector of state variables, each taking on a finite number of values.  Your model builds $\theta$ by creating new state variable objects and adding them to your model using `DP::EndogenousStates`(). </LI>
<LI>Different states in the model can affect the utility of actions. The value of state variables can affect the set of feasible actions. </LI>
<LI>Restrictions of actions is possible by providing a <code>FeasibleActions()</code> routine with your model.  It takes as an input argument the matrix of all possible actions, $A$, and returns a column of 0s and 1s indicating which rows are feasible at the current state.</LI>
<LI>As with <code>Utility()</code>, the <span class="n">DDP</span> code will always set the value of state variables before calling <code>FeasibleActions()</code>.  The value of a state variable is stored in its <code>.v</code> data member, or your code can send the state variable to `CV`() which will return the value.</LI>
</OL>

<h2>Exercises</h2>
<OL class="steps">Make a couple copies of <code>BobsChoiceB.ox</code> and <code>BobsChoiceC.ox</code> and experiment with them as follows. (Do not change the original file's contents.)
<LI>Suppose Bob has also applied for a Justin Bieber Scholarship which is worth &#36;100 <em>if</em> Bob attends a Canadian university.  Add a state variable to version B to account for this possibility as well as the Queen's scholarship.  Now there will be 2 state variables and four possible states.</LI>
<LI>Suppose McGill's Physics department does <em>not</em> have an undergraduate degree, so McGill-Physics is not feasible but McGill-Econ is.  Modify <code>FeasibleActions</code> in version C so that this option is excluded for both states of <code>Yacc</code>.  </LI>
</OL>

<LI id="SC">Choices Linked over Time (Dynamic States)</LI>

So far the environment has multiple discrete choices but no dynamics.  Now we introduce dynamics.  We will use Tommy's choice to introduce dynamics.   That is, we will model how many hours Tommy studies for an exam in the period leading up to the exam.</p>

<DT>Tommy Choices Redux</DT>
<DD>Tommy chooses among $J=13$ different discrete numbers of hours each night.  Nights are different (and thus his choice) because the opportunity cost of studying goes up on the weekend.</DD>
<DD>For now we continue a very simple (trivial) reason Tommy cares about studying. Later we add to the model that he knows how studying affects his expected score. </DD>

<h2>Discounting and the Overall Objective</h2>

In the static examples earlier utility is the objective and there is nothing that ties together multiple states.  Once we introduce sequential choice it becomes necessary to describe how choices and their utilities are aggregated over time.  The power of dynamic programming is to tie together simple static choices in order to explain sequential choice.</p>

<DT>Time</DT>
    <DD>Let <var>t</var> denote time, takes on discrete values. The first decision occurs at <var>t=0</var>.  Suppose the chooser at time 0 is trying to decide among different sequences of choices.  A particular sequence of choices is $\alpha \equiv \alpha_0, \alpha_0, \dots$.  For the moment, the total number of periods is left unspecified. It could be finite or infinite (the decisions keep going forever).  And, for the moment, we suppress any other state variables that may be part of the model other than <var>t</var>.</DD>

    <DT>Then, <span class="n">DDP</span> and the basic DP framework itself assume that the chooser places value on that sequence equal to</DT>
$$V(\alpha)  = \sum_{t=0,1,\dots}\  \delta^t U(\alpha_t; t).$$
    <DD>The new and fundamental parameter appearing in $V()$ is the <em>discount factor $\delta$</em>, which typically is in the range $[0,1)$.
    <details class="aside"><summary>Notes</summary>
    <DD>If the decision horizon is finite then $\delta=1$ is permissible.  If $\delta=0$ the person puts zero weight on future utility.  The decider only cares about the current utility even though current decisions might affect the future.</DD>
    <DD>This formulation allows utility to depend on time as well as the action chosen at time.  But soon we will move <var>t</var> into the state vector $\theta$ and it will simply be a specialized state variable not completely set apart from other state variables.</DD></details>
    </DD>

<h2>The Clock and Value Function Iteration</h2>

Dynamic programming accounts for a choice made today affects outcomes in the future along with the current appreciation (utility) of the choice.  The linear separability of $V()$ (and geometric discounting) makes it possible to solve the overall dynamic problem by breaking it into many connected static problems.</p>

<DT>Transitions</DT>
    <DD>The only required time distinction in DP is between now (today) and later (tomorrow). It is common in the literature to use &prime; to denote things that will happen tomorrow.  These notes follow that convention.  So if <var>t</var> is a state variable, then <var>t</var> alone is its value today when a choice is being made.  The value it takes on tomorrow is the denoted <var>t&prime;</var>. A state variable moving from today to its value tomorrow is called a <em>transition</em>.</DD>

<DT>Stationary and Non-Stationary Clocks</DT>
    <DD>There are two basic clocks in DP.  The first is a stationary clock in which today is the same as tomorrow.  This means that the decision horizon is infinite, because if decisions end sometime in the future then tomorrow is one period closer to the end then today.  Therefore they are not the same. The second is a special kind of non-stationary world which ends after fixed number of periods.  This is usually called a <em>finite horizon</em> model.  However, <span  class="n">DDP</span> makes distinctions between different kinds of finite horizon clocks.  So what is usually called a finite horizon is here called <em>normal aging</em>.</DD>

<DT>Normal Aging</DT>
    <DD>In normal aging, time starts at <code>0</code> and ends at some fixed last time, denoted <code>T-1</code>.  That is, <code>T</code> is the number of periods of choice. In normal aging, the time next period is always one period later than the last (until T-1).  That is,
    $$t' = t + 1,\quad   0 \le t \lt T.$
    This is the <em>transition</em> of a normal aging clock.  Each day you get another day older.  Note this is really defining a function,  <var>t'(t) = t+1</var>.  Technically, the transition is undefined for when <var>t=T-1</var> because that is the last period and the world ends immediately after.</DD>

<DT>Infinite Horizon (stationary)</DT>
    <DD>A stationary simply says tomorrow is just like today:
    $$t' = t$$
    Again, tomorrow is different from today but when the chooser wakes up tomorrow it is just like today.  However, what state the chooser is in tomorrow may be different than the state he was in today.
    <details class="aside"><summary>A perfect analogy</summary>In <em>Ground Hog Day</em> in which Bill Murray knows he will wake up and relive the same day, apparently forever, but his state can be different each day he wakes up.  He can learn to play the piano and every day he wakes up a little better.  And each day he makes choices that affect his utility in the future.  It is a perfect analogy to a stationary decision environment and also a perfect movie.</details>
    An the important feature of time is that if there is a yesterday (as in normal aging) it can never come again.  Any state that occur in the future cannot have a value of time less than <var>t</var>.  </DD>

<h2>Clocks in <span class="n">DDP</span></h2>


<h2>State Variables that Depend on Current and Past Events <span class="n">DDP</span></h2>

In BobsChoiceB there were two states.  Either Bob has a scholarship or not and this changes his decision.  Dynamic programs can account for state variables that depend on what has happened in the past.  For example a very common state variable counts how many times a choice has been made before the current period.  If $a$ is binary choice then utility might depend on how many times $a=1$ before now.
$$A_t = {\sum}_{s=0}^{t-1} a_s.$$
Note that this does not clearly define the initial value $A_0.$  Typically the model would set this to 0, but it could be initialized to a different value.  For dynamic programming it does not matter how the current value $A_t$ came about.  What matters is how current decisions will alter the effect $A$ in the future.  This is what is meant by the <em>transition of $A$</em>.  We can state this simply as
$$A^\prime = A + a.$$
That is, next period's value of $A$ will be what it is today plus the value of the current decision.  If $a=1$ then $A^\prime = A + 1.$  Otherwise it is unchanged.</p>

<h2>Terminal States</h2>

In some environments decision-making ends when a particular state is reached.  These are called terminal states.  A value of state variable can be made terminal using `StateVariable::MakeTerminal`().

<LI id="MD">Multiple Deciders</LI>

<em>Not completed yet: You can see information about this in <a href="Variables.ox.html#Fixed">Time Invariant or Grouping Variables </a></em>



</OL>

</OL>

@author &copy; 2011-2021 <a href="https://ferrall.github.io/">Christopher Ferrall</a></dd>

**/
