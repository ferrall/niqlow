/** Foundations of (Discrete) Dynamic Programming in <span class="n">DDP</span> .

<h2>Contents</h2>
<OL class="contents">
<LI><a href="OV">Overview</a></LI>
<LI><a href="#DC">A Single Discrete Choice</a></LI>
<LI><a href="#SP">Smoothed Choice Probabilities</a></LI>
<LI><a href="#MS">More than One Unlinked Choice (Multiple Static States)</a></LI>
<LI><a href="#SC">Choices Linked over Time (Dynamic States)</a></LI>
<LI><a href="#MD">Multiple Deciders</a></LI>
<LI><a href="#Next">Where to go next</a></LI>
</OL>

<a name="OV"><h2>Overview</h2></a>

<DT>What is Dynamic Programming?</DT>
<DD>DP  is a framework for choices when those choices affect what happens in the future, and therefore anticipating future choices is crucial for understanding choices today.</DD>
<DD>DP explains sequential choice over time rather than specifying a full set of choices all at once.  In the jargon of dynamic macroeconomics, the choice is a <em>policy</em> that is contingent (conditional) on which state is realized in the future not pre-commitment to a particular choice at a point in time.</DD>
<DT>A special case of DP is multiple decisions that are not linked together through time.  </DT>
<DD>There are two things that create this link in dynamic programming.
<UL>
<LI>First, the decider cares about future outcomes.  </LI>
<LI>Second, choices made now affect which states will occur in the future. .</DD></UL>
<DD>A special case of unlinked (non-sequential) decisions is a choice made once (at a single state).</DD>
<DD>Because this document presents the foundations of DP, the discussion starts with a single static choice.</DD>

<DT>What is <span class="n">DDP</span></DT>
<DD><span class="n">DDP</span> is the component of <span class="n">niqlow</span> that lets a user design and solve a discrete dynamic programming.</DD>
<DD>It is written in Ox and requires Ox to run.  The user <em>imports</em> <span class="n">DDP</span> into their Ox program to use it.</DD>

<DT>Why <em>Discrete</em> Dynamic Programming (DDP)?</DT>
 <DD>DDP is a term used in <span class="n">niqlow</span> that applies to models that have discrete (as opposed to continuous) choices and/or states. </DD>
 <DD>A model of decisions with discrete states (including as a very special case a single state/decisin) but continuous choices typically involves solving first order conditions or carrying non-linear optimization. <span class="n">DDP</span> does not incorporate continuous choice directly</DD>
 <DD>Continuous choice (and solving non-linear systems) is the topic of the <span class="n"><a href="../FiveO/default.html">FiveO</a></span> component of this package.</DD>
 <DD>Methods that embed continuous choice inside a discrete state spaces is therefore a hybrid of the two parts of <span class="n">niqlow</span>. These types of models are covered in the separate <a href="../Hybrids/default.html">Hybrids</a> module.</DD>

<DT>What does this document do?</DT>
<DD>Presents DDP from scratch starting with simple static choices then building up to dynamics</DD>
<DD>Shows how the key concepts of DDP are represented in <span class="n">DDP</span>.</DD>

<DT>What does this document <em>not</em> do?</DT>
<DD>Does <em>not</em> teach computer programming or the Ox programming language.
<LI>If you have no background in computing or no knowledge of Ox you should first consult  <a href="">??</a> and then return here.</LI></DD>
<DD>Does <em>not</em> present the general DDP framework nor the technical elements of <span class="n">DDP</span> in a systematic way.</DD>
<DD>If you are already comfortable with Ox and with dynamic programming, then you might find this document too slow.  <a href="">??</a> may be a better place to start (and that includes links to examples the presume more background knowledge than what is presented here).</DD>


<DT>What about estimation?</DT>
<DD>Within <span class="n">niqlow</span>, <em>estimation</em> refers to finding statistical estimates (guesses) of parameters of the model using data that is assumed to be generated by the model.</DD>
<DD>A parameter is a quantity that enters the model and whose value is determined outside the model (an exogenous or so-called structural parameter).</DD>
<DD>Parameters are typically continuous values and so guessing or estimating them is typically carried out using algorithms in <span class="n"><a href="../FiveO/default.html">FiveO</a></span>.</DD>
<DD><span class="n">DDP</span> includes methods to handle data generated by the model or created outside but assumed to be generated from the model.</DD>
<DD>This document does not concern itself with DDP data and estimation, which is discussed in <a href="">???</a>.</DD>

<h1>Foundations of DDP</h1>

<OL class="body">
<a name="DC"><LI>Discrete Choice</LI></a>

We start with  <em>choice</em> without states and dynamics.  We start with such a simple  static choice framework that it might seem trivial. The computer code shown to implement the model will appear to be overkill.  Bear with this simple start because you will see that all the elements of dynamic programming in many areas of economics can be handled easily within the framework.


<h2>The choice set</h2>

Choice is simply picking from a set of options.  We will let $A$ denote the finite set of possible actions to choose among.   And we will let an arbitrary element of $A$ be denoted $\alpha$.  We will work through a couple basic choices:

<DT>Example 1. Bob wants to go to university. Four have accepted him. These four places make up his discrete choice set:</DT>
<DD>$A = $ {<code>Harvard, Yale, Queen's, McGill</code>}.  </DD>
<DD>We will sometimes need a symbol for the number of actions available, and will use <code>J</code> for that purpose.  So for Bob, <code>J=4</code>.</DD>
<DD>Of course there are other schools but Bob either did not apply or did not get in to them.  They  are <em>infeasible</em> choices, and $A$ is his feasible choice set.</DD>
<DD>While $\alpha$ stands for any of the feasible actions, at some point Bob makes a choice. The school he chooses becomes special, and we distinguish the choice made from the options considered. If Bob chooses to go to Yale we would write $\alpha^\star = $ <code>Yale</code>.  </DD>
<DD>In these notes things marked with $\star$ are typically related to the optimal choice. </DD>

In some cases we might use discrete choice to approximate a continuous choice, so here is another example:

<DT>Example 2.  Tommy is choosing how much time to study for a test the next day.</DT>
<DD>So the choice might be any number between 0 and, say, 12 hours.</DD>
<DD>It would then be natural (or convenient or intuitive) to think of the choice set as $A=[0,12]$, to let $\alpha$ denote any real number in that range, and to characterize the optimal choice with a first-order condition (or marginal utility, $U^{\,\prime}(\alpha)$.  </DD>
<DD>However, for various reasons, we might stick with a discrete choice set to model Tommy's choice.  To be specific, we might think of him as studying for a number of hours: $A=$ {<code>0,1,2,&hellip;,12</code>}.   In this case $J=13$.  </DD>
<DD><details open><summary>What if Tommy were an actual person (say in a data set) and studied 2.2 hours?  </summary> Let's write $\hat\alpha$ = 2.2 to denote the observed choice.  This would suggest that Tommy did not choose among whole hours. This difference between actual and model behaviour might not matter to us (this choice might be part of a much bigger model).  So we might approximate by rounding to the nearest element of $A$ so that $\alpha^\star=$<code>2</code> would explain Tommy's choice. This is getting ahead of ourselves, but you might be wondering about accepting 2 to stand for an observed choice of 2.2.  One way in which empirical DDP work deals with this kind of discrepancy is to assume that there is <em>measurement error</em> in the data.  So the difference $d =$ <code>2.2-2</code> would be attributed to the measurement error (i.e. Tommy says he worked 2.2 hours but he really worked exactly 2 hours).  This can bother empirical people who are used to working with, say, regression models.  But the error in a regression model plays a similar role in "explaining" the difference between observed and predicted values.  </DD>

<h2>Utility</h2>

<DT>How did Bob choose Yale? And how Tommy chose 2 hours (according to our discrete approximation)?</DT>
<DD>Perhaps they considered all the pros and cons of their options and carefully weighed the factors to determine which was best.  Or maybe they threw a dart at the wall.</DD>

<DT>Whatever process they used ...</DT>
 <DD>we are not concerned with <em>how</em> the choice was made, but that a chose was made based on a  <em>utility</em> associated with each action.  <details class="aside" open><summary>Optimality and <em>function</em></summary>Of course, much of social science concerns choices that are not optimal, but for our purposes what people do will always be optimal for them given their options and preferences. Also, a teacher of mine emphasized that utility is by definition a function, so writing and saying "utility function" is redundant. It would be like saying "truck vehicle" instead of simply "truck," because by definition a truck is a vehicle. word </details></DD>

<DT>Let $U(\alpha)$ denote Bob's utility for school $\alpha$.</DT>
 <DD>$U(\alpha)$ is just a real number associated with $\alpha$.  </DD>
 <DD>Bob can have utility for options that are not feasible (not in $A$), such as U(<code>UCLA</code>), but we don't have a complete model until utility is defined for each element of $A$.  </DD>
 <DD>So, we can get fancy and write $U: A\,\to\, \Re$.  But since $A$ is a discrete set, $U$ is really just four numbers, one for each feasible school.</DD>

<DT>For example, Bob would set $\alpha^\star=$ <code>Yale</code> if his utility were:</DT>
<DD><pre>
&alpha;               U(&alpha;)
----------------------
Harvard        -20
Yale            4.2
Queen's        e<sup>1</sup>
McGill          ln(0.0009)
</pre>
Many other utility levels would explain <code>Yale</code> as an optimal choice.
</DD>

<h2>Multiple Dimensions</h2>

<DT>In many cases we want to model choices in more than one dimension.</DT>
  <DD>Bob was making a choice in the <em>school</em> dimension.  But he might also be making decisions in other dimensions, such as <em>major</em>, <em>roommate</em>, etc.</DD>
    <DD>Before showing <span class="n">DDP</span> code for Bob's choice let's have him decide his major at the same time.</DD>
<DD>One way to do this is to convert a choice in several (discrete) dimensions into a one dimensional choice by simply listing all the possible combinations.</DD>
<DD>Another way is to make &alpha; a vector not a scalar.</DD>
<DD>We illustrate both ways in the next example.</DD>

<DT>Example: Bob is choosing both a university <em>and</em> a major. Which major is best might depend on which school he chooses.  </DT>
<DD>For simplicity, suppose Bob's parents have told him he has to choose either <code>Economics</code> or <code>Physics</code>.  </DD>
<DD>Convert to 1 Dimensional: We could simply expand his choice set as follows: <code>{Harvard-Econ,Harvard-Physics,Yale-Econ,Yale-Physics,Queen's-Econ,Queen's-Econ,McGill-Econ,McGill-Physics}</code>.  Utility is then a number assigned to each of these <code>J=8</code> options.  </DD>
<DD>Add a 2nd Dimension: However, it can be more convenient not to collapse the two dimensions, but to consider them separate but simultaneous.  In this case, we can let school be the row and major the column:
<pre>
BOB'S UTILITY ON THE FEASIBLE MAJOR-SCHOOL CHOICE SET
                    Econ                    Physics
Harvard           -20                       -18
Yale              4.2                      -0.6
Queen's           1.5                        3.2
McGill           -25                        -0.5
</pre>
</DD>
<DD>Apparently Yale has a good Econ program, but if Bob had not gotten into Yale he would have chosen Queen's and majored in Physics.</DD>

<DT>We can also keep the dimensions separate by letting $\alpha$ be a <em>vector</em> of action variables: $\alpha = (a_0, a_1, \dots, a_{D-1})$.</DT>

<DD>In the major-school choice, <code>D=2</code>, because there are two dimensions of choice,  $a_0$ is the major choice, and $a_1$ the major choice.  <details class="aside" open><summary>Start at 0?</summary>We start counting at 0 because that is the way counting is done in many computer languages.  So doing so now may avoid some confusion later when we see code.</details>

Although using rows and columns to represent two different action variables is clear, it is not very helpful when there are three or more variables.

<DT>We can combine the list version with the vector version to get something like this:</DT>
<DD><pre>
BOB'S UTILITY USING ACTION VARIABLES
     &alpha;
a<sub>0</sub>            a<sub>1</sub>                U(&alpha;)
--------------------------------------
Econ          Harvard          -20
Physics       Harvard          -18
Econ          Yale                4.2
Physics       Yale              -0.6
Econ          Queen's           1.5
Physics       Queen's          3.2
Econ           McGill           -25
Physics       McGill           -0.5
</pre></DD>

<DT>In  <span class="n">DDP</span>:  </DT>
<DD>the action vector is built up by in this way by adding `ActionVariable`s to it as part of the set-up of the model.  </DD>
<DD>The creation of the list of actions as shown above is done for you by <span class="n">DDP</span> as you add action variables to the action vector.</DD>
<DD>As the user you only have to concern yourself with which variables are chosen and how many different values they take on.</DD>
<DD>You can provide value labels for each action variable you add to the model.  In the example above "Econ" would be a label for an integer option value.</DD>


<h2>Full Description of Bob's Problem</h2>

As with the first example, each row is an action $\alpha$, but it is associated with values of two action variables.

<DD>Labelling the choices certainly helps to understand them, but a computer program that is designed to handle any kinds of choices can hard-code labels like "Harvard" and use it to refer to utility.  </DD>
<DD>Hopefully it is clear that in the abstract the choice over the four universities is just a case of choosing among 4 possibilities.  The labels are helpful to us, but generically the choices can just be numbered 0, 1, 2, and 3.</DD>

<DT>The school-major choice can be describe with action variables that are just integers along with labels for each value.</DT>
<dd><pre>
&alpha; =  (a<sub>0</sub> a<sub>1</sub>)

Index     Label        Options     Choice Set     Value Labels
-------------------------------------------------------------------------
0          Major             2         0 &hellip; 1       Econ, Physics
1          School           4         0 &hellip; 3       Harvard,Yale,Queen's,McGill

A = [0&hellip;1] &times; [0&hellip;3]
a<sub>0</sub>     a<sub>1</sub>          U(&alpha;)
-------------------------
0       0        -20
1       0        -18
0       1         4.2
1       1        -0.6
0       2         1.5
1       2         3.2
0       3        -25
1       3        -0.5</pre></DD>

<DT>Bob's Choice</DT>

<DD>So far we just have 8 options with arbitrary values (utilities) assigned to them.  </DD>
<DD>And it turns out that the maximum of those utilities is 4.2 for the action of choosing <code>Yale</code> and <code>Econ</code>.
<DD class="disp">
$\alpha^\star \equiv \arg\max_{A}U(\alpha)$ = (<code>Econ</code>,<code>Yale</code>)

$EV \equiv \max_{A} U(\alpha) = U(\alpha^\star)$ = <code>4.2</code>.
</DD>

<DT>V, and EV</DT>
<DD>In microeconomics the highest utility possible is named <em>indirect utility</em>, but in dynamic programming it is usually called the (optimal) <em>value</em> of a state.</DD>
 <DD>So above $V$ is used to represent the value Bob receives from his choice once optimized.  </DD>
 <DD>We use $EV$ and not just $V$ in anticipation that there can elements of uncertainty in the choice.   In particular, the person deciding will know everything up until today when the choice is made, but they will have to make that choice without knowing everything that will happen in the future (tomorrow).  </DD>
 <DD>$EV$ will end up being good notation since when, deciding today, they will have to average (take the <b>E</b>expectation of) their optimal choices made tomorrow when they have more information than they have now.
<details class="aside" open><summary>Ties</summary><DD>The notation also assumes there are no ties, the optimal choice is unique.  That won't be necessary.  Ties are handled properly by <span class="n">DDP</span>,  but assuming no  ties does make the example simpler.</DD></details></DD>

<h2>Coding Bob's Choice in <span class="n">DDP</span></h2>

<DT>Bob's choice is trivial.</DT>
  <DD>Simply look at the 8 numbers that make up his utility.  </DD>
  <DD>Find the biggest number.  </DD>
  <DD>Look up the labels associated with that action. Finished.  </DD>

<DD>These operations could easily be done by hand, in a spreadsheet, or even in Ox using its built in <a href=""><code>maxc()</code> and <code>maxcindex()</code></a> routines.

<DD>When you look at the code for Bob's Choice in <span class="n">DDP</span> you will see that it has some elements that are not obvious.  Indeed, it relies on some sophisticated features of the Ox programming language.  Even if you have done some programming in similar languages such as Matlab, Python or R, your reaction may be: that is a lot of complexity for such a simple choice.  And you are right!</DD>

<DD>However, hopefully you see some logic in the complications so that the code is not completely unrelated to the problem as you understand it.  And you might see that some of the complication is there to make real models easier to build than if simple tools only were used.</DD>

<details><summary>The Code.</summary>
<DT>Source: <a href="../../examples/BobsChoice.ox">niqlow/examples/BobsChoice.ox</a></DT>
<DD><pre><object width="75%" height="300" type="text/Plain" data="../../examples/BobsChoice.ox" border="1" ><p style="font-size:14pt"></object></pre></dd></details>

<DT>Line-by-line explanation of the code</DT>
<DT> <code>&#35;import &hellip;</code>:</DT>
 <DD>Import is a way to tell Ox that you are using code that is not part of this file. In this, case the program is importing <span class="n">DDP</span>!
<details class="aside"><summary>&#35;import and &#35;include</summary> If you want to know more, see <a href="http://www.doornik.com/ox/oxtutlan.html#ox_tutlan_link">Multiple files in Ox</a>. Most examples shown in Ox start with <code>#include "oxstd.h"</code>.  That is done in <span class="n">DDP</span> so it is not necessary to do it explicitly.</details></DD>

<DT><code>class &hellip; { &hellip; }</code>: Class Declaration</DT>
<DD>A user's model is represented by a <code>class</code>, which is way to combine
data and functions that work on the data in one package.  This is called <em>object oriented programming</em>. See <a href="http://www.doornik.com/ox/oxtutlan.html#ox_tutlan_oo">OOP in Ox</a> if you are familiar with objects already and want to know how they work in Ox.</DD>

<details class="aside"><summary>Declare and  Define</summary>In a
programming language like Ox your program will have different things in it.  The language has to know what things are in the program, and this is the idea of <em>declaring</em>
something.  This is like listing a chapter of a book in a table of contents.  But Ox also has to know what the things are, which is like the contents of the chapter.  This is <em>defining</em>. How things are declared and defined in Ox depends on the kind of thing it is.  And, in some cases your program might declare and define something at the same time not separately.</details>
<DD>If you are not used to OOP, it can be mysterious and confusing.  And Ox can be used without OOP, but it is the way in which <span class="n">DDP</span> lets the user develop an economic model so it is essential for our purpose.  So here is the basic idea.  What is happening is the program is telling Ox that a new class is going to appear in this program.  A class is a description of a kind of thing (objects).  These lines are describing the class for Ox, which is just a list of the variables (also called <em>members</em> and functions (also called <em>methods</em>) that make up the class.  Every class has a name, and the code gives this class the name <code>BobsChoice</code>.  </DD>
<DD>One of the powerful (but complicated and confusing) features of OOP is that one class can be based on another class that is already defined.  In this case, <code>BobsChoice</code> is based on a class called `OneStateModel` which is defined in <span class="n">DDP</span>.  The name is meant to imply that
this model is really simple.  Unlike real dynamic programs there are many states at which the person (Bob) is making choices. He makes a choice once, so there is just one state.  (In turn, <code>OneStateModel</code> is a class based on other classes which are more flexible. The advantage of having a special class for a simple one state model is that some things can be done for the user.</DD>
<DD>The choice involves two action variables, and this class makes room for them with the <code>static decl</code> statement.  The names are short but understandable in the context.  <code>decl</code> is short for <em>declare</em> and does just make room for two things.  What they end up holding is determined by other parts of the program.</DD>
<DD>The <code>static</code> tag is important but at this point it is not necessary to understand why it is there.  It does not have anything to do with the fact that Bob's choice is a static choice.</DD>
<DD>The <code>BobsChoice</code> class also has two functions in it: <code>Decide()</code> and <code>Utility()</code>.  Note that <code>Decide()</code> is declared <code>static</code> but <code>Utility()</code> is not.  Again, this is not important to understand yet.  And like <code>decl</code>, listing functions in the declaration of  a class does not say anything about what they do.  The have to be <em>defined</em> later.</DD>

<DT><code>main(){&hellip;}</code></DT>
<DD>Every Ox program has to have a function (or routine) called <code>main()</code>. This is where the Ox program actually starts.  Even though the class declaration comes first in the file, it is <code>main()</code> that is the first thing to happen. I have written the code so that <code>main()</code> just does one thing: it asks that <code>Decide()</code> be executed. Once it is finished
<code>main()</code> is finished (not other statements  appear inside <code>main()</code>).  Most experienced programmers make their main routines pretty simple. </DD>
<DT><code>BobsChoice::Decide(){&hellip;}</code></DT>
<DD>Above the class declaration said that a routine belonging to <code>BobsChoice</code> and named <code>Decide()</code> would appear,
and these lines <em>define</em> what this routine does.  It is the lines of code that are executed when <code>main()</code> refers to the function.  </DD>
<DD><code>Decide()</code> does four things.  THat is, it has four statements each ending with <code>;</code>.  The first two say that <code>maj</code> and
<code>sch</code> will each contain an <em>action variable</em>.  This routine
<code>ActionVariables()</code> is part of <span class="n">DDP</span>, so it would not work to call it if we had not imported <span class="n">DDP</span>.  Hopefully, you can see that labels attached to the two variables are "major" and "school", respectively. Like nearly all computer languages, Ox asks you to put text inside quotes.  So <code>maj</code>, which is not in quotes is referring to a variable with that name. Ox will make room for because it was <code>decl</code>ared in the class declaration. However, the action variable has a name "major" which is just those characters not a variable or a routine.  </DD>
<DD>In Bob's choice he had only two majors to choose from.  We could write
<code>ActionVariable("major",2)</code> and this would mean <code>maj</code> would hold an action variable with two possible values.  This would not given meaningful names (or labels) to the two options: they are just be coded as <code>0</code> and <code>1</code>.  By sending two strings in quotes instead of <code>2</code> this gives each major a meaningful label.  The routine <code>ActionVariable()</code> counts the labels and knows that there are two choices.  They are still coded as <code>0</code> and <code>1</code> but those codes now have the labels "Econ" and "Physics". This will make some output easier to read.</DD>
<DD>The variable <code>sch</code> will contain another action variable that takes on four values (because a list of four labels are sent).</DD>
<DD>The next statement calls a routine with the name <code>Initialize()</code>.  Again, this routine is part of <span class="n">DDP</span> not Ox itself.  As the name implies, it initializes or sets up the problem meant to solve Bob's Choice.  Four things are sent to <code>Initialize()</code>.  The first is a bit mysterious: <code>new BobsChoice()</code>. The second is the number <code>0</code>.  As with some other parts of the code, it is not <em>yet</em> important to understand how these are chosen or used for this problem. In particular, to explain what <code>new BobsChoice()</code> means and why it is there would take us on a tangent that is not necessary for now. </DD>
<DD>However, it is important to see that the variables <code>maj</code> and <code>sch</code> are sent as well.  This is how the <code>OneStateModel</code> in <span class="n">DDP</span> knows what action variables are part of the model</DD>
<DD>You may think that the line that says <code>maj</code> contains an <code>ActionVariable</code> would add it to the model automatically.  It would be possible to make it work that way, but for other aspects of DP models (namely state variables) it is easier to separate creation of the variable from including it in the model. So to be consistent the same procedure is followed for action variables.</DD>
<DD>The last thing <code>Decide()</code> does is call a routine `VISolve`(), where "VI" stands for "value iteration."   That is a technique
for solving a dynamic programming problem. This one state model is the most simple example of a DP problem, but one part of value iteration is to solve for the optimal choice at each state.  So this is where <span class="n">DDP</span> will actually solve Bob's problem.</DD>
<DT><code>BobsChoice::Utility(){ &hellip;}</code></DT>
<DD>Any DDP problem has to supply a utility, and here is Bob's.  In general, the utility has to return (send back to whoever called the routine) a vector of utilities in the same order as the actions &alpha;.  The use of <code>&lt; -20; &hellip; &gt;</code> is the way Ox lets you hard-code a vector or matrix of numbers.   So the list of utilities above has been copied and written in that format.  We should definitely check that the action variables are organized so that the numbers match up: we wouldn't want Bob to mistakenly go to McGill and major in Physics.</DD>
<DD>Usually <code>Utility()</code> is a function of actions (and later states) and <em>parameters</em> of the problem.  So Bob's utility is not all typical in how it is coded.  Later examples and exercises will have utilities that are more like what show up in real DP problems.</DD>

<h2>Run the program and look at the output.</h2>

Getting Ox to run <code>BobsChoice.ox</code> and be able to use <span class="n">DDP</span> is not hard  if you are used to doing this kind of thing.  Otherwise, it can be a pain for the first few times you try something.  Go <a href="">here ???</a> for some help.</DD>

<details><summary><b>The output you get should look something like this:</b></summary>
<DT>Source: <a href="../examples/output/BobsChoice.output.txt">niqlow/docs/examples/output/BobsChoice.output.txt</a>.</DT>
<DD><pre><object width="75%" height="300" type="text/plain" data="../examples/output/BobsChoice.output.txt" border="1" ><p style="font-size:14pt"></object></pre></dd></details>

<DT>Selected Output</DT>
That output contains a lot of information that is not relevant to this simple case, so here are selected parts of the output:
<details><summary>Selected Output</summary><dd><pre>
-------------------- DP Model Summary ------------------------
4. ACTION VARIABLES
   Number of Distinct action vectors: 8
         major  schoo
    a.N      2      4

6. FEASIBLE ACTION SETS

    alpha       A[0]
    ----------------------
    (00)          X        -Econ-Harvard
    (10)          X        -Physics-Harvard
    (01)          X        -Econ-Yale
    (11)          X        -Physics-Yale
    (02)          X        -Econ-Queen's
    (12)          X        -Physics-Queen's
    (03)          X        -Econ-McGill
    (13)          X        -Physics-McGill
   &#35;States        1
    ----------------------
    Key: X = row vector is feasible. - = infeasible


     Value of States and Choice Probabilities
     ------------------------------------------------------------------------------
    Indx   I   T   A   q   t     r     f       EV      |Choice Probabilities:
       0   1   0   0   0   0     0     0       4.200000 0.000000 0.000000 1.000000 0.000000 0.000000 0.000000 0.000000 0.000000
     ------------------------------------------------------------------------------</pre></dd></details>

<DT>Explanation of Relevant Output</DT>
<DD>The "DP Model Summary" is produced by <code>Initialize()</code> after everything is set up.  (In models that have more than one state <code>Initialize()</code> only does
some of the things done here.  The rest are done by a routine that it calls but typically is called from the user's program.)</DD>
<DD>Part 4 of the summary shows you the action vector &alpha;.  It shows that the new action variables assigned to <code>maj</code> and <code>sch</code> were added to the vector and that together they create 8 different options.</DD>
<dd>Part 6 shows you the feasible action set $A$.  It shows you each action &alpha; as two integer values and the labels that go along with them.  Recall that <code>Yale-Econ</code> was the third option in the list and it appears that $A$ was set up so that it matches up with the vector in <code>Utility()</code> (but make sure!). </dd>
<DD>In real DP problems the actions that are feasible can depend on which state the system is at.  So this output is set up to show you the different values of $A$ in the model.  But with one state only there is one feasible set only.  Thus $A$ is called <code>A[0]</code>, because in some cases there will be <code>A[1]</code> and so forth.</DD>
<DD><code>VISolve()</code> produces the table of values and choice probabilities.  The first 8 columns really don't apply to this simple model because
there is only one state.  The table is designed to show the value of each state in the model.</DD>
<DD>The parts that matter are the <code>EV</code> and <code>Choice Probabilities</code>.  Recall the <code>EV</code> is the DP version of indirect utility, and we
know that the best Bob can do is 4.2.  And the optimal choice is the third element of $A$, which Bob should chose with probability 1.0.  All the other options are
sub-optimal and are chosen with 0 probability.</DD>

<h2>Summary</h2>

Here is what is important to gather from the example and code above.

<OL class="steps">
<LI>At the heart of a <span class="n">DDP</span> model is a discrete choice.  Discrete dynamic programming links together many different discrete choices
that are connected by state variables that evolve and forward-looking choices.</LI>
<LI>In <span class="n">DDP</span> you build your model by defining a <em>class</em> that is derived from one of the built-in `Bellman` classes.  The simplest
model to build from is `OneStateModel`, which is a single discrete choice (one state, no dynamics, etc).</LI>
<LI>The discrete choice $\alpha$ is a vector of action variables, each taking on a finite number of values.  Your model builds $\alpha$ by creating new `ActionVariable`s and
adding them to your model.  In a `OneStateModel` you send all the action variables to `OneStateModel::Initialize`().  In general your code will add action variables to yourself after
calling <code>Initialize()</code>.</LI>
<LI>Your model must supply a <code>Utility()</code> which returns a vector of numbers corresponding to the elements of the feasible set $A$.</LI>
<LI>The optimal choice for the model can be found by calling `VISolve`(), which does a four things that typically the user's code will do for itself in order to control output and solution methods.  For a one state model it simply finds $\alpha^\star$, the maximizing action.  This framework makes it possible to build a DP model around discrete choices. </LI>
</OL>

<h2>Exercises</h2>
<OL class="steps">Make a couple copies of <code>BobsChoice.ox</code> and experiment with them as follows. (Do not change the original file's contents.)
<LI>Change the utility vector so that two options tie as the optimal choice.  See what happens to the output.</LI>
<LI>Add a third option to the major choices, <code>Psycho</code>.  Modify utility so that <code>McGill-Psychology</code> is the optimal choice. Run the program and verify
the changes.  Try to correct any errors in the code that Ox complains about.</LI>
<LI>Modify your model to include a third action variable: <code>res</code> which is a binary choice to either live <code>ON</code> campus or <code>OFF</code>.  Tweak
the utility so that Yale-Econ is still optimal as is <code>ON</code>.  Run the code and confirm your changes.</LI>
<LI>Tommy's Choice</LI>
<DD>Make a copy of the file named <code>TommysChoice.ox</code>.  Change the name of the <code>class</code> defined in the file.  Delete <code>major</code> and rename
<code>school</code> to <code>hours</code>.  Use this create the action variable: <code>hours = new ActionVariable("h",13)</code>.  Because it is not
useful to label hours in the same way that schools and majors, just send the number of options (no value labels will be created).</DD>
<DD>Change the utility utility so that it is a function of the <code>hours</code> not just a list of arbitrary numbers.  In particular, if <var>U(h) = -(h-2.2)<sup>2</sup></var> then the optimal discrete
choice will be <var>h* = 2</var>, which is the 3rd value that <code>hours</code> takes on (0, 1, 2, etc). The vector of values that <code>hours</code> takes on can be accessed as <code>CV(hours)</code>.
<DD>So, modify <code>Utility()</code> to have this form:
<pre>
TommysChoice::Utility() {
        return -sqr(aa(hours)-2.2);
        }</pre></DD>
<DD>Debug your program  and run it until it produces the correct values of <code>EV</code> and choice probabilities.</DD>
<LI>Continuing with <code>TommysChoice</code> change the utility to be <var>-(h-2.5)<sup>2</sup></var>, which induces a tie between 2 and 3.  Inspect the output.</LI>
</OL>

<a name="SP"><LI>Smoothed Choice Probabilities</LI></a>

The examples above are not good models of behaviour.  For one thing, the utility values are arbitrary, whereas a good model would relate utility to observable characteristics of the chooser and the choices.  And, second, suppose Bob did not choose Yale-Econ?  Then our model is incorrect with probability 1.  This is not surprising because we typically are not modeling a single person's choice and we will never have access to the utility of all the options.  Instead, real discrete models provide a probability of people making choices not a 0/1 outcome.</p>

<DT>The <em>logit model</em> of choice includes a continuous random variable in the utility:</DT>
<dd class="disp">
$$U(\alpha) = Util(\alpha) + z_\alpha.$$
</dd>
<DD>The extra component $z_\alpha$ is assumed to follow the <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Type I Extreme Value (Gumbel) distribution</a>.  Across options $z_\alpha$ values are independent.  </DD>
<DD>The term <code>Util(&alpha;)</code> is a shortened version of name of the routine that is part the model in <span class="disp">DDP</span>.   So the user's coding would be the same <em>and the term $z_\alpha$ is not explicitly added to it.</em></DD>

<DT>The <em>probit model</em> of choice makes a different distributional assumption</DT>
<dd class="disp">
$$U(\alpha) = Util(\alpha) + z_\alpha, \qquad z_\alpha \sim N(0,1).$$
</dd>
<DD>Across options $z_\alpha$ values are independent.  </DD>

<DT>Conceptually ...</DT>
 <DD>Bob sees both <code>Utility()</code> (the part we see or assume or estimate) and the vector of values of $z_\alpha$ (the part we don't see but assume follows a convenient distribution).</DD>
  <DD>With his information Bob still simply chooses the $\alpha$ that maximizes $U(\alpha)$.  But since we don't see $z_\alpha$ any of the options might be optimal to Bob.  We simply get different <em>probabilities</em> of choices being optimal.  </DD>
  <DD>The probabilities depend on the observed vector of utilities, to the 0/1 vector of ouptut above would become a  vector of numbers between 0 and 1.  Since <code>Yale-Econ</code> has the highest value of <code>Utility()</code> it will have the greatest probability of being chosen, but the probability will not be 1.0.</DD>

<DT>Choice Probabilities under logit</DT>
<DD>Logit:
$$P(\alpha) = {e^{Util(\alpha)}  \over \sum_{\alpha'\in A} e^{Util(\alpha')}} = {e^{Util(\alpha)-Util(\alpha^\star)}  \over \sum_{\alpha'\in A} e^{Util(\alpha')-Util(\alpha^\star)}}$$
The second version expresses the probability in relative utility loss compared to the optimal choice $\alpha^\star$.</DD>
<DD>Following the literature, <span class="n">DDP</span> generalizes the logit expression to allow the amount of smoothing to be chosen by setting a parameter $\rho$:
$$P(\alpha)  = {e^{\rho\left(Util(\alpha)-Util(\alpha^\star)\right)}  \over \sum_{\alpha'\in A} e^{\rho\left(Util(\alpha')-Util(\alpha^\star)\right)}}$$
As $\rho\to \infty$ the probability approach the non-smooth values.  As $\rho\to 0$ the smoothing becomes complete and each feasible choice is equally likely.</DD>

<details><summary>See Train (20??)</summary>
<a href="">Train (20??)</a> explains all aspects of discrete choice with an emphasis on econometric applications.   Readers are encouraged to refer to Train () for in-depth discussion of the static case.  We start even simpler than Train does.  Eventually the set up starts to look like the material in Train, but we move to dynamic choice before going into the depths of various static choice models that Train does.</details>

<DT>Choice Probabilities under Probit</DT>
<DD>Independent Probit:
$$\eqalign{ P(\alpha) &= Prob( U(\alpha) \ge U(\alpha') ), \forall \alpha'\in A.\cr
&= \prod_{\alpha'\ne\alpha}\ \Phi\left( \rho(U(\alpha)-U(\alpha'))\right)\cr}$$
</DD>
<DD>Again, <span class="n">DDP</span> generalizes the standard probit by varying how important the deterministic component to utility is to choice. The smoothing parameter $\rho = 1/\sigma$ and $\sigma$ is the standard deviation of the difference $z_\alpha-z_{\alpha'}$.  This smoothing would not be identified if/when $U(\alpha)$ is based on estimated coefficients on variables related to $\alpha$. </DD>
<DD>Correlated Probit:</DD>



<h2>Smoothing Choice Probabilities in <span class="n">DDP</span></h2>

<DT>Select a Smoothing Method</DT>
<DD>Recall that Bob's Choice was derived from the `OneStateModel` class of problems.</DD>
<DD>That class is in turn a special case (a derived class) of the `ExPostSmoothing` class.</DD>
<DD>The method for smoothing for the one state model is made at this point in the code:
<pre>Initialize(new BobsChoice(),0,maj,sch)</pre></DD>
<DD>As you can see from the documentation of `OneStateModel::Initialize`(), the second argument is the smoothing method choice.</DD>
<DD>To avoid having to explain everything at once, it was set to 0 in the code, which is the default choice in the general <code>ExPostSmoothing</code> class.</DD>
<DD>Another more descriptive way to send 0 is to use a name for zero that is defined in <span class="n">DDP</span> in order to make the internal code easier to follow and debug:
<pre>Initialize(new BobsChoice(),NoSmoothing,maj,sch)</pre>
Here <code>NoSmoothing</code> is really just a name for the number 0.  It is one of the `SmoothingMethods` that are really just names for the integers
0, 1, and 2 (at the time of writing).</DD>
<DD>The code for the ExPostSmoothing class of problems will look at the method sent to it and decide if and how to smooth the resulting choice probabilities.</DD>

<DT>Logit Smoothing</DT>
<DD>To make Bob's Choice a logit model, just modify the code to ask for logit smoothing:
<pre>Initialize(new BobsChoice(),LogitKernel,maj,sch)</pre></DD>

<DT>Probit Smoothing</DT>
<DD>To make Bob's Choice a probit model, just modify the code to ask for probit smoothing:
<pre>Initialize(new BobsChoice(),GaussKernel,maj,sch)</pre></DD>

<details><summary>Why Ex Post?</summary>
<DD>In the standard discrete choice econometric model the error term $z$ is considered something known/observed by the chooser but not us.  In a more complicated situation, namely a dynamic one, Bob might make other choices before he knows the value of $z$.  For example, $z$ may be observed when Bob wakes up in the morning but he had to make a choice the day before that was affected by what he does today.  In that case, the value of $z$ is not just smoothing the choice probability for our sake, it is also have a direct effect on other decisions.  </DD>
<DD><span class="n">DDP</span> distinguishes between choice probability smoothing for our own sake and real uncertainty that affects the chooser in a dynamic environment.</DD>
<DD>The case of <em> ex-post smoothing</em> is when $z$ is not a real part of Bob's environment.  He is really just making a deterministic choice based on $Util(\alpha)$.  But after his choice we smooth the probability as if he also add a value of $z$.  </DD>
<DD>The case of <em>ex-ante</em> smoothing allows $z$ to play a role in other decisions made by Bob so it is a real "structural" error term.  </DD>
</details>

<a name="MS"><LI>More than One Unlinked Choice (Multiple Static States)</LI></a>

Bob and Tommy's choices are one-time discrete choices.  By deriving a model of their choices from the <code>OneStateModel</code> class there was no possibility of dynamics or examining how the situation affects the choice.

<DT>A state is a situation in which the agent makes a decision.  States differ from each other for various reasons:</DT>
<DD>Utility may differ between states (leading to different decisions).  </DD>
<DD>The feasible action set may differ between states (which could handled be a change in utility, but <span class="n">DDP</span> distinguishes between the two).</DD>
<DD>The effect of choices on which states occur in the future may differ between current states.</DD>

Obviously the next step from a one-state model is a two-state model.  We delay for a bit longer introducing time (sequential decisions) and consider two states in a static problem.

<h2>The State Space</h2>

In dynamic programming the choice of action $\alpha$ is contingent on the state the chooser is in.  To store and solve models efficiently, <span class="n">DDP</span> will
distinguish between types of states.  But a generic state will be denoted $\theta$.  And the set of states the problem might be in is denoted $\Theta$.

As with feasible actions, the state space is constructed by adding state variables to it.   We illustrate this with simple extension of Bob's Choice.

<DT>Example.  Bob is still deciding between schools and majors, however he may or may not receive a scholarship to Queen's for which he is applied for.</DT>

<DD>Obviously he will make his decision based on whether he gets the scholarship or not, so the choice is contingent upon that.  We want the model to capture both choices.  One reason to do this is that we want to apply the model to data in which some people get scholarships and some don't.  </DD>
<DD>The values of utility above will be the values without a scholarship.  Then in the event of getting the scholarship the utility of choosing Queen's goes up.</DD>
<DD>Scholarships are measured in dollars, but Bob's utility from getting and accepting a scholarship may be different that the monetary value.  However, to keep things simple we will assume that the utilities already listed are in money-equivalent values so we can add the value of the scholarship to them in order to compute net utility for Bob.  </DD>
<DD>Specifically, the Queen's scholarship has a value of &#36;2,200 and utility is measured in thousands of dollars. So in the case of getting the scholarship and going to Queen's utility increases by 2.2.</DD>

<h2>Coding Bob's New Choice in <span class="n">DDP</span></h2>

<DT>State Variables</DT>
<DD>The `StateVariable` class is designed to represents state variables in a dynamic program.  Like action variables, state variables are created and stored in a <code>static</code> member of the model.  They get added to the model and then <span class="n">DDP</span> incorporates them into the state space and the solution methods.</DD>
<DD>Unlike action variables, which are mainly different only in the number of values they can take on, state variables differ from each in a lot of ways. So <span class="n">DDP</span> provides a number of different kinds of state variables to add to a model as well as the possibility of defining your own type if it does not exist already.</DD>
<DD>However, in this simple static situation, the base <code>StateVariable</code> class will suffice.  So Bob's award status will be of that class.</DD>
<DD>The simple `OneStateModel` class is no longer appropriate because Bob either gets the award or not.  So this model of Bob's choice is derived from a different class of problems.  Again,
since it is a very simple situation, the base `Bellman` class will do the trick.  </DD>
<DT>Static Time</DT>
<DD>The Bellman class does not restrict the model to be static, so unlike <code>OneStateModel</code> the code will have to specify this feature of the environment.
<DD>But this will take one simple line of code.  Below the role of time is introduced in earnest.</DD>
<DT>Utility</DT>
<DD>Finally, the new model of Bob's choice will have to modify <code>Utility()</code> to account for scholarships.  </DD>
<DD>As with the first example, this may seem much more complicated than necessary, and it is for this simple static two-state environment.  But the approach to doing this will handle any kind of utility with limited programming complexity.</DD>


<details><summary><b>The Code</b></summary>
<DT>Source: <a href="../../examples/BobsChoiceB.ox">niqlow/examples/BobsChoiceB.ox</a></DT>
<DD><pre><object width="75%" height="300" type="text/plain" data="../../examples/BobsChoiceB.ox" border="1" >
<p style="font-size:14pt"></p></object></pre></DD></details>

<DT>Line-by-line explanation of the (new and modified) code</DT>

<DT><code>class BobsChoiceB : Bellman {</code>
<DD>We could continue to use the same name but for clarity this  model will be called <code>BobsChoiceB</code></DD>
<DD>The base class is no longer <code>OneStateModel</code> but the <code>Bellman</code></DD>
<DT><code>static const decl Uv =  &lt; -20; -18;  4.2; -0.6;  1.5;  3.2; -25; -0.5&gt;;  //Added</code>
<DD>The vector of utilities has been moved from the utility to a constant member of the class.  Now the value of the scholarship can be
added to <code>Uv</code> in order to compute overall utility.</DD>
<DT><code>static decl Qsch, </code></DT>
<DD>A variable to hold the scholarship state has been added to the model.</DD>
<DT>Changes to <code>Decide</code></DT>
<DD>We still need to <code>Initialize</code> the model, but the Bellman version of <code>Initialize</code> is different than the very special
OneStateModel version.  It does not allow you to list actions.  </DD>
<DD><code> SetClock(StaticProgram)</code> says that this is a static (but possibly multi-state) model.</DD>
<DD><code>Actions(maj,sch)</code>:  Except in specialized models like OneStateModel, action variables are added to the model by sending them to `DP::Actions`().  </DD>
<DD><code>Qsch = new StateVariable("Qsch",2)</code>:  this creates a basic state variable that takes on two values.</DD>
<DD><code>EndogenousStates(Qsch);</code>: As mentioned earlier, different kinds of states are tracked in <span class="n">DDP</span> and here we have added <code>Qsch</code>
to the <em>endogenous</em> state space.
<DD><code>    CreateSpaces()</code>: In the first example this routine is called inside `OneStateModel::Initialize`().  But in all other kinds of models, the user calls this method themselves once they have added all the elements to model.  Each kind of Bellman model has its own version of `DP::CreateSpaces`(), and the one that is called is the one that the model is derived from (here <code>Bellman</code>'s own base version).  Sometimes arguments can or have to be sent to <code>CreateSpaces</code>, but often the default values apply and nothing is sent.</DD></pre>
<DT>Utility</DT>
<DD>Utility now returns a different vector depending on whether <code>Qsch</code> equals 0 or 1.</DD>
<DD><code>Qsch</code> is the state variable and takes on 2 values, 0 and 1.  We set utility up so that a value of 1 indicates the Queen's scholarship was received and its value should be added to the utility of Queen's programs when choosing where to go.  </DD>
<DD>The underlying code will call <code>Utility</code> for all values of all states added to the model.  The value of <code>Qsch</code> when called can be accessed a couple different ways.  One way is directly:  <code>Qsch.v</code> will hold either 0 or 1 when Utility is called.  Or the variable itself can be sent to `CV`() which will access the value return it.  <code>CV()</code> is a flexible way of incorporating state variables, parameters and even constants in your model in such way that it looks the same regardless of the kind of thing it is.</DD>
<DD>The trickier thing to understand at this point is accessing values of the school choice, which is an action variable not a state variable.  Unlike states, which take on a single value when Utility is called, the full action vector is relevant so <code>CV(sch)</code> will not work.
</DD>
<DD>The value of the scholarship is 2.2, but only if Queen's is chosen.  Looking at the list of labels associated with <code>sch</code>, Queen's is the 3rd value, or <code>2</code> since
we start actions at 0.  Ox's <code>.==</code> operator will compare a vector to a value and return a vector of 0s and 1s for whether corresponding element equals the value.  If this
is unclear, you can add a <code>print()</code> statement above <code>return</code> to see what <code>CV(sch)*(aa(sch).==2)</code> contributes to utlity.</DD>

<h2>Run the program and look at the output.</h2>

<details><summary><b>The output you get should look something like this:</b></summary>
<DT>Source: <a href="../examples/output/BobsChoiceB.output.txt">../examples/output/BobsChoiceB.output.txt</a>.</DT>
<dd><pre><object width="75%" height="300" type="text/plain" data="../examples/output/BobsChoiceB.output.txt" border="1" ><p style="font-size:14pt"></p></object></pre></dd></details>


<h2>Restricted Feasible Actions</h2>

<DT>Instead of the Queen's scholarship, we want our model to consider what happens if Yale does or does not accept him. </DT>
<DD>We handle this by creating a model with two states, one in which Yale is in the feasible set of schools and another at which it is not.</DD>

<DT>State-Dependence in $A$ </DT>
<DD>An important feature of many dynamic programming models: the feasible set depends on the state the action is being taken at.</DD>
<DD>One way to handle this is to modify the utility so that infeasible states have value -&infin;.  This works fine, but <span class="n">DDP</span> also provides a method for creating
different feasible sets.</DD>
<DD>Given a state $\theta$, we can specify that the feasible action set is $A(\theta)$.  If we let plain old $A$ denote the unrestricted set (as above), then
$A(\theta) \subseteq A$.  </DD>
<DD>Recall that each action vector $\alpha$ is represented in the code as a row in a matrix.  So far we have not needed to access this matrix directly (it's stored and available as `Alpha::Matrix`).</DD>

<h2>Coding Bob's New Choice in <span class="n">DDP</span></h2>

<DT><code>FeasibleActions()</code></DT>
<DD>To restrict feasible actions, the user provides a <em>method</em> that returns a column vector of 0s and 1s.  This column vector depends on the current state of the model. A 1 indicates the action (row) is feasible at the state, 0 means infeasible.  </DD>
<DD>The user's method is called by <span class="n">DDP</span> inside `DP::CreateSpaces`() for each possible state.   </DD>

<details><summary><b>The Code</b></summary>
<DT>Source: <a href="../../examples/BobsChoiceC.ox">niqlow/examples/BobsChoiceC.ox</a></DT>
<DD><pre><object width="75%" height="300" type="text/plain" data="../../examples/BobsChoiceC.ox" border="1" >
<p style="font-size:14pt"></p></object></pre></DD></details>

<DT>Line-by-line explanation of the (new and modified) code</DT>
<DD><code>Yacc</code> is added to the class so that it can</DD>


<h2>Run the program and look at the output.</h2>

<details><summary><b>The output you get should look something like this:</b></summary>
<DT>Source: <a href="../examples/output/BobsChoiceC.output.txt">../examples/output/BobsChoiceB.output.txt</a>.</DT>
<dd><pre><object width="75%" height="300" type="text/plain" data="../examples/output/BobsChoiceB.output.txt" border="1" ><p style="font-size:14pt"></p></object></pre></dd></details>


<h2>Summary</h2>
Here is what is important to gather from the examples:
<OL class="steps">
<LI>Discrete dynamic programming links together many different discrete choices that are made at different states.</LI>
<LI>A static problem with more than one state generalizes the `OneStateModel` introduced first but is a special case of sequential decisions made over time.</LI>
<LI>More generally then in the one-state example, your code will call routines/methods to first  <em>initialize</em> the DP environment, then add state variables and action variables to the model then <em>create the spaces</em> implied by the items added to the model.  Then the model can be solved.</LI>
<LI>Unlike the simple one state set up, you build the action vector $\alpha$ by sending action variables to `DP::Actions`() which will add them to the model.  This is done between initializing and creating spaces.</LI>
<LI>In <span class="n">DDP</span> timing is controlled by a clock and a static problem is specified by setting the clock as <code>StaticProgram</code>.  This is done between initializing and creating spaces.</LI>
<LI>The discrete state vector $\theta$ is a vector of state variables, each taking on a finite number of values.  Your model builds $\theta$ by creating new state variable objects and
adding them to your model using `DP::EndogenousStates`(). </LI>
<LI>Different states in the model can affect the utility of actions and the set of feasible actions. </LI>
<LI>Restrictions of actions is possible by providing a <code>FeasibleActions()</code> routine with your model.  It takes as an input argument the matrix of all possible actions, $A$, and returns a column of 0s and 1s indicating which rows are feasible at the current state.</LI>
<LI>As with <code>Utility()</code>, the <span class="n">DDP</span> code will always set the value of state variables before calling <code>FeasibleActions()</code>.  The value of a state variable is stored in its <code>.v</code> data member, or your code can send the state variable to `CV`() which will return the value.</LI>
</OL>

<h2>Exercises</h2>
<OL class="steps">Make a couple copies of <code>BobsChoiceB.ox</code> and <code>BobsChoiceC.ox</code> and experiment with them as follows. (Do not change the original file's contents.)
<LI>Suppose Bob has also applied for a Justin Bieber Scholarship which is worth &#36;100 <em>if</em> Bob attends a Canadian university.  Add a state variable to version B to account for this possibility as well as the Queen's scholarship (so now there will be two state variables and four possible states).</LI>
<LI>Suppose McGill's Physics department does <em>not</em> have an undergraduate degree, so McGill-Physics is not feasible but McGill-Econ is.  Modify <code>FeasibleActions</code> in version C so that this option is excluded for both states of <code>Yacc</code>.  </LI>
</OL>

<a name="SC"><LI>Choices Linked over Time (Dynamic States)</LI></a>

So far the environment has multiple discrete choices but no dynamics.  Now we introduce dynamics.  We will use Tommy's choice to introduce dynamics.   That is, we will model how many hours Tommy studies for an exam in the period leading up to the exam.

<DT>Tommy Choices Redux</DT>
<DD>Tommy chooses among $J=13$ different discrete numbers of hours each night.  Nights are different (and thus his choice) because the opportunity cost of studying goes up on the weekend.</DD>
<DD>For now we continue a very simple (trivial) reason Tommy cares about studying. Later we add to the model that he knows how studying affects his expected score. </DD>

<h2>Discounting and the Overall Objective</h2>

In the static examples earlier utility is the objective and there is nothing that ties together multiple states.  Once we introduce sequential choice it becomes necessary to describe how choices and their utilities are aggregated over time.  The power of dynamic programming is to tie together simple static choices in order to explain sequential choice.</p>

<DT>Time</DT>
<DD>Let <var>t</var> denote time, which is discrete.</DD>
<DD>The first decision occurs at <var>t=0</var>.  Suppose the chooser at time 0 is trying to decide among different sequences of choices.  </DD>
<DD>A particular sequence of choices is $\alpha \equiv \alpha_0, \alpha_0, \dots$.  For the moment the number of periods is left unspecified. It could be finite or infinite.  And for the moment we suppress any other state variables that may be part of the model other than <var>t</var>.</DD>

<DT>Then, <span class="n">DDP</span> and the basic DP framework itself assume that the chooser places value (utility) on that sequence equal to</DT>
<dd span="display">
$$V(\alpha)  = \sum_{t=0,1,\dots}\  \delta^t U(\alpha_t; t).$$
</dd>
<DD>The new and fundamental parameter appearing in $V()$ is the <em>discount factor $\delta$</em>, which typically is in the range $[0,1)$.
<details class="aside"><summary>Notes</summary>
<DD>If the decision horizon is finite then $\delta=1$ is permissible.  </DD>
<DD>This formulation allows utility to depend on time as well as the action chosen at time.  But soon we will move <var>t</var> into the state vector $\theta$ and it will simply be a somewhat specialized state variable not something completely set apart from other states.</DD></details></DD>

<h2>The Clock and Value Function Iteration</h2>

Dynamic programming accounts for a choice made today affects outcomes in the future along with the current appreciation (utility) of the choice.  The linear separability of $V()$ (and geometric discounting) makes it possible to solve the overall dynamic problem by breaking it into many connected static problems.

<DT>Transitions</DT>
<DD>The only required time distinction in DP is between now (today) and later (tomorrow).  </DD>
<DD>It is common in the literature to use &prime; to denote things that will happen tomorrow.  These notes follow that convention.  </DD>
<DD>So if <var>t</var> is a state variable, then <var>t</var> alone is its value today when a choice is being made.  The value it takes on tomorrow is the denoted <var>t&prime;</var>.  </DD>
<DD>A state variable moving from today to its value tomorrow is called a <em>transition</em>.</DD>

<DT>Stationary and Non-Stationary Clocks</DT>
<DD>There are two basic clocks in DP.  The first is a stationary clock in which today is the same as tomorrow.  This means that the decision horizon is infinite, because if decisions end sometime in the future then tomorrow is one period closer to the end then today.  Therefore they are not the same.</DD>
<DD>The second is a special kind of non-stationary world which ends after fixed number of periods.  The is usually called a <em>finite horizon</em> model.  However, <span  class="n">DDP</span> makes distinctions between different kinds of finite horizon clocks.  So what is usually called a finite horizon is here called <em>normal aging</em>.</DD>

<DT>Normal Aging</DT>
<DD>In normal aging, time starts at <code>0</code> and ends at some fixed last time, denoted <code>T-1</code>.  That is, <code>T</code> is the number of periods of choice.</DD>
<DD>In normal aging, the time next period is always one period later than the last (until T-1).  That is,
<pre>t' = t + 1,   for 0 &le; t &lt; T-1.</pre>
This is the <em>transition</em> of a normal aging clock.  Each day you get another day older.  Note this is really defining a function,  <var>t'(t) = t+1</var>. </DD>
<DD>Technically, the transition is undefined for when <var>t=T-1</var> because that is the last period and the world ends immediately after.</DD>

<DT>Infinite Horizon (stationary)</DT>
<DD>A stationary simply says tomorrow is just like today:
<pre>t' = t</pre>
</DD>
<DD>Again, tomorrow is different from today but when the chooser wakes up tomorrow it is just like today.  However, what state the chooser is in tomorrow may be different than the state he was in today.  <details class="aside"><summary>A perfect analogy</summary>In <em>Ground Hog Day</em> in which Bill Murray knows he will wake up and relive the same day, apparently forever, but his state can be different each day he wakes up.  He can learn to play the piano and every day he wakes up a little better.  And each day he makes choices that affect his utility in the future.  It is a perfect analogy to a stationary decision environment and also a perfect movie.</details></DD>
<DD>An the important feature of time is that if there is a yesterday (as in normal aging) it can never come again.  Any state that occur in the future cannot have a value of time less than <var>t</var>.  </DD>

<h2>Setting the Clock in <span class="n">DDP</span></h2>

<LI><a name="MD">Multiple Deciders</a></LI>

<LI><a name="Next">Where to go next</a></LI>


</OL>

**/
