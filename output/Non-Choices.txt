Output of Rservation Wage Tests:Non-Choices
---------------------------
DP::Intialize is complete. Action and State spaces are empty.
 Log file name is: logs/DP-DDP-3-11-2021-10-35-38.log
Action variable objected stored in d.  Label = 'd'.  Number of values: 2
-------------------- DP Model Summary ------------------------
0. USER BELLMAN CLASS
    WStarC | One Dimensional Choice  | Bellman
1. CLOCK
    1. Ergodic
2. STATE VARIABLES
              |eps   |eta |theta -clock        |gamma
              s11    s21    wrk      t     t'      r      f
       s.N      1      1      2      1      1      1      1


     Transition Categories (not counting placeholders and variables with N=1)
                 NonRandom       Random   Coevolving    Augmented       Timing    Invariant
     #Vars               0            0            0            1            0            0

3. SIZE OF SPACES

                       Number of Points
    Exogenous(Epsilon)                1
    SemiExogenous(Eta)                1
   Endogenous(Theta_t)                2
                 Times                1
         EV()Iterating                2
      ChoiceProb.track                2
         Random Groups                1
 Dynamic Random Groups                1
          Fixed Groups                1
   Total Groups(Gamma)                1
       Total Untrimmed                2

4. ACTION VARIABLES
   Number of Distinct action vectors: 2
             d
    a.N      2


5. TRIMMING AND SUBSAMPLING OF THE ENDOGENOUS STATE SPACE (Theta)
                           N
    TotalReachable         2
         Terminal          0
     Approximated          0
Index of first state by t (t=0..T-1)
      0      1


6. FEASIBLE ACTION SETS
 
     i    [d]        A[0]     A[1]   
     ------------------------------------
     000 (0)           X        -        
     001 (1)           X        X        
        #States        1        1
     -----------------------------------
         Key: X = row vector is feasible. - = infeasible

-------------------- End of Model Summary ------------------------
Phase :Initializing Increment:       0.04. Cumulative:         0.04
sigma = 0.4

     Value of States, Ergodic Distn, and Choice Probabilities 
    Indx   T   A wrk   t     r     f       EV      |  Erg.Distn  |Choice Probabilities:
       1   1   1   1   0     0     0        4.910676  0.7396985 1.000000
       0   1   0   0   0     0     0        5.690148  0.2603015 0.431660 0.568340       0.731140
     ------------------------------------------------------------------------------
sigma = 0.45

     Value of States, Ergodic Distn, and Choice Probabilities 
    Indx   T   A wrk   t     r     f       EV      |  Erg.Distn  |Choice Probabilities:
       1   1   1   1   0     0     0        5.048905  0.7326937 1.000000
       0   1   0   0   0     0     0        5.850319  0.2673063 0.451795 0.548205       0.745492
     ------------------------------------------------------------------------------
sigma = 0.5

     Value of States, Ergodic Distn, and Choice Probabilities 
    Indx   T   A wrk   t     r     f       EV      |  Erg.Distn  |Choice Probabilities:
       1   1   1   1   0     0     0        5.189799  0.7266742 1.000000
       0   1   0   0   0     0     0        6.013577  0.2733258 0.468273 0.531727       0.760194
     ------------------------------------------------------------------------------
sigma = 0.55

     Value of States, Ergodic Distn, and Choice Probabilities 
    Indx   T   A wrk   t     r     f       EV      |  Erg.Distn  |Choice Probabilities:
       1   1   1   1   0     0     0        5.332712  0.7214689 1.000000
       0   1   0   0   0     0     0        6.179175  0.2785311 0.481947 0.518053       0.775103
     ------------------------------------------------------------------------------
sigma = 0.6

     Value of States, Ergodic Distn, and Choice Probabilities 
    Indx   T   A wrk   t     r     f       EV      |  Erg.Distn  |Choice Probabilities:
       1   1   1   1   0     0     0        5.477192  0.7169275 1.000000
       0   1   0   0   0     0     0        6.346588  0.2830725 0.493467 0.506533       0.790175
     ------------------------------------------------------------------------------
sigma = 0.65

     Value of States, Ergodic Distn, and Choice Probabilities 
    Indx   T   A wrk   t     r     f       EV      |  Erg.Distn  |Choice Probabilities:
       1   1   1   1   0     0     0        5.622910  0.7129331 1.000000
       0   1   0   0   0     0     0        6.515435  0.2870669 0.503298 0.496702       0.805374
     ------------------------------------------------------------------------------
sigma = 0.7

     Value of States, Ergodic Distn, and Choice Probabilities 
    Indx   T   A wrk   t     r     f       EV      |  Erg.Distn  |Choice Probabilities:
       1   1   1   1   0     0     0        5.769624  0.7093943 1.000000
       0   1   0   0   0     0     0        6.685437  0.2906057 0.511782 0.488218       0.820676
     ------------------------------------------------------------------------------
sigma = 0.75

     Value of States, Ergodic Distn, and Choice Probabilities 
    Indx   T   A wrk   t     r     f       EV      |  Erg.Distn  |Choice Probabilities:
       1   1   1   1   0     0     0        5.917149  0.7062386 1.000000
       0   1   0   0   0     0     0        6.856379  0.2937614 0.519175 0.480825       0.836063
     ------------------------------------------------------------------------------
sigma = 0.8

     Value of States, Ergodic Distn, and Choice Probabilities 
    Indx   T   A wrk   t     r     f       EV      |  Erg.Distn  |Choice Probabilities:
       1   1   1   1   0     0     0        6.065345  0.7033922 1.000000
       0   1   0   0   0     0     0        7.028099  0.2966078 0.525709 0.474291       0.851590
     ------------------------------------------------------------------------------
sigma = 0.85

     Value of States, Ergodic Distn, and Choice Probabilities 
    Indx   T   A wrk   t     r     f       EV      |  Erg.Distn  |Choice Probabilities:
       1   1   1   1   0     0     0        6.214102  0.7008411 1.000000
       0   1   0   0   0     0     0        7.200467  0.2991589 0.531459 0.468541       0.867097
     ------------------------------------------------------------------------------
sigma = 0.9

     Value of States, Ergodic Distn, and Choice Probabilities 
    Indx   T   A wrk   t     r     f       EV      |  Erg.Distn  |Choice Probabilities:
       1   1   1   1   0     0     0        6.363331  0.6985287 1.000000
       0   1   0   0   0     0     0        7.373384  0.3014713 0.536587 0.463413       0.882655
     ------------------------------------------------------------------------------
sigma = 0.95

     Value of States, Ergodic Distn, and Choice Probabilities 
    Indx   T   A wrk   t     r     f       EV      |  Erg.Distn  |Choice Probabilities:
       1   1   1   1   0     0     0        6.512963  0.6964233 1.000000
       0   1   0   0   0     0     0        7.546767  0.3035767 0.541188 0.458812       0.898255
     ------------------------------------------------------------------------------
sigma = 1

     Value of States, Ergodic Distn, and Choice Probabilities 
    Indx   T   A wrk   t     r     f       EV      |  Erg.Distn  |Choice Probabilities:
       1   1   1   1   0     0     0        6.662941  0.6944986 1.000000
       0   1   0   0   0     0     0        7.720551  0.3055014 0.545338 0.454662       0.913892
     ------------------------------------------------------------------------------
sigma = 1.05

     Value of States, Ergodic Distn, and Choice Probabilities 
    Indx   T   A wrk   t     r     f       EV      |  Erg.Distn  |Choice Probabilities:
       1   1   1   1   0     0     0        6.813217  0.6927324 1.000000
       0   1   0   0   0     0     0        7.894680  0.3072676 0.549101 0.450899       0.929561
     ------------------------------------------------------------------------------
... finished.
